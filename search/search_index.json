{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Genie Tooling","text":"<p>Genie Tooling is a hyper-pluggable Python middleware designed to empower developers in building sophisticated Agentic AI and LLM-powered applications. It provides a modular, async-first framework that emphasizes clear interfaces and interchangeable components, allowing for rapid iteration and customization of agent capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simplified Development with the <code>Genie</code> Facade: The primary entry point for most applications, offering a high-level API for common agentic tasks like LLM interaction, Retrieval Augmented Generation (RAG), tool execution, and natural language command processing.</li> <li>Hyper-Pluggable Architecture: Almost every piece of functionality\u2014from LLM providers and tools to data loaders and caching mechanisms\u2014is a plugin. This allows you to easily swap, extend, or create custom components.</li> <li>Simplified Configuration: Utilize <code>FeatureSettings</code> within <code>MiddlewareConfig</code> for quick setup of common features, with the flexibility for detailed overrides.</li> <li>Async First: Built with <code>asyncio</code> for high-performance, I/O-bound operations common in AI applications.</li> <li><code>@tool</code> Decorator: Effortlessly convert your existing Python functions into Genie-compatible tools with automatic metadata and schema generation. Remember to enable these tools via <code>tool_configurations</code> after registration.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running quickly with the <code>Genie</code> facade:</p> <pre><code>import asyncio\nimport logging\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    logging.basicConfig(level=logging.INFO)\n\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            llm=\"ollama\", # Default: ollama, ensure it's running\n            llm_ollama_model_name=\"mistral:latest\",\n            command_processor=\"llm_assisted\",\n            tool_lookup=\"embedding\" \n        ),\n        tool_configurations={ # Explicitly enable tools\n            \"calculator_tool\": {} \n        }\n    )\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    # LLM Chat\n    chat_response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Tell me about Genie Tooling.\"}])\n    print(f\"LLM: {chat_response['message']['content']}\")\n\n    # Command Execution (e.g., calculator tool is built-in)\n    cmd_result = await genie.run_command(\"What is 5 times 12?\")\n    print(f\"Command Result: {cmd_result.get('tool_result')}\")\n\n    await genie.close()\n    print(\"Genie torn down.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li>User Guide: Learn how to install, configure, and use Genie Tooling for your projects.<ul> <li>Installation</li> <li>Configuration &amp; Simplified Configuration</li> <li>Using LLM Providers</li> <li>Using Tools</li> <li>Using RAG</li> <li>Using Command Processors</li> <li>Tool Lookup</li> <li>Logging</li> <li>Prompt Management</li> <li>Conversation State</li> <li>Observability &amp; Tracing</li> <li>Human-in-the-Loop (HITL)</li> <li>Token Usage Tracking</li> <li>Guardrails</li> <li>Distributed Tasks</li> </ul> </li> <li>Developer Guide: Understand the plugin architecture and learn how to create your own custom plugins and tools.<ul> <li>Plugin Architecture</li> <li>Creating Tool Plugins</li> <li>Creating RAG Plugins</li> <li>Creating Other Plugins</li> </ul> </li> <li>API Reference: Detailed reference for all public modules and classes.</li> <li>Tutorials &amp; Examples: Step-by-step guides and practical examples to get you started.</li> </ul> <p>We encourage you to explore the documentation and examples to unlock the full potential of Genie Tooling!</p>"},{"location":"LLM_code_assist_instructions/LLM_api_guide/","title":"LLM_GUIDE_METADATA_V0.4","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#this-file-is-a-hyper-compressed-semantic-representation-of-genie-tooling-for-llm-consumption","title":"THIS FILE IS A HYPER-COMPRESSED SEMANTIC REPRESENTATION OF GENIE TOOLING FOR LLM CONSUMPTION.","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#llm-if-you-update-the-genie-tooling-project-code-classes-methods-params-core-logic","title":"LLM: IF YOU UPDATE THE GENIE TOOLING PROJECT CODE (CLASSES, METHODS, PARAMS, CORE LOGIC),","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#you-must-update-this-file-to-reflect-those-changes-accurately","title":"YOU MUST UPDATE THIS FILE TO REFLECT THOSE CHANGES ACCURATELY.","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#maintain-the-compressed-format-human-readability-is-not-a-priority","title":"MAINTAIN THE COMPRESSED FORMAT. HUMAN READABILITY IS NOT A PRIORITY.","text":"<p>Project: Genie Tooling. Python Agentic/LLM Middleware. Async. MIT.</p> <p>Facade: <code>genie_tooling.genie.Genie</code> *   <code>init</code>: <code>Genie.create(cfg:MiddlewareConfig, kp_inst?:KeyProvider)</code> *   <code>llm</code>: <code>LLMInterface</code>     *   <code>chat(msgs:List[ChatMessage], prov_id?:str, stream?:bool, **kw) -&gt; LLMChatResp|AsyncIter[LLMChatChunk]</code> (kwargs can include <code>output_schema</code> for GBNF with Llama.cpp)     *   <code>generate(prompt:str, prov_id?:str, stream?:bool, **kw) -&gt; LLMCompResp|AsyncIter[LLMCompChunk]</code> (kwargs can include <code>output_schema</code> for GBNF with Llama.cpp)     *   <code>parse_output(resp:LLMChatResp|LLMCompResp, parser_id?:str, schema?:Any) -&gt; ParsedOutput</code> *   <code>rag</code>: <code>RAGInterface</code>     *   <code>index_directory(path:str, collection_name?:str, loader_id?:str, splitter_id?:str, embedder_id?:str, vector_store_id?:str, loader_config?:Dict, splitter_config?:Dict, embedder_config?:Dict, vector_store_config?:Dict, **kw) -&gt; Dict[str,Any]</code>     *   <code>index_web_page(url:str, collection_name?:str, loader_id?:str, splitter_id?:str, embedder_id?:str, vector_store_id?:str, loader_config?:Dict, splitter_config?:Dict, embedder_config?:Dict, vector_store_config?:Dict, **kw) -&gt; Dict[str,Any]</code>     *   <code>search(query:str, collection_name?:str, top_k?:int, retriever_id?:str, retriever_config?:Dict, **kw) -&gt; List[RetrievedChunk]</code> *   <code>tools</code>: (Methods on <code>Genie</code> instance)     *   <code>execute_tool(tool_identifier:str, **params:Any) -&gt; Any</code> (Tool must be enabled in <code>tool_configurations</code>)     *   <code>run_command(command:str, processor_id?:str, conversation_history?:List[ChatMessage]) -&gt; CmdProcResp</code> (Integrates HITL if configured; tools used must be enabled in <code>tool_configurations</code>) *   <code>tool_reg</code>: (Methods on <code>Genie</code> instance)     *   <code>@genie_tooling.tool</code> (Decorator for functions. Auto-generates metadata: id, name, desc_human, desc_llm, input_schema, output_schema).     *   <code>await genie.register_tool_functions(functions:List[Callable])</code> (Registers decorated functions. Invalidates tool lookup index. Tools still need to be enabled in <code>tool_configurations</code> to be used by <code>execute_tool</code> or <code>run_command</code>). *   <code>prompts</code>: <code>PromptInterface</code>     *   <code>get_prompt_template_content(name:str, version?:str, registry_id?:str) -&gt; str?</code>     *   <code>render_prompt(name:str, data:PromptData, version?:str, registry_id?:str, template_engine_id?:str) -&gt; FormattedPrompt?</code>     *   <code>render_chat_prompt(name:str, data:PromptData, version?:str, registry_id?:str, template_engine_id?:str) -&gt; List[ChatMessage]?</code>     *   <code>list_templates(registry_id?:str) -&gt; List[PromptIdentifier]</code> *   <code>conversation</code>: <code>ConversationInterface</code>     *   <code>load_state(session_id:str, provider_id?:str) -&gt; ConversationState?</code>     *   <code>save_state(state:ConversationState, provider_id?:str)</code>     *   <code>add_message(session_id:str, message:ChatMessage, provider_id?:str)</code>     *   <code>delete_state(session_id:str, provider_id?:str) -&gt; bool</code> *   <code>observability</code>: <code>ObservabilityInterface</code>     *   <code>trace_event(event_name:str, data:Dict, component?:str, correlation_id?:str)</code> *   <code>human_in_loop</code>: <code>HITLInterface</code>     *   <code>request_approval(request:ApprovalRequest, approver_id?:str) -&gt; ApprovalResponse</code> *   <code>usage</code>: <code>UsageTrackingInterface</code>     *   <code>record_usage(record:TokenUsageRecord)</code>     *   <code>get_summary(recorder_id?:str, filter_criteria?:Dict) -&gt; Dict</code> *   <code>task_queue</code>: <code>TaskQueueInterface</code>     *   <code>submit_task(task_name:str, args?:Tuple, kwargs?:Dict, queue_id?:str, task_options?:Dict) -&gt; str?</code>     *   <code>get_task_status(task_id:str, queue_id?:str) -&gt; TaskStatus?</code>     *   <code>get_task_result(task_id:str, queue_id?:str, timeout_seconds?:float) -&gt; Any?</code>     *   <code>revoke_task(task_id:str, queue_id?:str, terminate?:bool) -&gt; bool</code> *   <code>teardown</code>: <code>await genie.close()</code> (Cleans up all managers and plugins)</p> <p>Agent Classes (in <code>genie_tooling.agents</code>): *   <code>BaseAgent(genie:Genie, agent_cfg?:Dict)</code>     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Abstract method) *   <code>ReActAgent(BaseAgent)</code>     *   <code>cfg</code>: <code>max_iterations:int</code> (Def:7), <code>system_prompt_id:str</code> (Def: <code>react_agent_system_prompt_v1</code>), <code>llm_provider_id:str?</code>, <code>tool_formatter_id:str</code> (Def: <code>compact_text_formatter_plugin_v1</code>), <code>stop_sequences:List[str]</code> (Def: <code>[\"Observation:\"]</code>), <code>llm_retry_attempts:int</code> (Def:1), <code>llm_retry_delay_seconds:float</code> (Def:2.0)     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Implements ReAct loop: Thought, Action, Observation) *   <code>PlanAndExecuteAgent(BaseAgent)</code>     *   <code>cfg</code>: <code>planner_system_prompt_id:str</code> (Def: <code>plan_and_execute_planner_prompt_v1</code>), <code>planner_llm_provider_id:str?</code>, <code>tool_formatter_id:str</code> (Def: <code>compact_text_formatter_plugin_v1</code>), <code>max_plan_retries:int</code> (Def:1), <code>max_step_retries:int</code> (Def:0), <code>replan_on_step_failure:bool</code> (Def:False)     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Implements Plan-then-Execute loop)</p> <p>Config: <code>genie_tooling.config.models.MiddlewareConfig</code> (<code>MWCfg</code>) *   <code>features: FeatureSettings</code> -&gt; <code>ConfigResolver</code> (<code>CfgResolver</code>) populates <code>MWCfg</code>.     *   <code>llm: Literal[\"ollama\", \"openai\", \"gemini\", \"llama_cpp\", \"llama_cpp_internal\", \"none\"]</code>     *   <code>llm_ollama_model_name: str?</code>     *   <code>llm_openai_model_name: str?</code>     *   <code>llm_gemini_model_name: str?</code>     *   <code>llm_llama_cpp_model_name: str?</code>     *   <code>llm_llama_cpp_base_url: str?</code>     *   <code>llm_llama_cpp_api_key_name: str?</code>     *   <code>llm_llama_cpp_internal_model_path: str?</code>     *   <code>llm_llama_cpp_internal_n_gpu_layers: int</code> (Def: 0)     *   <code>llm_llama_cpp_internal_n_ctx: int</code> (Def: 2048)     *   <code>llm_llama_cpp_internal_chat_format: str?</code>     *   <code>llm_llama_cpp_internal_model_name_for_logging: str?</code>     *   <code>cache: Literal[\"in-memory\", \"redis\", \"none\"]</code>     *   <code>cache_redis_url: str?</code>     *   <code>rag_embedder: Literal[\"sentence_transformer\", \"openai\", \"none\"]</code>     *   <code>rag_embedder_st_model_name: str?</code>     *   <code>rag_vector_store: Literal[\"faiss\", \"chroma\", \"qdrant\", \"none\"]</code>     *   <code>rag_vector_store_chroma_path: str?</code>     *   <code>rag_vector_store_chroma_collection_name: str?</code>     *   <code>rag_vector_store_qdrant_url: str?</code>     *   <code>rag_vector_store_qdrant_path: str?</code>     *   <code>rag_vector_store_qdrant_api_key_name: str?</code>     *   <code>rag_vector_store_qdrant_collection_name: str?</code>     *   <code>rag_vector_store_qdrant_embedding_dim: int?</code>     *   <code>tool_lookup: Literal[\"embedding\", \"keyword\", \"none\"]</code>     *   <code>tool_lookup_formatter_id_alias: str?</code>     *   <code>tool_lookup_chroma_path: str?</code>     *   <code>tool_lookup_chroma_collection_name: str?</code>     *   <code>tool_lookup_embedder_id_alias: str?</code>     *   <code>command_processor: Literal[\"llm_assisted\", \"simple_keyword\", \"none\"]</code>     *   <code>command_processor_formatter_id_alias: str?</code>     *   <code>observability_tracer: Literal[\"console_tracer\", \"otel_tracer\", \"none\"]</code>     *   <code>observability_otel_endpoint: str?</code>     *   <code>hitl_approver: Literal[\"cli_hitl_approver\", \"none\"]</code>     *   <code>token_usage_recorder: Literal[\"in_memory_token_recorder\", \"otel_metrics_recorder\", \"none\"]</code>     *   <code>input_guardrails: List[str]</code>     *   <code>output_guardrails: List[str]</code>     *   <code>tool_usage_guardrails: List[str]</code>     *   <code>prompt_registry: Literal[\"file_system_prompt_registry\", \"none\"]</code>     *   <code>prompt_template_engine: Literal[\"basic_string_formatter\", \"jinja2_chat_formatter\", \"none\"]</code>     *   <code>conversation_state_provider: Literal[\"in_memory_convo_provider\", \"redis_convo_provider\", \"none\"]</code>     *   <code>default_llm_output_parser: Literal[\"json_output_parser\", \"pydantic_output_parser\", \"none\"]</code>     *   <code>task_queue: Literal[\"celery\", \"rq\", \"none\"]</code>     *   <code>task_queue_celery_broker_url: str?</code>     *   <code>task_queue_celery_backend_url: str?</code> *   <code>tool_configurations: Dict[str_id_or_alias, Dict[str, Any]]</code> (Primary way to enable tools. Key presence enables tool. Value is tool-specific config.) *   <code>ConfigResolver</code> (<code>genie_tooling.config.resolver.py</code>): <code>features</code> + aliases -&gt; canonical IDs &amp; cfgs. <code>PLUGIN_ID_ALIASES</code> dict. *   <code>key_provider_id: str?</code> Def: <code>env_keys</code> if no <code>key_provider_instance</code>. *   <code>key_provider_instance: KeyProvider?</code> -&gt; Passed to <code>Genie.create()</code>. *   <code>*_configurations: Dict[str_id_or_alias, Dict[str, Any]]</code> (e.g., <code>llm_provider_configurations</code>). *   <code>plugin_dev_dirs: List[str]</code>.</p> <p>Plugins: <code>PluginManager</code>. IDs/paths: <code>pyproject.toml</code> -&gt; <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code>. Aliases: <code>genie_tooling.config.resolver.PLUGIN_ID_ALIASES</code>.</p> <p>Key Plugins (ID | Alias | Cfg/Notes): *   <code>KeyProv</code>: <code>environment_key_provider_v1</code>|<code>env_keys</code>. *   <code>LLMProv</code>:     *   <code>ollama_llm_provider_v1</code>|<code>ollama</code>. Cfg: <code>base_url</code>, <code>model_name</code>, <code>request_timeout_seconds</code>.     *   <code>openai_llm_provider_v1</code>|<code>openai</code>. Cfg: <code>model_name</code>, <code>api_key_name</code>, <code>openai_api_base</code>, <code>openai_organization</code>. Needs KP.     *   <code>gemini_llm_provider_v1</code>|<code>gemini</code>. Cfg: <code>model_name</code>, <code>api_key_name</code>, <code>system_instruction</code>, <code>safety_settings</code>. Needs KP.     *   <code>llama_cpp_llm_provider_v1</code>|<code>llama_cpp</code>. Cfg: <code>base_url</code>, <code>model_name</code>, <code>request_timeout_seconds</code>, <code>api_key_name</code>. Needs KP if <code>api_key_name</code> set.     *   <code>llama_cpp_internal_llm_provider_v1</code>|<code>llama_cpp_internal</code>. Cfg: <code>model_path</code>, <code>n_gpu_layers</code>, <code>n_ctx</code>, <code>chat_format</code>, <code>model_name_for_logging</code>, etc. Does not use KP for keys. *   <code>CmdProc</code>:     *   <code>simple_keyword_processor_v1</code>|<code>simple_keyword_cmd_proc</code>. Cfg: <code>keyword_map</code>, <code>keyword_priority</code>.     *   <code>llm_assisted_tool_selection_processor_v1</code>|<code>llm_assisted_cmd_proc</code>. Cfg: <code>llm_provider_id</code>, <code>tool_formatter_id</code>, <code>tool_lookup_top_k</code>, <code>system_prompt_template</code>, <code>max_llm_retries</code>. *   <code>Tools</code>: (Examples: <code>calculator_tool</code>, <code>sandboxed_fs_tool_v1</code>, etc. Must be listed in <code>tool_configurations</code> to be active.) *   <code>Observability</code>:     *   <code>console_tracer_plugin_v1</code>|<code>console_tracer</code>. Cfg: <code>log_level</code>.     *   <code>otel_tracer_plugin_v1</code>|<code>otel_tracer</code>. Cfg: <code>otel_service_name</code>, <code>exporter_type</code> (console, otlp_http, otlp_grpc), endpoints, headers, etc. *   <code>TokenUsage</code>:     *   <code>in_memory_token_usage_recorder_v1</code>|<code>in_memory_token_recorder</code>.     *   <code>otel_metrics_token_recorder_v1</code>|<code>otel_metrics_recorder</code>. Emits OTel metrics. *   <code>TaskQueues</code>:     *   <code>celery_task_queue_v1</code>|<code>celery_task_queue</code>. Cfg: <code>celery_app_name</code>, <code>celery_broker_url</code>, <code>celery_backend_url</code>.     *   <code>redis_queue_task_plugin_v1</code>|<code>rq_task_queue</code>. Cfg: <code>redis_url</code>, <code>default_queue_name</code>. *   (Other plugin categories like DefFormatters, RAG, ToolLookupProv, CodeExec, CacheProv, HITL, Guardrails, Prompts, Conversation, LLMOutputParsers, InvocationStrategies remain structurally similar but their instances are loaded based on configuration.)</p> <p>Types: *   <code>ChatMessage</code>: <code>{role:Literal[\"system\"|\"user\"|\"assistant\"|\"tool\"], content?:str, tool_calls?:List[ToolCall], tool_call_id?:str, name?:str}</code> *   <code>ToolCall</code>: <code>{id:str, type:Literal[\"function\"], function:{name:str, arguments:str_json}}</code> *   <code>LLMCompResp</code>: <code>{text:str, finish_reason?:str, usage?:LLMUsageInfo, raw_response:Any}</code> *   <code>LLMChatResp</code>: <code>{message:ChatMessage, finish_reason?:str, usage?:LLMUsageInfo, raw_response:Any}</code> *   <code>LLMUsageInfo</code>: <code>{prompt_tokens?:int, completion_tokens?:int, total_tokens?:int}</code> *   <code>LLMCompChunk</code>: <code>{text_delta?:str, finish_reason?:str, usage_delta?:LLMUsageInfo, raw_chunk:Any}</code> *   <code>LLMChatChunkDeltaMsg</code>: <code>{role?:\"assistant\", content?:str, tool_calls?:List[ToolCall]}</code> *   <code>LLMChatChunk</code>: <code>{message_delta?:LLMChatChunkDeltaMsg, finish_reason?:str, usage_delta?:LLMUsageInfo, raw_chunk:Any}</code> *   <code>CmdProcResp</code>: <code>{chosen_tool_id?:str, extracted_params?:Dict, llm_thought_process?:str, error?:str, raw_response?:Any}</code> *   <code>RetrievedChunk</code>: <code>{content:str, metadata:Dict, id?:str, score:float, rank?:int}</code> *   <code>CodeExecRes</code>: <code>(stdout:str, stderr:str, result?:Any, error?:str, exec_time_ms:float)</code> (NamedTuple) *   <code>PromptData</code>: <code>Dict[str,Any]</code> *   <code>FormattedPrompt</code>: <code>Union[str, List[ChatMessage]]</code> *   <code>PromptIdentifier</code>: <code>{name:str, version?:str, description?:str}</code> *   <code>ConversationState</code>: <code>{session_id:str, history:List[ChatMessage], metadata?:Dict}</code> *   <code>TraceEvent</code>: <code>{event_name:str, data:Dict, timestamp:float, component?:str, correlation_id?:str}</code> *   <code>ApprovalRequest</code>: <code>{request_id:str, prompt:str, data_to_approve:Dict, context?:Dict, timeout_seconds?:int}</code> *   <code>ApprovalResponse</code>: <code>{request_id:str, status:ApprovalStatus, approver_id?:str, reason?:str, timestamp?:float}</code> *   <code>ApprovalStatus: Literal[\"pending\"|\"approved\"|\"denied\"|\"timeout\"|\"error\"]</code> *   <code>TokenUsageRecord</code>: <code>{provider_id:str, model_name:str, prompt_tokens?:int, completion_tokens?:int, total_tokens?:int, timestamp:float, call_type?:str, user_id?:str, session_id?:str, custom_tags?:dict}</code> *   <code>GuardrailViolation</code>: <code>{action:Literal[\"allow\"|\"block\"|\"warn\"], reason?:str, guardrail_id?:str, details?:Dict}</code> *   <code>ParsedOutput</code>: <code>Any</code> *   <code>AgentOutput</code>: <code>{status:Literal[\"success\"|\"error\"|\"max_iterations_reached\"|\"user_stopped\"], output:Any, history?:List[Any], plan?:List[PlannedStep]}</code> *   <code>PlannedStep</code>: <code>{step_number:int, tool_id:str, params:Dict, reasoning?:str}</code> *   <code>ReActObservation</code>: <code>{thought:str, action:str, observation:str}</code> *   <code>TaskStatus: Literal[\"pending\", \"running\", \"success\", \"failure\", \"revoked\", \"unknown\"]</code></p>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/","title":"LLM refactor guide","text":""},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#genie-tooling-refactoring-migration-guide-for-llms","title":"Genie Tooling Refactoring &amp; Migration Guide for LLMs","text":"<p>Primary LLM Directive: Your goal is to analyze an existing Python codebase and systematically refactor its components to utilize the <code>genie-tooling</code> middleware. The primary interaction point will be the <code>genie_tooling.genie.Genie</code> facade. Prioritize modularity, configurability, and leveraging Genie's built-in plugin ecosystem. Key Change: Tools are now explicitly enabled via <code>MiddlewareConfig.tool_configurations</code>.</p> <p>Core Reference Documents for LLM: 1.  <code>docs/LLM_code_assist_instructions/LLM_api_guide.md</code>: This is your primary technical schema for Genie Tooling's API, plugin types, configuration structure, and core concepts. Internalize this deeply, noting the change in tool enablement and new features like the internal Llama.cpp provider and distributed task queues. 2.  <code>README.md</code>: For high-level vision, core concepts (Genie facade, plugins, <code>@tool</code>, explicit tool enablement), and quick start examples. 3.  <code>pyproject.toml</code> (<code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> section): Canonical source for plugin IDs and their Python module paths. 4.  <code>src/genie_tooling/config/resolver.py</code> (<code>PLUGIN_ID_ALIASES</code> dict): Mapping of user-friendly aliases to canonical plugin IDs. 5.  <code>examples/</code> directory: Practical usage patterns and configuration examples, now reflecting explicit tool enablement and new features. 6.  <code>docs/guides/</code>: Detailed human-readable guides for specific features (e.g., RAG, LLM Providers, Configuration, Using Tools, Distributed Tasks, Observability).</p>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-0-deep-ingestion-semantic-understanding-of-genie-tooling","title":"Phase 0: Deep Ingestion &amp; Semantic Understanding of Genie Tooling","text":"<p>LLM Instructions:</p> <ol> <li>Master <code>docs/LLM_code_assist_instructions/LLM_api_guide.md</code>: This document is specifically structured for your consumption. Treat it as the ground truth for Genie's API, available plugin types, configuration parameters (via <code>MiddlewareConfig</code> and <code>FeatureSettings</code>), and core data structures (e.g., <code>ChatMessage</code>, <code>ToolCall</code>, <code>AgentOutput</code>). Pay special attention to the <code>tool_configurations</code> field in <code>MiddlewareConfig</code> as the method for enabling tools, and note the new <code>llama_cpp_internal</code> LLM provider and <code>task_queue</code> interface.</li> <li>Internalize Core Concepts (<code>README.md</code>):<ul> <li><code>Genie</code> Facade: Understand its role as the central interaction point.</li> <li>Plugin Architecture: Grasp that most functionalities are swappable plugins.</li> <li>Explicit Tool Enablement: Recognize that tools are only loaded if their ID is a key in <code>MiddlewareConfig.tool_configurations</code>.</li> <li>Managers: Note their role in orchestrating plugin types (though direct interaction is rare when using the facade).</li> <li><code>@tool</code> Decorator: Recognize its utility for quickly turning Python functions into tools. These decorated tools also need to be enabled via <code>tool_configurations</code> after registration.</li> <li>Configuration (<code>MiddlewareConfig</code>, <code>FeatureSettings</code>, <code>ConfigResolver</code>): Understand the layered configuration approach.</li> </ul> </li> <li>Plugin Identification &amp; Aliasing:<ul> <li>Correlate plugin IDs from <code>pyproject.toml</code> with aliases in <code>PLUGIN_ID_ALIASES</code> from <code>resolver.py</code>.</li> </ul> </li> <li>Facade &amp; Interface Study:<ul> <li>Analyze <code>src/genie_tooling/genie.py</code> to see how the <code>Genie</code> class initializes and exposes its sub-interfaces, including the new <code>TaskQueueInterface</code>.</li> <li>Thoroughly review <code>src/genie_tooling/interfaces.py</code>.</li> </ul> </li> <li>Practical Application Patterns:<ul> <li>Examine all files in the <code>examples/</code> directory. Pay close attention to how <code>MiddlewareConfig</code> (especially <code>tool_configurations</code>) and <code>FeatureSettings</code> are used, including examples for new features like <code>E17_observability_tracing_example.py</code>, <code>E20_token_usage_example.py</code>, and <code>E22_distributed_task_example.py</code>.</li> </ul> </li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-1-target-project-analysis-genie-capability-mapping","title":"Phase 1: Target Project Analysis &amp; Genie Capability Mapping","text":"<p>LLM Instructions:</p> <ol> <li>Deconstruct Target Project: (Same as before)</li> <li>Create a Refactoring Map: (Same as before, but when mapping to a Genie Tool, note that it will need to be added to <code>tool_configurations</code>).</li> <li>Assess API Key Handling: (Same as before)</li> <li>Identify Custom Plugin Candidates: (Same as before)</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-2-core-genie-integration-configuration-setup","title":"Phase 2: Core Genie Integration &amp; Configuration Setup","text":"<p>LLM Instructions:</p> <ol> <li>Add Dependency: (Same as before)</li> <li>Design <code>MiddlewareConfig</code>:<ul> <li>Create a central configuration point.</li> <li>Instantiate <code>genie_tooling.config.models.MiddlewareConfig</code>.</li> <li>Prioritize using <code>genie_tooling.config.features.FeatureSettings</code>.</li> <li>Crucially, populate <code>app_config.tool_configurations</code> with entries for every tool that needs to be active. For tools without specific settings, an empty dictionary is sufficient (e.g., <code>app_config.tool_configurations = {\"calculator_tool\": {}}</code>).<ul> <li>Example:     <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_features = FeatureSettings(\n    llm=\"ollama\", \n    # ... other features like task_queue=\"celery\" ...\n)\napp_config = MiddlewareConfig(\n    features=app_features,\n    tool_configurations={\n        \"calculator_tool\": {}, # Enable calculator\n        \"my_existing_api_tool_id\": {\"api_base_url\": \"https://service.com/api\"}, # Enable &amp; configure\n        \"another_simple_tool_id\": {} \n    }\n)\n</code></pre></li> </ul> </li> <li>For functionalities not covered by <code>FeatureSettings</code> or requiring specific overrides, populate the relevant <code>*_configurations</code> dictionaries (e.g., <code>llm_provider_configurations</code>, <code>distributed_task_queue_configurations</code>).</li> <li>If custom plugins reside in project-specific directories, add these paths to <code>app_config.plugin_dev_dirs</code>.</li> </ul> </li> <li>Implement/Configure <code>KeyProvider</code>: (Same as before)</li> <li>Instantiate <code>Genie</code> Facade: (Same as before)</li> <li>Integrate Teardown: (Same as before)</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-3-iterative-refactoring-of-mapped-components","title":"Phase 3: Iterative Refactoring of Mapped Components","text":"<p>LLM Instructions: Systematically replace existing functionalities with their Genie Tooling equivalents.</p> <ol> <li>LLM Interactions (<code>genie.llm</code>): (Same as before. Consider <code>llama_cpp_internal</code> for local execution.)</li> <li>Tool Definition &amp; Execution (<code>@tool</code>, <code>genie.execute_tool</code>, <code>genie.run_command</code>):<ul> <li>Refactor Functions to Tools: Apply <code>@genie_tooling.tool</code>.</li> <li>Register Tools: Call <code>await genie.register_tool_functions([...])</code>.</li> <li>Enable Registered Tools: Ensure the identifiers of these registered tools are added as keys to <code>MiddlewareConfig.tool_configurations</code> (e.g., <code>{\"my_decorated_function_name\": {}}</code>).</li> <li>Replace Direct Calls: Change to <code>await genie.execute_tool(\"tool_name_as_string\", ...)</code>.</li> <li>Refactor Command Parsing: Replace with <code>await genie.run_command(user_query_string)</code>. Ensure any tools the command processor might select are enabled in <code>tool_configurations</code>.</li> </ul> </li> <li>Command Processing (<code>genie.run_command</code>):<ul> <li>(Same as before, but reiterate that any tools the processor might select must be enabled in <code>tool_configurations</code>).</li> </ul> </li> <li>RAG Pipeline (<code>genie.rag</code>): (Same as before)</li> <li>Prompt Management (<code>genie.prompts</code>): (Same as before)</li> <li>Conversation State (<code>genie.conversation</code>): (Same as before)</li> <li>Observability (<code>genie.observability</code>): (Same as before, noting OpenTelemetry support)</li> <li>Human-in-the-Loop (<code>genie.human_in_loop</code>): (Same as before)</li> <li>Token Usage Tracking (<code>genie.usage</code>): (Same as before, noting OpenTelemetry metrics support)</li> <li>Guardrails: (Same as before)</li> <li>LLM Output Parsing (<code>genie.llm.parse_output</code>): (Same as before)</li> <li>Agentic Loops (<code>genie_tooling.agents</code>):<ul> <li>(Same as before, ensuring any tools used by the agent are enabled in <code>tool_configurations</code>).</li> </ul> </li> <li>Distributed Task Offloading (<code>genie.task_queue</code>):<ul> <li>Replace direct calls to Celery/RQ client libraries with <code>await genie.task_queue.submit_task(...)</code>, <code>await genie.task_queue.get_task_status(...)</code>, etc.</li> <li>If the remote task involves executing a Genie tool, that tool must be enabled in the worker's Genie configuration.</li> <li>Consider if the <code>DistributedTaskInvocationStrategy</code> can simplify tool offloading, if applicable and configured.</li> </ul> </li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-4-testing-validation-and-refinement","title":"Phase 4: Testing, Validation, and Refinement","text":"<p>LLM Instructions:</p> <ol> <li>Unit Tests: (Same as before)</li> <li>Integration Tests:<ul> <li>Ensure <code>MiddlewareConfig</code> in tests correctly enables all necessary tools via <code>tool_configurations</code>.</li> <li>Test new features like task queue interactions and internal Llama.cpp provider usage.</li> </ul> </li> <li>Behavioral Verification: (Same as before)</li> <li>Configuration Robustness: (Same as before)</li> <li>Observability Review: (Same as before, check OTel traces/metrics if used)</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-5-finalization-documentation-code-cleanup","title":"Phase 5: Finalization - Documentation &amp; Code Cleanup","text":"<p>LLM Instructions:</p> <ol> <li>Update Project Documentation:<ul> <li>Clearly document that tools must be enabled via <code>MiddlewareConfig.tool_configurations</code>.</li> <li>Update guides for new features: Llama.cpp internal provider, distributed task queues (Celery &amp; RQ), OpenTelemetry tracing and metrics.</li> </ul> </li> <li>Code Cleanup: (Same as before)</li> <li>Configuration Review:<ul> <li>Verify that <code>tool_configurations</code> correctly lists all active tools.</li> </ul> </li> <li>Update <code>LLM_api_guide.md</code> (If Genie Tooling Itself Was Modified):<ul> <li>CRITICAL LLM SELF-REFLECTION: (Same as before. Ensure the API guide reflects the explicit tool enablement via <code>tool_configurations</code> and all new features like <code>llama_cpp_internal</code> and <code>task_queue</code> interface.)</li> </ul> </li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the auto-generated API documentation for Genie Tooling.</p> <p>Explore the modules and classes below for detailed information.</p>"},{"location":"api/#core","title":"Core","text":"<ul> <li>Genie Facade &amp; Interfaces</li> <li>Decorators (@tool)</li> <li>Core Types </li> <li>Plugin Manager</li> </ul>"},{"location":"api/#configuration","title":"Configuration","text":"<ul> <li>MiddlewareConfig</li> <li>FeatureSettings</li> </ul>"},{"location":"api/#plugins-protocols-implementations","title":"Plugins (Protocols &amp; Implementations)","text":"<p>This section can be expanded by <code>mkdocstrings</code> or by manually linking to specific plugin protocol documentation pages. Key plugin categories include: *   Tools *   LLM Providers *   Command Processors *   RAG Components (Loaders, Splitters, Embedders, Vector Stores, Retrievers) *   Tool Lookup Providers *   Definition Formatters *   Caching Providers *   Key Providers *   Invocation Strategies *   Error Handling *   Logging &amp; Redaction *   Code Executors *   Prompt System Plugins (Registries, Templates) *   Conversation State Providers *   Observability Tracers *   HITL Approvers *   Token Usage Recorders *   Guardrail Plugins *   LLM Output Parsers</p>"},{"location":"api/#genie_tooling","title":"genie_tooling","text":""},{"location":"api/#genie_tooling--my-agentic-middleware-library","title":"My Agentic Middleware Library","text":"<p>A hyper-pluggable Python middleware for Agentic AI and LLM applications. Async-first for performance.</p>"},{"location":"api/#genie_tooling-classes","title":"Classes","text":""},{"location":"api/#genie_tooling.BaseAgent","title":"BaseAgent","text":"<pre><code>BaseAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for agents that implement specific execution patterns (e.g., ReAct, Plan-and-Execute) using the Genie facade.</p> <p>Initializes the BaseAgent.</p> <p>Parameters:</p> Name Type Description Default <code>genie</code> <code>Genie</code> <p>An initialized instance of the Genie facade.</p> required <code>agent_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary for agent-specific configurations.</p> <code>None</code> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initializes the BaseAgent.\n\n    Args:\n        genie: An initialized instance of the Genie facade.\n        agent_config: Optional dictionary for agent-specific configurations.\n    \"\"\"\n    if not genie:\n        raise ValueError(\"A Genie instance is required to initialize an agent.\")\n    self.genie = genie\n    self.agent_config = agent_config or {}\n    logger.info(f\"Initialized {self.__class__.__name__} with Genie instance and config: {self.agent_config}\")\n</code></pre>"},{"location":"api/#genie_tooling.BaseAgent-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.BaseAgent.run","title":"run  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>run(goal: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>The main entry point to execute the agent's logic for a given goal.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The high-level goal or task for the agent to accomplish.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional runtime parameters specific to the agent's execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The final result or outcome of the agent's execution.</p> <code>Any</code> <p>The structure of the result is specific to the agent implementation.</p> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>@abc.abstractmethod\nasync def run(self, goal: str, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    The main entry point to execute the agent's logic for a given goal.\n\n    Args:\n        goal: The high-level goal or task for the agent to accomplish.\n        **kwargs: Additional runtime parameters specific to the agent's execution.\n\n    Returns:\n        The final result or outcome of the agent's execution.\n        The structure of the result is specific to the agent implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.BaseAgent.teardown","title":"teardown  <code>async</code>","text":"<pre><code>teardown() -&gt; None\n</code></pre> <p>Optional method for any cleanup specific to the agent. The Genie instance itself should be torn down separately by the application.</p> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>async def teardown(self) -&gt; None:\n    \"\"\"\n    Optional method for any cleanup specific to the agent.\n    The Genie instance itself should be torn down separately by the application.\n    \"\"\"\n    logger.info(f\"{self.__class__.__name__} teardown initiated.\")\n    # Add any agent-specific cleanup here if needed in the future.\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.PlanAndExecuteAgent","title":"PlanAndExecuteAgent","text":"<pre><code>PlanAndExecuteAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseAgent</code></p> <p>Implements the Plan-and-Execute agentic loop. 1. Planner: LLM generates a sequence of steps (tool calls) to achieve the goal. 2. Executor: Executes these steps sequentially.</p> Source code in <code>src/genie_tooling/agents/plan_and_execute_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    super().__init__(genie, agent_config)\n    self.planner_system_prompt_id = self.agent_config.get(\"planner_system_prompt_id\", DEFAULT_PLANNER_SYSTEM_PROMPT_ID)\n    self.planner_llm_provider_id = self.agent_config.get(\"planner_llm_provider_id\")\n    self.tool_formatter_id = self.agent_config.get(\"tool_formatter_id\", \"compact_text_formatter_plugin_v1\")\n    self.max_plan_retries = self.agent_config.get(\"max_plan_retries\", 1)\n    self.max_step_retries = self.agent_config.get(\"max_step_retries\", 0) # Default: no retry on step failure\n    self.replan_on_step_failure = self.agent_config.get(\"replan_on_step_failure\", False)\n\n    logger.info(\n        f\"PlanAndExecuteAgent initialized. Planner Prompt ID: {self.planner_system_prompt_id}, \"\n        f\"Planner LLM: {self.planner_llm_provider_id or 'Genie Default'}\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.ReActAgent","title":"ReActAgent","text":"<pre><code>ReActAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseAgent</code></p> <p>Implements the ReAct (Reason-Act) agentic loop. The agent iteratively reasons about the goal, decides on an action (tool use), executes the action, observes the result, and repeats until the goal is achieved or a stop condition is met.</p> Source code in <code>src/genie_tooling/agents/react_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    super().__init__(genie, agent_config)\n    self.max_iterations = self.agent_config.get(\"max_iterations\", DEFAULT_REACT_MAX_ITERATIONS)\n    self.system_prompt_id = self.agent_config.get(\"system_prompt_id\", DEFAULT_REACT_SYSTEM_PROMPT_ID)\n    self.llm_provider_id = self.agent_config.get(\"llm_provider_id\") # Optional, uses Genie default if None\n    self.tool_formatter_id = self.agent_config.get(\"tool_formatter_id\", \"compact_text_formatter_plugin_v1\")\n    self.stop_sequences = self.agent_config.get(\"stop_sequences\", [\"Observation:\"])\n    self.llm_retry_attempts = self.agent_config.get(\"llm_retry_attempts\", 1)\n    self.llm_retry_delay = self.agent_config.get(\"llm_retry_delay_seconds\", 2.0)\n\n    logger.info(\n        f\"ReActAgent initialized. Max iterations: {self.max_iterations}, \"\n        f\"System Prompt ID: {self.system_prompt_id}, LLM Provider: {self.llm_provider_id or 'Genie Default'}\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.AgentOutput","title":"AgentOutput","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized output structure from an agent's run method.</p>"},{"location":"api/#genie_tooling.PlannedStep","title":"PlannedStep","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single step in a generated plan.</p>"},{"location":"api/#genie_tooling.ReActObservation","title":"ReActObservation","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents one cycle of Thought-Action-Observation in ReAct.</p>"},{"location":"api/#genie_tooling.CacheProviderPlugin","title":"CacheProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a cache provider, designed for async operations.</p>"},{"location":"api/#genie_tooling.CacheProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CacheProviderPlugin.get","title":"get  <code>async</code>","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Retrieves an item from the cache. Args:     key: The key of the item to retrieve. Returns:     The cached item, or None if the key is not found or item is expired.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def get(self, key: str) -&gt; Optional[Any]:\n    \"\"\"\n    Retrieves an item from the cache.\n    Args:\n        key: The key of the item to retrieve.\n    Returns:\n        The cached item, or None if the key is not found or item is expired.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' get method not fully implemented.\")\n    return None\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.set","title":"set  <code>async</code>","text":"<pre><code>set(key: str, value: Any, ttl_seconds: Optional[int] = None) -&gt; None\n</code></pre> <p>Stores an item in the cache. Args:     key: The key under which to store the item.     value: The item to store. Should be serializable if cache is external.     ttl_seconds: Optional time-to-live in seconds. If None, item may persist indefinitely                  or use a default TTL defined by the cache provider.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -&gt; None:\n    \"\"\"\n    Stores an item in the cache.\n    Args:\n        key: The key under which to store the item.\n        value: The item to store. Should be serializable if cache is external.\n        ttl_seconds: Optional time-to-live in seconds. If None, item may persist indefinitely\n                     or use a default TTL defined by the cache provider.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' set method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(key: str) -&gt; bool\n</code></pre> <p>Deletes an item from the cache. Args:     key: The key of the item to delete. Returns:     True if the key existed and was deleted, False otherwise.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def delete(self, key: str) -&gt; bool:\n    \"\"\"\n    Deletes an item from the cache.\n    Args:\n        key: The key of the item to delete.\n    Returns:\n        True if the key existed and was deleted, False otherwise.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' delete method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.exists","title":"exists  <code>async</code>","text":"<pre><code>exists(key: str) -&gt; bool\n</code></pre> <p>Checks if a key exists in the cache (and is not expired). Args:     key: The key to check. Returns:     True if the key exists and is valid, False otherwise.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def exists(self, key: str) -&gt; bool:\n    \"\"\"\n    Checks if a key exists in the cache (and is not expired).\n    Args:\n        key: The key to check.\n    Returns:\n        True if the key exists and is valid, False otherwise.\n    \"\"\"\n    # Default implementation using get()\n    logger.debug(f\"CacheProvider '{self.plugin_id}' exists method using default get() check.\")\n    return await self.get(key) is not None\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.clear_all","title":"clear_all  <code>async</code>","text":"<pre><code>clear_all() -&gt; bool\n</code></pre> <p>Clears all items from the cache managed by this provider. Use with caution, especially for shared caches. Returns:     True if the clear operation was successful or attempted, False on failure.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def clear_all(self) -&gt; bool:\n    \"\"\"\n    Clears all items from the cache managed by this provider.\n    Use with caution, especially for shared caches.\n    Returns:\n        True if the clear operation was successful or attempted, False on failure.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' clear_all method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.CodeExecutionResult","title":"CodeExecutionResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Standardized result structure for code execution.</p>"},{"location":"api/#genie_tooling.CodeExecutorPlugin","title":"CodeExecutorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that executes code, ideally in a sandboxed environment.</p>"},{"location":"api/#genie_tooling.CodeExecutorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CodeExecutorPlugin.execute_code","title":"execute_code  <code>async</code>","text":"<pre><code>execute_code(\n    language: str,\n    code: str,\n    timeout_seconds: int,\n    input_data: Optional[Dict[str, Any]] = None,\n) -&gt; CodeExecutionResult\n</code></pre> <p>Asynchronously executes the provided code string in the specified language. Implementations should strive for secure execution (sandboxing). Args:     language: The programming language of the code.     code: The code script to execute.     timeout_seconds: Maximum allowed execution time in seconds.     input_data: Optional dictionary to make available as variables within the code's scope                 (e.g., as a global dict named <code>_input</code> or similar, executor-defined). Returns:     A CodeExecutionResult NamedTuple.</p> Source code in <code>src/genie_tooling/code_executors/abc.py</code> <pre><code>async def execute_code(\n    self,\n    language: str,\n    code: str,\n    timeout_seconds: int,\n    input_data: Optional[Dict[str, Any]] = None # For passing structured input to the code's execution scope\n) -&gt; CodeExecutionResult:\n    \"\"\"\n    Asynchronously executes the provided code string in the specified language.\n    Implementations should strive for secure execution (sandboxing).\n    Args:\n        language: The programming language of the code.\n        code: The code script to execute.\n        timeout_seconds: Maximum allowed execution time in seconds.\n        input_data: Optional dictionary to make available as variables within the code's scope\n                    (e.g., as a global dict named `_input` or similar, executor-defined).\n    Returns:\n        A CodeExecutionResult NamedTuple.\n    \"\"\"\n    logger.warning(f\"CodeExecutor '{self.plugin_id}' execute_code method not fully implemented.\")\n    return CodeExecutionResult(stdout=\"\", stderr=\"Not implemented\", result=None, error=\"Executor not implemented\", execution_time_ms=0.0)\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorPlugin","title":"CommandProcessorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that processes a natural language command to determine which tool to use and what parameters to pass to it.</p>"},{"location":"api/#genie_tooling.CommandProcessorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CommandProcessorPlugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]]) -&gt; None\n</code></pre> <p>Initializes the command processor. Args:     config: Processor-specific configuration dictionary. Expected to contain             'genie_facade: Genie' for accessing other middleware components.</p> Source code in <code>src/genie_tooling/command_processors/abc.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Initializes the command processor.\n    Args:\n        config: Processor-specific configuration dictionary. Expected to contain\n                'genie_facade: Genie' for accessing other middleware components.\n    \"\"\"\n    # Example:\n    # self._genie_facade = config.get(\"genie_facade\")\n    # if not isinstance(self._genie_facade, Genie): # Assuming Genie type is available for check\n    #    logger.error(f\"{self.plugin_id}: Genie facade not found in config or invalid.\")\n    await super().setup(config) # Call Plugin's default setup\n    logger.debug(f\"CommandProcessorPlugin '{self.plugin_id}': Base setup logic (if any) completed.\")\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorPlugin.process_command","title":"process_command  <code>async</code>","text":"<pre><code>process_command(\n    command: str, conversation_history: Optional[List[ChatMessage]] = None\n) -&gt; CommandProcessorResponse\n</code></pre> <p>Processes the given command, potentially using conversation history and other Genie components (via the facade provided in setup), to decide on a tool and its parameters.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The natural language command string from the user.</p> required <code>conversation_history</code> <code>Optional[List[ChatMessage]]</code> <p>Optional list of previous ChatMessages in the conversation.</p> <code>None</code> <p>Returns:</p> Type Description <code>CommandProcessorResponse</code> <p>A CommandProcessorResponse dictionary.</p> Source code in <code>src/genie_tooling/command_processors/abc.py</code> <pre><code>async def process_command(\n    self,\n    command: str,\n    conversation_history: Optional[List[ChatMessage]] = None\n) -&gt; CommandProcessorResponse:\n    \"\"\"\n    Processes the given command, potentially using conversation history and\n    other Genie components (via the facade provided in setup), to decide\n    on a tool and its parameters.\n\n    Args:\n        command: The natural language command string from the user.\n        conversation_history: Optional list of previous ChatMessages in the conversation.\n\n    Returns:\n        A CommandProcessorResponse dictionary.\n    \"\"\"\n    logger.error(f\"CommandProcessorPlugin '{self.plugin_id}' process_command method not implemented.\")\n    raise NotImplementedError(f\"CommandProcessorPlugin '{self.plugin_id}' does not implement 'process_command'.\")\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorResponse","title":"CommandProcessorResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response from a CommandProcessorPlugin.</p>"},{"location":"api/#genie_tooling.PluginManager","title":"PluginManager","text":"<pre><code>PluginManager(plugin_dev_dirs: Optional[List[str]] = None)\n</code></pre> <p>Manages discovery, loading, and access to plugins. Implements Hybrid: Entry Points + Configured Dev Directory discovery.</p> Source code in <code>src/genie_tooling/core/plugin_manager.py</code> <pre><code>def __init__(self, plugin_dev_dirs: Optional[List[str]] = None):\n    self.plugin_dev_dirs = [Path(p).resolve() for p in plugin_dev_dirs] if plugin_dev_dirs else []\n    self._plugin_instances: Dict[str, Plugin] = {}\n    self._discovered_plugin_classes: Dict[str, Type[Plugin]] = {}\n    self._plugin_source_map: Dict[str, str] = {}\n    logger.debug(f\"PluginManager initialized. Dev dirs: {self.plugin_dev_dirs}\")\n</code></pre>"},{"location":"api/#genie_tooling.Chunk","title":"Chunk","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a chunk of a document after splitting.</p>"},{"location":"api/#genie_tooling.Document","title":"Document","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a loaded document before splitting.</p>"},{"location":"api/#genie_tooling.Plugin","title":"Plugin","text":"<p>               Bases: <code>Protocol</code></p> <p>Base protocol for all plugins.</p>"},{"location":"api/#genie_tooling.Plugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.Plugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Optional asynchronous setup method for plugins. Called after instantiation.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Optional asynchronous setup method for plugins. Called after instantiation.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.Plugin.teardown","title":"teardown  <code>async</code>","text":"<pre><code>teardown() -&gt; None\n</code></pre> <p>Optional asynchronous teardown method for plugins. Called before application shutdown.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def teardown(self) -&gt; None:\n    \"\"\"Optional asynchronous teardown method for plugins. Called before application shutdown.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.RetrievedChunk","title":"RetrievedChunk","text":"<p>               Bases: <code>Chunk</code>, <code>Protocol</code></p> <p>Represents a chunk retrieved from a vector store, with a relevance score.</p>"},{"location":"api/#genie_tooling.StructuredError","title":"StructuredError","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized structure for reporting errors, especially to LLMs.</p>"},{"location":"api/#genie_tooling.DefinitionFormatterPlugin","title":"DefinitionFormatterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that formats a tool's metadata into a specific structure (e.g., for LLMs, for human readability).</p>"},{"location":"api/#genie_tooling.DefinitionFormatterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DefinitionFormatterPlugin.format","title":"format","text":"<pre><code>format(tool_metadata: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Takes the comprehensive metadata from Tool.get_metadata() and transforms it into the specific output format.</p> <p>Parameters:</p> Name Type Description Default <code>tool_metadata</code> <code>Dict[str, Any]</code> <p>The raw metadata dictionary from a Tool instance.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted definition (e.g., a dict for JSON, a string for text).</p> <code>Any</code> <p>The type <code>Any</code> allows flexibility for various output formats.</p> Source code in <code>src/genie_tooling/definition_formatters/abc.py</code> <pre><code>def format(self, tool_metadata: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Takes the comprehensive metadata from Tool.get_metadata()\n    and transforms it into the specific output format.\n\n    Args:\n        tool_metadata: The raw metadata dictionary from a Tool instance.\n\n    Returns:\n        The formatted definition (e.g., a dict for JSON, a string for text).\n        The type `Any` allows flexibility for various output formats.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.DocumentLoaderPlugin","title":"DocumentLoaderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Loads documents from a source into an async stream of Document objects.</p>"},{"location":"api/#genie_tooling.DocumentLoaderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DocumentLoaderPlugin.load","title":"load  <code>async</code>","text":"<pre><code>load(\n    source_uri: str, config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Document]\n</code></pre> <p>Loads documents from the given source URI. Args:     source_uri: The URI of the data source (e.g., file path, URL, database connection string).     config: Loader-specific configuration dictionary. Yields:     Document objects.</p> Source code in <code>src/genie_tooling/document_loaders/abc.py</code> <pre><code>async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]:\n    \"\"\"\n    Loads documents from the given source URI.\n    Args:\n        source_uri: The URI of the data source (e.g., file path, URL, database connection string).\n        config: Loader-specific configuration dictionary.\n    Yields:\n        Document objects.\n    \"\"\"\n    # Example of how to make an async generator that does nothing if not implemented:\n    logger.warning(f\"DocumentLoaderPlugin '{self.plugin_id}' load method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin","title":"EmbeddingGeneratorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Generates embeddings for an async stream of Chunks.</p>"},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin.embed","title":"embed  <code>async</code>","text":"<pre><code>embed(\n    chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]\n</code></pre> <p>Generates embeddings for each chunk. Args:     chunks: An async iterable of Chunk objects.     config: Embedder-specific configuration (e.g., model_name, batch_size, key_provider). Yields:     Tuples of (Chunk, EmbeddingVector).</p> Source code in <code>src/genie_tooling/embedding_generators/abc.py</code> <pre><code>async def embed(self, chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]:\n    \"\"\"\n    Generates embeddings for each chunk.\n    Args:\n        chunks: An async iterable of Chunk objects.\n        config: Embedder-specific configuration (e.g., model_name, batch_size, key_provider).\n    Yields:\n        Tuples of (Chunk, EmbeddingVector).\n    \"\"\"\n    logger.warning(f\"EmbeddingGeneratorPlugin '{self.plugin_id}' embed method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.ErrorFormatter","title":"ErrorFormatter","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for formatting StructuredError for different consumers (e.g., LLM, logs).</p>"},{"location":"api/#genie_tooling.ErrorFormatter-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ErrorFormatter.format","title":"format","text":"<pre><code>format(structured_error: StructuredError, target_format: str = 'llm') -&gt; Any\n</code></pre> <p>Formats the structured_error. Args:     structured_error: The error dictionary produced by an ErrorHandler.     target_format: A hint for the desired output format (e.g., \"llm\", \"json\", \"human_log\").                    Default is \"llm\". Returns:     The formatted error (e.g., a string for LLM, a dict for JSON). This method is synchronous.</p> Source code in <code>src/genie_tooling/error_formatters/abc.py</code> <pre><code>def format(self, structured_error: StructuredError, target_format: str = \"llm\") -&gt; Any:\n    \"\"\"\n    Formats the structured_error.\n    Args:\n        structured_error: The error dictionary produced by an ErrorHandler.\n        target_format: A hint for the desired output format (e.g., \"llm\", \"json\", \"human_log\").\n                       Default is \"llm\".\n    Returns:\n        The formatted error (e.g., a string for LLM, a dict for JSON).\n    This method is synchronous.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ErrorHandler","title":"ErrorHandler","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for handling exceptions during tool execution and converting them to StructuredError.</p>"},{"location":"api/#genie_tooling.ErrorHandler-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ErrorHandler.handle","title":"handle","text":"<pre><code>handle(\n    exception: Exception, tool: Any, context: Optional[Dict[str, Any]]\n) -&gt; StructuredError\n</code></pre> <p>Handles an exception that occurred during tool.execute() or related steps. Args:     exception: The exception instance caught.     tool: The Tool instance that was being executed (typed as Any to avoid circular import).           Implementations can cast or use getattr to access tool.identifier.     context: Optional context dictionary active during the call. Returns:     A StructuredError dictionary. This method is synchronous as error classification is typically CPU-bound.</p> Source code in <code>src/genie_tooling/error_handlers/abc.py</code> <pre><code>def handle(self, exception: Exception, tool: Any, context: Optional[Dict[str, Any]]) -&gt; StructuredError:\n    \"\"\"\n    Handles an exception that occurred during tool.execute() or related steps.\n    Args:\n        exception: The exception instance caught.\n        tool: The Tool instance that was being executed (typed as Any to avoid circular import).\n              Implementations can cast or use getattr to access tool.identifier.\n        context: Optional context dictionary active during the call.\n    Returns:\n        A StructuredError dictionary.\n    This method is synchronous as error classification is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.GuardrailPlugin","title":"GuardrailPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Base protocol for all guardrail plugins.</p>"},{"location":"api/#genie_tooling.InputGuardrailPlugin","title":"InputGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check input data (e.g., prompts, user messages).</p>"},{"location":"api/#genie_tooling.InputGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InputGuardrailPlugin.check_input","title":"check_input  <code>async</code>","text":"<pre><code>check_input(data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation\n</code></pre> <p>Checks input data. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_input(self, data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks input data.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"InputGuardrailPlugin '{self.plugin_id}' check_input method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.OutputGuardrailPlugin","title":"OutputGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check output data (e.g., LLM responses, tool results).</p>"},{"location":"api/#genie_tooling.OutputGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.OutputGuardrailPlugin.check_output","title":"check_output  <code>async</code>","text":"<pre><code>check_output(data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation\n</code></pre> <p>Checks output data. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_output(self, data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks output data.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"OutputGuardrailPlugin '{self.plugin_id}' check_output method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin","title":"ToolUsageGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check tool usage attempts.</p>"},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin.check_tool_usage","title":"check_tool_usage  <code>async</code>","text":"<pre><code>check_tool_usage(\n    tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None\n) -&gt; GuardrailViolation\n</code></pre> <p>Checks if a tool usage attempt is permissible. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_tool_usage(self, tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks if a tool usage attempt is permissible.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"ToolUsageGuardrailPlugin '{self.plugin_id}' check_tool_usage method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.GuardrailViolation","title":"GuardrailViolation","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the outcome of a guardrail check.</p>"},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin","title":"HumanApprovalRequestPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that requests human approval for an action.</p>"},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin.request_approval","title":"request_approval  <code>async</code>","text":"<pre><code>request_approval(request: ApprovalRequest) -&gt; ApprovalResponse\n</code></pre> <p>Requests human approval for a given action or data. Implementations will vary (e.g., CLI prompt, web UI notification, API call).</p> Source code in <code>src/genie_tooling/hitl/abc.py</code> <pre><code>async def request_approval(self, request: ApprovalRequest) -&gt; ApprovalResponse:\n    \"\"\"\n    Requests human approval for a given action or data.\n    Implementations will vary (e.g., CLI prompt, web UI notification, API call).\n    \"\"\"\n    logger.warning(f\"HumanApprovalRequestPlugin '{self.plugin_id}' request_approval method not fully implemented.\")\n    # Default to denied if not implemented\n    return ApprovalResponse(\n        request_id=request.request_id,\n        status=\"denied\",\n        approver_id=\"system_default\",\n        reason=\"Plugin not implemented.\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.ApprovalRequest","title":"ApprovalRequest","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a request for human approval.</p>"},{"location":"api/#genie_tooling.ApprovalResponse","title":"ApprovalResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the response from a human approval request.</p>"},{"location":"api/#genie_tooling.InputValidationException","title":"InputValidationException","text":"<pre><code>InputValidationException(\n    message: str, errors: Any = None, params: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>               Bases: <code>ValueError</code></p> <p>Custom exception for input validation errors, providing more context.</p> Source code in <code>src/genie_tooling/input_validators/abc.py</code> <pre><code>def __init__(self, message: str, errors: Any = None, params: Optional[Dict[str,Any]] = None):\n    super().__init__(message)\n    self.errors = errors\n    self.params = params\n</code></pre>"},{"location":"api/#genie_tooling.InputValidator","title":"InputValidator","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for input parameter validators.</p>"},{"location":"api/#genie_tooling.InputValidator-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InputValidator.validate","title":"validate","text":"<pre><code>validate(params: Dict[str, Any], schema: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Validates parameters against a schema. Should raise InputValidationException on failure. May return params (possibly coerced or with defaults applied by validator). This method is synchronous as validation is typically CPU-bound.</p> Source code in <code>src/genie_tooling/input_validators/abc.py</code> <pre><code>def validate(self, params: Dict[str, Any], schema: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validates parameters against a schema.\n    Should raise InputValidationException on failure.\n    May return params (possibly coerced or with defaults applied by validator).\n    This method is synchronous as validation is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.InvocationStrategy","title":"InvocationStrategy","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a strategy that defines the complete lifecycle of invoking a tool. This includes validation, execution, transformation, caching, and error handling. All strategies must be designed to be async.</p>"},{"location":"api/#genie_tooling.InvocationStrategy-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InvocationStrategy.invoke","title":"invoke  <code>async</code>","text":"<pre><code>invoke(\n    tool: Tool,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]],\n    invoker_config: Dict[str, Any],\n) -&gt; Any\n</code></pre> <p>Executes the full tool invocation lifecycle according to this strategy.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Tool</code> <p>The Tool instance to invoke.</p> required <code>params</code> <code>Dict[str, Any]</code> <p>The parameters for the tool.</p> required <code>key_provider</code> <code>KeyProvider</code> <p>The async key provider.</p> required <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Optional context dictionary.</p> required <code>invoker_config</code> <code>Dict[str, Any]</code> <p>Configuration from the ToolInvoker, including:             - \"plugin_manager\": PluginManager instance             - \"validator_id\": Optional[str]             - \"transformer_id\": Optional[str]             - \"error_handler_id\": Optional[str]             - \"error_formatter_id\": Optional[str]             - \"cache_provider_id\": Optional[str]             - \"cache_config\": Optional[Dict[str, Any]]</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool execution (possibly transformed by an OutputTransformer)</p> <code>Any</code> <p>or a structured error (formatted by an ErrorFormatter). The exact type depends</p> <code>Any</code> <p>on the formatter's output.</p> Source code in <code>src/genie_tooling/invocation_strategies/abc.py</code> <pre><code>async def invoke(\n    self,\n    tool: Tool,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]],\n    invoker_config: Dict[str, Any] # Contains plugin_manager and override IDs for components\n) -&gt; Any:\n    \"\"\"\n    Executes the full tool invocation lifecycle according to this strategy.\n\n    Args:\n        tool: The Tool instance to invoke.\n        params: The parameters for the tool.\n        key_provider: The async key provider.\n        context: Optional context dictionary.\n        invoker_config: Configuration from the ToolInvoker, including:\n                        - \"plugin_manager\": PluginManager instance\n                        - \"validator_id\": Optional[str]\n                        - \"transformer_id\": Optional[str]\n                        - \"error_handler_id\": Optional[str]\n                        - \"error_formatter_id\": Optional[str]\n                        - \"cache_provider_id\": Optional[str]\n                        - \"cache_config\": Optional[Dict[str, Any]]\n\n    Returns:\n        The result of the tool execution (possibly transformed by an OutputTransformer)\n        or a structured error (formatted by an ErrorFormatter). The exact type depends\n        on the formatter's output.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.DefaultAsyncInvocationStrategy","title":"DefaultAsyncInvocationStrategy","text":"<p>               Bases: <code>InvocationStrategy</code></p>"},{"location":"api/#genie_tooling.LLMProviderPlugin","title":"LLMProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that interacts with a Large Language Model provider.</p>"},{"location":"api/#genie_tooling.LLMProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LLMProviderPlugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]]) -&gt; None\n</code></pre> <p>Initializes the LLM provider. The 'config' dictionary is expected to contain 'key_provider: KeyProvider' if the specific LLM provider implementation requires API keys.</p> Source code in <code>src/genie_tooling/llm_providers/abc.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Initializes the LLM provider.\n    The 'config' dictionary is expected to contain 'key_provider: KeyProvider'\n    if the specific LLM provider implementation requires API keys.\n    \"\"\"\n    await super().setup(config)\n    logger.debug(f\"LLMProviderPlugin '{getattr(self, 'plugin_id', 'UnknownPluginID')}': Base setup logic (if any) completed.\")\n</code></pre>"},{"location":"api/#genie_tooling.ChatMessage","title":"ChatMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single message in a chat conversation. Compatible with OpenAI's ChatCompletion message structure.</p>"},{"location":"api/#genie_tooling.LLMChatResponse","title":"LLMChatResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response for chat completion LLM calls.</p>"},{"location":"api/#genie_tooling.LLMCompletionResponse","title":"LLMCompletionResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response for text completion LLM calls.</p>"},{"location":"api/#genie_tooling.LLMUsageInfo","title":"LLMUsageInfo","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents token usage information from an LLM response.</p>"},{"location":"api/#genie_tooling.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a tool call requested by the LLM. Compatible with OpenAI's tool_calls structure.</p>"},{"location":"api/#genie_tooling.ToolCallFunction","title":"ToolCallFunction","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the function to be called within a ToolCall.</p>"},{"location":"api/#genie_tooling.LogAdapterPlugin","title":"LogAdapterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a logging/monitoring adapter.</p>"},{"location":"api/#genie_tooling.LogAdapterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LogAdapterPlugin.setup_logging","title":"setup_logging  <code>async</code>","text":"<pre><code>setup_logging(config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Configures logging handlers or integrates with external monitoring systems. This method is called after the adapter is instantiated. Args:     config: Adapter-specific configuration dictionary. May include 'plugin_manager'             if this adapter needs to load other plugins (e.g., a RedactorPlugin).</p> Source code in <code>src/genie_tooling/log_adapters/abc.py</code> <pre><code>async def setup_logging(self, config: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Configures logging handlers or integrates with external monitoring systems.\n    This method is called after the adapter is instantiated.\n    Args:\n        config: Adapter-specific configuration dictionary. May include 'plugin_manager'\n                if this adapter needs to load other plugins (e.g., a RedactorPlugin).\n    \"\"\"\n    logger.warning(f\"LogAdapter '{self.plugin_id}' setup_logging method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.LogAdapterPlugin.process_event","title":"process_event  <code>async</code>","text":"<pre><code>process_event(\n    event_type: str,\n    data: Dict[str, Any],\n    schema_for_data: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Processes a structured event (e.g., for monitoring or detailed logging). Data should be pre-sanitized by this method or by a configured RedactorPlugin. Args:     event_type: A string identifying the type of event (e.g., \"tool_invoked\", \"tool_error\").     data: A dictionary containing event-specific data.     schema_for_data: Optional JSON schema corresponding to 'data', to aid redaction.</p> Source code in <code>src/genie_tooling/log_adapters/abc.py</code> <pre><code>async def process_event(self, event_type: str, data: Dict[str, Any], schema_for_data: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Processes a structured event (e.g., for monitoring or detailed logging).\n    Data should be pre-sanitized by this method or by a configured RedactorPlugin.\n    Args:\n        event_type: A string identifying the type of event (e.g., \"tool_invoked\", \"tool_error\").\n        data: A dictionary containing event-specific data.\n        schema_for_data: Optional JSON schema corresponding to 'data', to aid redaction.\n    \"\"\"\n    logger.warning(f\"LogAdapter '{self.plugin_id}' process_event method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.RankedToolResult","title":"RankedToolResult","text":"<pre><code>RankedToolResult(\n    tool_identifier: str,\n    score: float,\n    matched_tool_data: Optional[Dict[str, Any]] = None,\n    description_snippet: Optional[str] = None,\n)\n</code></pre> <p>Represents a tool found by a lookup provider.</p> Source code in <code>src/genie_tooling/lookup/types.py</code> <pre><code>def __init__(self, tool_identifier: str, score: float, matched_tool_data: Optional[Dict[str, Any]] = None, description_snippet: Optional[str] = None):\n    self.tool_identifier: str = tool_identifier\n    self.score: float = score # Relevance score (higher is better)\n\n    # The data that the provider used for matching (e.g., formatted description, keywords).\n    # This helps the LLM or orchestrator understand why this tool was chosen.\n    self.matched_tool_data: Optional[Dict[str, Any]] = matched_tool_data or {}\n\n    # A short snippet or reason for the match, if provided by the lookup provider.\n    self.description_snippet: Optional[str] = description_snippet\n</code></pre>"},{"location":"api/#genie_tooling.InteractionTracerPlugin","title":"InteractionTracerPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that records interaction traces.</p>"},{"location":"api/#genie_tooling.InteractionTracerPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InteractionTracerPlugin.record_trace","title":"record_trace  <code>async</code>","text":"<pre><code>record_trace(event: TraceEvent) -&gt; None\n</code></pre> <p>Records a single trace event. Implementations should handle batching or asynchronous sending if needed.</p> Source code in <code>src/genie_tooling/observability/abc.py</code> <pre><code>async def record_trace(self, event: TraceEvent) -&gt; None:\n    \"\"\"\n    Records a single trace event.\n    Implementations should handle batching or asynchronous sending if needed.\n    \"\"\"\n    logger.warning(f\"InteractionTracerPlugin '{self.plugin_id}' record_trace method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.TraceEvent","title":"TraceEvent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single event to be traced.</p>"},{"location":"api/#genie_tooling.OutputTransformer","title":"OutputTransformer","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for output transformers.</p>"},{"location":"api/#genie_tooling.OutputTransformer-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.OutputTransformer.transform","title":"transform","text":"<pre><code>transform(output: Any, schema: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Transforms raw tool output according to an output_schema or desired format. Raises OutputTransformationException on failure. This method is synchronous as transformation is typically CPU-bound.</p> Source code in <code>src/genie_tooling/output_transformers/abc.py</code> <pre><code>def transform(self, output: Any, schema: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Transforms raw tool output according to an output_schema or desired format.\n    Raises OutputTransformationException on failure.\n    This method is synchronous as transformation is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.PromptRegistryPlugin","title":"PromptRegistryPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that stores and retrieves prompt templates.</p>"},{"location":"api/#genie_tooling.PromptTemplatePlugin","title":"PromptTemplatePlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that renders prompt templates.</p>"},{"location":"api/#genie_tooling.ConversationStateProviderPlugin","title":"ConversationStateProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that stores and retrieves conversation state.</p>"},{"location":"api/#genie_tooling.LLMOutputParserPlugin","title":"LLMOutputParserPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that parses the text output of an LLM.</p>"},{"location":"api/#genie_tooling.LLMOutputParserPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LLMOutputParserPlugin.parse","title":"parse","text":"<pre><code>parse(text_output: str, schema: Optional[Any] = None) -&gt; ParsedOutput\n</code></pre> <p>Parses the LLM's text output. Args:     text_output: The raw text string from the LLM.     schema: Optional schema (e.g., Pydantic model, JSON schema) to guide parsing. Returns:     The parsed data, which could be a dict, list, Pydantic model instance, etc. Raises:     ValueError or a custom parsing exception if parsing fails.</p> Source code in <code>src/genie_tooling/prompts/llm_output_parsers/abc.py</code> <pre><code>def parse(self, text_output: str, schema: Optional[Any] = None) -&gt; ParsedOutput:\n    \"\"\"\n    Parses the LLM's text output.\n    Args:\n        text_output: The raw text string from the LLM.\n        schema: Optional schema (e.g., Pydantic model, JSON schema) to guide parsing.\n    Returns:\n        The parsed data, which could be a dict, list, Pydantic model instance, etc.\n    Raises:\n        ValueError or a custom parsing exception if parsing fails.\n    \"\"\"\n    logger.warning(f\"LLMOutputParserPlugin '{self.plugin_id}' parse method not implemented.\")\n    # Basic fallback: return as is, or attempt JSON if it looks like it\n    if text_output.strip().startswith(\"{\") and text_output.strip().endswith(\"}\"):\n        try:\n            return json.loads(text_output) # type: ignore\n        except Exception:\n            pass\n    return text_output # type: ignore\n</code></pre>"},{"location":"api/#genie_tooling.RedactorPlugin","title":"RedactorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a data redaction plugin.</p>"},{"location":"api/#genie_tooling.RedactorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.RedactorPlugin.sanitize","title":"sanitize","text":"<pre><code>sanitize(data: Any, schema_hints: Optional[Dict[str, Any]] = None) -&gt; Any\n</code></pre> <p>Sanitizes data to remove or mask sensitive information. This method is synchronous as redaction is typically CPU-bound. Args:     data: The data to sanitize (can be any Python object, commonly dicts or lists).     schema_hints: Optional JSON schema corresponding to 'data'. If provided,                   the redactor can use schema annotations (e.g., 'x-sensitive', 'format')                   to guide redaction. Returns:     The sanitized data, with sensitive parts replaced or removed.</p> Source code in <code>src/genie_tooling/redactors/abc.py</code> <pre><code>def sanitize(self, data: Any, schema_hints: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Sanitizes data to remove or mask sensitive information.\n    This method is synchronous as redaction is typically CPU-bound.\n    Args:\n        data: The data to sanitize (can be any Python object, commonly dicts or lists).\n        schema_hints: Optional JSON schema corresponding to 'data'. If provided,\n                      the redactor can use schema annotations (e.g., 'x-sensitive', 'format')\n                      to guide redaction.\n    Returns:\n        The sanitized data, with sensitive parts replaced or removed.\n    \"\"\"\n    logger.warning(f\"Redactor '{self.plugin_id}' sanitize method not fully implemented. Returning data as is.\")\n    return data # Default: no redaction\n</code></pre>"},{"location":"api/#genie_tooling.RetrieverPlugin","title":"RetrieverPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Retrieves relevant chunks based on a query, typically by composing an embedder and a vector store.</p>"},{"location":"api/#genie_tooling.RetrieverPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.RetrieverPlugin.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    query: str, top_k: int, config: Optional[Dict[str, Any]] = None\n) -&gt; List[RetrievedChunk]\n</code></pre> <p>Retrieves relevant chunks for the given query. Args:     query: The natural language query string.     top_k: The number of top results to return.     config: Retriever-specific configuration. This should include details for               the embedder (e.g., \"embedder_id\", \"embedder_config\") and               vector store (e.g., \"vector_store_id\", \"vector_store_config\")               that this retriever instance will use. It should also contain \"plugin_manager\". Returns:     A list of RetrievedChunk objects.</p> Source code in <code>src/genie_tooling/retrievers/abc.py</code> <pre><code>async def retrieve(self, query: str, top_k: int, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]:\n    \"\"\"\n    Retrieves relevant chunks for the given query.\n    Args:\n        query: The natural language query string.\n        top_k: The number of top results to return.\n        config: Retriever-specific configuration. This should include details for\n                  the embedder (e.g., \"embedder_id\", \"embedder_config\") and\n                  vector store (e.g., \"vector_store_id\", \"vector_store_config\")\n                  that this retriever instance will use. It should also contain \"plugin_manager\".\n    Returns:\n        A list of RetrievedChunk objects.\n    \"\"\"\n    logger.warning(f\"RetrieverPlugin '{self.plugin_id}' retrieve method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.KeyProvider","title":"KeyProvider","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a component that securely provides API keys. This protocol must be implemented by the consuming application. The middleware itself does not store or manage API keys directly. All methods must be async.</p>"},{"location":"api/#genie_tooling.KeyProvider-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.KeyProvider.get_key","title":"get_key  <code>async</code>","text":"<pre><code>get_key(key_name: str) -&gt; Optional[str]\n</code></pre> <p>Asynchronously retrieves the API key value for the given key name.</p> <p>Parameters:</p> Name Type Description Default <code>key_name</code> <code>str</code> <p>The logical name of the API key required by a tool       (e.g., \"OPENWEATHERMAP_API_KEY\", \"OPENAI_API_KEY\").</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The API key string if found and accessible, otherwise None.</p> <code>Optional[str]</code> <p>Implementations should fetch keys securely (e.g., from environment</p> <code>Optional[str]</code> <p>variables, a secrets manager/vault, or application configuration).</p> <code>Optional[str]</code> <p>It should log (at debug level) if a key is requested but not found,</p> <code>Optional[str]</code> <p>but avoid logging the key value itself.</p> Source code in <code>src/genie_tooling/security/key_provider.py</code> <pre><code>async def get_key(self, key_name: str) -&gt; Optional[str]:\n    \"\"\"\n    Asynchronously retrieves the API key value for the given key name.\n\n    Args:\n        key_name: The logical name of the API key required by a tool\n                  (e.g., \"OPENWEATHERMAP_API_KEY\", \"OPENAI_API_KEY\").\n\n    Returns:\n        The API key string if found and accessible, otherwise None.\n        Implementations should fetch keys securely (e.g., from environment\n        variables, a secrets manager/vault, or application configuration).\n        It should log (at debug level) if a key is requested but not found,\n        but avoid logging the key value itself.\n    \"\"\"\n    # Example of what an implementation might log (this is a protocol, so no actual logic here)\n    # logger.debug(f\"KeyProvider: Request received for key '{key_name}'.\")\n    # value = self._internal_secure_key_storage.get(key_name)\n    # if not value:\n    #     logger.warning(f\"KeyProvider: Key '{key_name}' not found in secure storage.\")\n    # return value\n    ... # Indicates abstract method in Protocol\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin","title":"DistributedTaskQueuePlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that interacts with a distributed task queue system.</p>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.submit_task","title":"submit_task  <code>async</code>","text":"<pre><code>submit_task(\n    task_name: str,\n    args: Tuple = (),\n    kwargs: Optional[Dict[str, Any]] = None,\n    queue_name: Optional[str] = None,\n    task_options: Optional[Dict[str, Any]] = None,\n) -&gt; str\n</code></pre> <p>Submits a task to the distributed queue. Returns:     str: The unique ID of the submitted task. Raises:     Exception: If submission fails.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def submit_task(\n    self,\n    task_name: str, # Name of the task registered with the queue system\n    args: Tuple = (),\n    kwargs: Optional[Dict[str, Any]] = None,\n    queue_name: Optional[str] = None, # Specific queue to send to\n    task_options: Optional[Dict[str, Any]] = None # Options like countdown, eta, priority\n) -&gt; str:\n    \"\"\"\n    Submits a task to the distributed queue.\n    Returns:\n        str: The unique ID of the submitted task.\n    Raises:\n        Exception: If submission fails.\n    \"\"\"\n    logger.error(f\"DistributedTaskQueuePlugin '{self.plugin_id}' submit_task not implemented.\")\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.get_task_status","title":"get_task_status  <code>async</code>","text":"<pre><code>get_task_status(task_id: str, queue_name: Optional[str] = None) -&gt; TaskStatus\n</code></pre> <p>Gets the status of a previously submitted task.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def get_task_status(self, task_id: str, queue_name: Optional[str] = None) -&gt; TaskStatus:\n    \"\"\"Gets the status of a previously submitted task.\"\"\"\n    logger.warning(f\"DistributedTaskQueuePlugin '{self.plugin_id}' get_task_status not implemented.\")\n    return \"unknown\"\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.get_task_result","title":"get_task_result  <code>async</code>","text":"<pre><code>get_task_result(\n    task_id: str,\n    queue_name: Optional[str] = None,\n    timeout_seconds: Optional[float] = None,\n) -&gt; Any\n</code></pre> <p>Retrieves the result of a completed task. May block until the task is complete or timeout occurs. Raises:     TimeoutError: If timeout is specified and exceeded.     Exception: If task failed or other error.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def get_task_result(\n    self, task_id: str, queue_name: Optional[str] = None, timeout_seconds: Optional[float] = None\n) -&gt; Any:\n    \"\"\"\n    Retrieves the result of a completed task.\n    May block until the task is complete or timeout occurs.\n    Raises:\n        TimeoutError: If timeout is specified and exceeded.\n        Exception: If task failed or other error.\n    \"\"\"\n    logger.error(f\"DistributedTaskQueuePlugin '{self.plugin_id}' get_task_result not implemented.\")\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.revoke_task","title":"revoke_task  <code>async</code>","text":"<pre><code>revoke_task(\n    task_id: str, queue_name: Optional[str] = None, terminate: bool = False\n) -&gt; bool\n</code></pre> <p>Revokes (cancels) a pending or running task. Args:     terminate: If True, attempt to terminate a running task. Returns:     True if revocation was successful or task was already completed/revoked.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def revoke_task(self, task_id: str, queue_name: Optional[str] = None, terminate: bool = False) -&gt; bool:\n    \"\"\"\n    Revokes (cancels) a pending or running task.\n    Args:\n        terminate: If True, attempt to terminate a running task.\n    Returns:\n        True if revocation was successful or task was already completed/revoked.\n    \"\"\"\n    logger.warning(f\"DistributedTaskQueuePlugin '{self.plugin_id}' revoke_task not implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.TextSplitterPlugin","title":"TextSplitterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Splits an async stream of Documents into an async stream of Chunks.</p>"},{"location":"api/#genie_tooling.TextSplitterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.TextSplitterPlugin.split","title":"split  <code>async</code>","text":"<pre><code>split(\n    documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Chunk]\n</code></pre> <p>Splits documents into smaller chunks. Args:     documents: An async iterable of Document objects.     config: Splitter-specific configuration (e.g., chunk_size, chunk_overlap). Yields:     Chunk objects.</p> Source code in <code>src/genie_tooling/text_splitters/abc.py</code> <pre><code>async def split(self, documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Chunk]:\n    \"\"\"\n    Splits documents into smaller chunks.\n    Args:\n        documents: An async iterable of Document objects.\n        config: Splitter-specific configuration (e.g., chunk_size, chunk_overlap).\n    Yields:\n        Chunk objects.\n    \"\"\"\n    logger.warning(f\"TextSplitterPlugin '{self.plugin_id}' split method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin","title":"TokenUsageRecorderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that records LLM token usage.</p>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.record_usage","title":"record_usage  <code>async</code>","text":"<pre><code>record_usage(record: TokenUsageRecord) -&gt; None\n</code></pre> <p>Records a single token usage event.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def record_usage(self, record: TokenUsageRecord) -&gt; None:\n    \"\"\"Records a single token usage event.\"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' record_usage method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.get_summary","title":"get_summary  <code>async</code>","text":"<pre><code>get_summary(filter_criteria: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieves a summary of token usage. The structure of the summary is implementation-dependent.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def get_summary(self, filter_criteria: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieves a summary of token usage.\n    The structure of the summary is implementation-dependent.\n    \"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' get_summary method not fully implemented.\")\n    return {\"error\": \"Not implemented\"}\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.clear_records","title":"clear_records  <code>async</code>","text":"<pre><code>clear_records(filter_criteria: Optional[Dict[str, Any]] = None) -&gt; bool\n</code></pre> <p>Clears recorded usage data, optionally based on criteria.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def clear_records(self, filter_criteria: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"Clears recorded usage data, optionally based on criteria.\"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' clear_records method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecord","title":"TokenUsageRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single LLM token usage event.</p>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin","title":"ToolLookupProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that finds relevant tools based on a query.</p>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.index_tools","title":"index_tools  <code>async</code>","text":"<pre><code>index_tools(\n    tools_data: List[Dict[str, Any]], config: Optional[Dict[str, Any]] = None\n) -&gt; None\n</code></pre> <p>Builds or updates an internal index using formatted tool data. 'tools_data' is a list of dictionaries, where each dict is typically the output of a DefinitionFormatterPlugin, expected to contain at least 'identifier' and some textual representation for matching (e.g., 'lookup_text_representation').</p> <p>Parameters:</p> Name Type Description Default <code>tools_data</code> <code>List[Dict[str, Any]]</code> <p>List of formatted data for each tool.</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific configuration, might include <code>plugin_manager</code> if this     provider needs to load sub-plugins (e.g., an embedder).</p> <code>None</code> <p>This method might be a no-op for stateless providers that search on-the-fly. For stateful providers (e.g., embedding-based), this prepares the search index.</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def index_tools(self, tools_data: List[Dict[str, Any]], config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Builds or updates an internal index using formatted tool data.\n    'tools_data' is a list of dictionaries, where each dict is typically the output of a\n    DefinitionFormatterPlugin, expected to contain at least 'identifier' and some\n    textual representation for matching (e.g., 'lookup_text_representation').\n\n    Args:\n        tools_data: List of formatted data for each tool.\n        config: Provider-specific configuration, might include `plugin_manager` if this\n                provider needs to load sub-plugins (e.g., an embedder).\n\n    This method might be a no-op for stateless providers that search on-the-fly.\n    For stateful providers (e.g., embedding-based), this prepares the search index.\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' index_tools method not fully implemented.\")\n    pass # Default no-op for stateless providers\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.find_tools","title":"find_tools  <code>async</code>","text":"<pre><code>find_tools(\n    natural_language_query: str, top_k: int = 5, config: Optional[Dict[str, Any]] = None\n) -&gt; List[RankedToolResult]\n</code></pre> <p>Searches the indexed tools (or performs a stateless search) based on the natural_language_query.</p> <p>Parameters:</p> Name Type Description Default <code>natural_language_query</code> <code>str</code> <p>The user's query for a tool's capability.</p> required <code>top_k</code> <code>int</code> <p>The maximum number of ranked results to return.</p> <code>5</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Provider-specific runtime configuration for the search operation.     Might include <code>plugin_manager</code> if query-time operations need sub-plugins.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[RankedToolResult]</code> <p>A list of RankedToolResult objects, sorted by relevance (highest score first).</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def find_tools(\n    self,\n    natural_language_query: str,\n    top_k: int = 5,\n    config: Optional[Dict[str, Any]] = None\n) -&gt; List[RankedToolResult]:\n    \"\"\"\n    Searches the indexed tools (or performs a stateless search)\n    based on the natural_language_query.\n\n    Args:\n        natural_language_query: The user's query for a tool's capability.\n        top_k: The maximum number of ranked results to return.\n        config: Provider-specific runtime configuration for the search operation.\n                Might include `plugin_manager` if query-time operations need sub-plugins.\n\n    Returns:\n        A list of RankedToolResult objects, sorted by relevance (highest score first).\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' find_tools method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.ToolPlugin","title":"ToolPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a tool that can be executed by the middleware. All tools must be designed to be async.</p>"},{"location":"api/#genie_tooling.ToolPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolPlugin.get_metadata","title":"get_metadata  <code>async</code>","text":"<pre><code>get_metadata() -&gt; Dict[str, Any]\n</code></pre> <p>Returns comprehensive metadata about the tool. This metadata is crucial for tool discovery, LLM function calling, and UI display.</p> <p>Expected structure: {     \"identifier\": str, (matches self.identifier)     \"name\": str, (human-friendly name)     \"description_human\": str, (detailed for developers/UI)     \"description_llm\": str, (concise, token-efficient for LLM prompts/function descriptions)     \"input_schema\": Dict[str, Any], (JSON Schema for tool parameters)     \"output_schema\": Dict[str, Any], (JSON Schema for tool's expected output structure)     \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", \"description\": \"Purpose of key\"}])     \"tags\": List[str], (for categorization, e.g., [\"weather\", \"api\", \"location\"])     \"version\": str, (e.g., \"1.0.0\")     \"cacheable\": bool, (optional, hints if tool output can be cached, default False)     \"cache_ttl_seconds\": Optional[int] (optional, default TTL if cacheable) }</p> Source code in <code>src/genie_tooling/tools/abc.py</code> <pre><code>async def get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns comprehensive metadata about the tool.\n    This metadata is crucial for tool discovery, LLM function calling, and UI display.\n\n    Expected structure:\n    {\n        \"identifier\": str, (matches self.identifier)\n        \"name\": str, (human-friendly name)\n        \"description_human\": str, (detailed for developers/UI)\n        \"description_llm\": str, (concise, token-efficient for LLM prompts/function descriptions)\n        \"input_schema\": Dict[str, Any], (JSON Schema for tool parameters)\n        \"output_schema\": Dict[str, Any], (JSON Schema for tool's expected output structure)\n        \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", \"description\": \"Purpose of key\"}])\n        \"tags\": List[str], (for categorization, e.g., [\"weather\", \"api\", \"location\"])\n        \"version\": str, (e.g., \"1.0.0\")\n        \"cacheable\": bool, (optional, hints if tool output can be cached, default False)\n        \"cache_ttl_seconds\": Optional[int] (optional, default TTL if cacheable)\n    }\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ToolPlugin.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]] = None,\n) -&gt; Any\n</code></pre> <p>Executes the tool with the given parameters. Must be an async method.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>Validated parameters for the tool, conforming to its input_schema.</p> required <code>key_provider</code> <code>KeyProvider</code> <p>An async key provider instance for fetching necessary API keys.</p> required <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Optional context dictionary carrying session/request-specific data.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool execution. The structure should align with output_schema.</p> <code>Any</code> <p>If an error occurs during execution that the tool handles, it should still</p> <code>Any</code> <p>return a structured response, possibly including an 'error' field, conforming</p> <code>Any</code> <p>to its output_schema. Unhandled exceptions will be caught by the InvocationStrategy.</p> Source code in <code>src/genie_tooling/tools/abc.py</code> <pre><code>async def execute(\n    self,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]] = None\n) -&gt; Any:\n    \"\"\"\n    Executes the tool with the given parameters.\n    Must be an async method.\n\n    Args:\n        params: Validated parameters for the tool, conforming to its input_schema.\n        key_provider: An async key provider instance for fetching necessary API keys.\n        context: Optional context dictionary carrying session/request-specific data.\n\n    Returns:\n        The result of the tool execution. The structure should align with output_schema.\n        If an error occurs during execution that the tool handles, it should still\n        return a structured response, possibly including an 'error' field, conforming\n        to its output_schema. Unhandled exceptions will be caught by the InvocationStrategy.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ToolManager","title":"ToolManager","text":"<pre><code>ToolManager(plugin_manager: PluginManager)\n</code></pre> <p>Manages the lifecycle and access to ToolPlugins.</p> Source code in <code>src/genie_tooling/tools/manager.py</code> <pre><code>def __init__(self, plugin_manager: PluginManager):\n    self._plugin_manager = plugin_manager\n    self._tools: Dict[str, Tool] = {} # identifier -&gt; Tool instance\n    self._tool_initial_configs: Dict[str, Dict[str, Any]] = {}\n    logger.debug(\"ToolManager initialized.\")\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin","title":"VectorStorePlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Interface for interacting with a vector database.</p>"},{"location":"api/#genie_tooling.VectorStorePlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.VectorStorePlugin.add","title":"add  <code>async</code>","text":"<pre><code>add(\n    embeddings: AsyncIterable[Tuple[Chunk, EmbeddingVector]],\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Adds chunks and their embeddings to the vector store. Should handle batching internally if the input stream is large. Args:     embeddings: An async iterable of (Chunk, EmbeddingVector) tuples.     config: Vector store-specific configuration (e.g., collection name, batch_size). Returns:     A dictionary with status, e.g., {\"added_count\": int, \"errors\": List[str]}</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def add(self, embeddings: AsyncIterable[Tuple[Chunk, EmbeddingVector]], config: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]: # Returns status/count\n    \"\"\"\n    Adds chunks and their embeddings to the vector store.\n    Should handle batching internally if the input stream is large.\n    Args:\n        embeddings: An async iterable of (Chunk, EmbeddingVector) tuples.\n        config: Vector store-specific configuration (e.g., collection name, batch_size).\n    Returns:\n        A dictionary with status, e.g., {\"added_count\": int, \"errors\": List[str]}\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' add method not fully implemented.\")\n    return {\"added_count\": 0, \"errors\": [\"Not implemented\"]}\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin.search","title":"search  <code>async</code>","text":"<pre><code>search(\n    query_embedding: EmbeddingVector,\n    top_k: int,\n    filter_metadata: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; List[RetrievedChunk]\n</code></pre> <p>Searches the vector store for chunks similar to the query_embedding. Args:     query_embedding: The embedding vector of the query.     top_k: The number of top results to return.     filter_metadata: Optional metadata filter to apply during search.     config: Search-specific configuration. Returns:     A list of RetrievedChunk objects.</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def search(self, query_embedding: EmbeddingVector, top_k: int, filter_metadata: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]:\n    \"\"\"\n    Searches the vector store for chunks similar to the query_embedding.\n    Args:\n        query_embedding: The embedding vector of the query.\n        top_k: The number of top results to return.\n        filter_metadata: Optional metadata filter to apply during search.\n        config: Search-specific configuration.\n    Returns:\n        A list of RetrievedChunk objects.\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' search method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    ids: Optional[List[str]] = None,\n    filter_metadata: Optional[Dict[str, Any]] = None,\n    delete_all: bool = False,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; bool\n</code></pre> <p>Deletes items from the vector store. Args:     ids: Optional list of chunk IDs to delete.     filter_metadata: Optional metadata filter to select items for deletion.     delete_all: If True, delete all items in the collection/store.     config: Deletion-specific configuration. Returns:     True if deletion was successful (or partially successful), False otherwise.</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def delete(self, ids: Optional[List[str]] = None, filter_metadata: Optional[Dict[str, Any]] = None, delete_all: bool = False, config: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"\n    Deletes items from the vector store.\n    Args:\n        ids: Optional list of chunk IDs to delete.\n        filter_metadata: Optional metadata filter to select items for deletion.\n        delete_all: If True, delete all items in the collection/store.\n        config: Deletion-specific configuration.\n    Returns:\n        True if deletion was successful (or partially successful), False otherwise.\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' delete method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.tool","title":"tool","text":"<pre><code>tool(func: Callable) -&gt; Callable\n</code></pre> <p>Decorator to mark a function as a Genie Tool and auto-generate its metadata.</p> Source code in <code>src/genie_tooling/decorators.py</code> <pre><code>def tool(func: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator to mark a function as a Genie Tool and auto-generate its metadata.\n    \"\"\"\n    globalns = getattr(func, \"__globals__\", {})\n    try:\n        type_hints = get_type_hints(func, globalns=globalns)\n    except NameError as e:\n        try:\n            type_hints = get_type_hints(func)\n        except Exception:\n            type_hints = {}\n            print(f\"Warning: Could not fully resolve type hints for {func.__name__} due to {e}. Schemas might be incomplete.\")\n\n    sig = inspect.signature(func)\n    docstring = inspect.getdoc(func) or \"\"\n\n    main_description = docstring.split(\"\\n\\n\")[0].strip()\n    if not main_description and func.__name__:\n        main_description = f\"Executes the '{func.__name__}' tool.\"\n\n    param_descriptions_from_doc = _parse_docstring_for_params(docstring)\n\n    properties: Dict[str, Any] = {}\n    required_params: List[str] = []\n\n    for name, param in sig.parameters.items():\n        if name == \"self\" or name == \"cls\" or \\\n           param.kind == inspect.Parameter.VAR_POSITIONAL or \\\n           param.kind == inspect.Parameter.VAR_KEYWORD:\n            continue\n\n        param_py_type = type_hints.get(name, Any)\n\n        if isinstance(param_py_type, str):\n            try:\n                # MODIFIED: Pass recursive_guard as a keyword argument\n                param_py_type = ForwardRef(param_py_type)._evaluate(globalns, {}, recursive_guard=frozenset())\n            except Exception:\n                 pass\n\n        is_optional_hint = False\n        origin = getattr(param_py_type, \"__origin__\", None)\n        args = getattr(param_py_type, \"__args__\", None)\n        if origin is Union and type(None) in (args or []):\n            is_optional_hint = True\n            if args:\n                param_py_type = next((t for t in args if t is not type(None)), Any)\n\n        schema_type_def = _map_type_to_json_schema(param_py_type)\n\n        # If _map_type_to_json_schema returned {} (for Any), default to string for schema\n        if not schema_type_def and param_py_type is Any: # MODIFIED\n            schema_type_def = {\"type\": \"string\"}\n\n        param_info_schema = schema_type_def\n        param_info_schema[\"description\"] = param_descriptions_from_doc.get(name, f\"Parameter '{name}'.\")\n\n        if param.default is inspect.Parameter.empty:\n            if not is_optional_hint:\n                required_params.append(name)\n        else:\n            param_info_schema[\"default\"] = param.default\n\n        properties[name] = param_info_schema\n\n    input_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": properties}\n    if required_params:\n        input_schema[\"required\"] = required_params\n\n    return_py_type = type_hints.get(\"return\", Any)\n    if isinstance(return_py_type, str):\n        try:\n            # MODIFIED: Pass recursive_guard as a keyword argument\n            return_py_type = ForwardRef(return_py_type)._evaluate(globalns, {}, recursive_guard=frozenset())\n        except Exception:\n            pass\n\n    output_schema_prop_def = _map_type_to_json_schema(return_py_type)\n\n    output_schema: Dict[str, Any] = {\n        \"type\": \"object\",\n        \"properties\": {\"result\": output_schema_prop_def},\n    }\n    if output_schema_prop_def.get(\"type\") != \"null\":\n         output_schema[\"required\"] = [\"result\"]\n\n    tool_metadata = {\n        \"identifier\": func.__name__,\n        \"name\": func.__name__.replace(\"_\", \" \").title(),\n        \"description_human\": main_description,\n        \"description_llm\": main_description,\n        \"input_schema\": input_schema,\n        \"output_schema\": output_schema,\n        \"key_requirements\": [],\n        \"tags\": [\"decorated_tool\"],\n        \"version\": \"1.0.0\",\n        \"cacheable\": False,\n    }\n\n    @wraps(func)\n    async def async_wrapper(*args, **kwargs):\n        return await func(*args, **kwargs)\n\n    @wraps(func)\n    def sync_wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n\n    chosen_wrapper = async_wrapper if inspect.iscoroutinefunction(func) else sync_wrapper\n\n    chosen_wrapper._tool_metadata_ = tool_metadata\n    chosen_wrapper._original_function_ = func\n\n    return chosen_wrapper\n</code></pre>"},{"location":"api/decorators/","title":"Decorators","text":""},{"location":"api/decorators/#genie_tooling.decorators","title":"genie_tooling.decorators","text":""},{"location":"api/decorators/#genie_tooling.decorators-functions","title":"Functions","text":""},{"location":"api/decorators/#genie_tooling.decorators.tool","title":"tool","text":"<pre><code>tool(func: Callable) -&gt; Callable\n</code></pre> <p>Decorator to mark a function as a Genie Tool and auto-generate its metadata.</p> Source code in <code>src/genie_tooling/decorators.py</code> <pre><code>def tool(func: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator to mark a function as a Genie Tool and auto-generate its metadata.\n    \"\"\"\n    globalns = getattr(func, \"__globals__\", {})\n    try:\n        type_hints = get_type_hints(func, globalns=globalns)\n    except NameError as e:\n        try:\n            type_hints = get_type_hints(func)\n        except Exception:\n            type_hints = {}\n            print(f\"Warning: Could not fully resolve type hints for {func.__name__} due to {e}. Schemas might be incomplete.\")\n\n    sig = inspect.signature(func)\n    docstring = inspect.getdoc(func) or \"\"\n\n    main_description = docstring.split(\"\\n\\n\")[0].strip()\n    if not main_description and func.__name__:\n        main_description = f\"Executes the '{func.__name__}' tool.\"\n\n    param_descriptions_from_doc = _parse_docstring_for_params(docstring)\n\n    properties: Dict[str, Any] = {}\n    required_params: List[str] = []\n\n    for name, param in sig.parameters.items():\n        if name == \"self\" or name == \"cls\" or \\\n           param.kind == inspect.Parameter.VAR_POSITIONAL or \\\n           param.kind == inspect.Parameter.VAR_KEYWORD:\n            continue\n\n        param_py_type = type_hints.get(name, Any)\n\n        if isinstance(param_py_type, str):\n            try:\n                # MODIFIED: Pass recursive_guard as a keyword argument\n                param_py_type = ForwardRef(param_py_type)._evaluate(globalns, {}, recursive_guard=frozenset())\n            except Exception:\n                 pass\n\n        is_optional_hint = False\n        origin = getattr(param_py_type, \"__origin__\", None)\n        args = getattr(param_py_type, \"__args__\", None)\n        if origin is Union and type(None) in (args or []):\n            is_optional_hint = True\n            if args:\n                param_py_type = next((t for t in args if t is not type(None)), Any)\n\n        schema_type_def = _map_type_to_json_schema(param_py_type)\n\n        # If _map_type_to_json_schema returned {} (for Any), default to string for schema\n        if not schema_type_def and param_py_type is Any: # MODIFIED\n            schema_type_def = {\"type\": \"string\"}\n\n        param_info_schema = schema_type_def\n        param_info_schema[\"description\"] = param_descriptions_from_doc.get(name, f\"Parameter '{name}'.\")\n\n        if param.default is inspect.Parameter.empty:\n            if not is_optional_hint:\n                required_params.append(name)\n        else:\n            param_info_schema[\"default\"] = param.default\n\n        properties[name] = param_info_schema\n\n    input_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": properties}\n    if required_params:\n        input_schema[\"required\"] = required_params\n\n    return_py_type = type_hints.get(\"return\", Any)\n    if isinstance(return_py_type, str):\n        try:\n            # MODIFIED: Pass recursive_guard as a keyword argument\n            return_py_type = ForwardRef(return_py_type)._evaluate(globalns, {}, recursive_guard=frozenset())\n        except Exception:\n            pass\n\n    output_schema_prop_def = _map_type_to_json_schema(return_py_type)\n\n    output_schema: Dict[str, Any] = {\n        \"type\": \"object\",\n        \"properties\": {\"result\": output_schema_prop_def},\n    }\n    if output_schema_prop_def.get(\"type\") != \"null\":\n         output_schema[\"required\"] = [\"result\"]\n\n    tool_metadata = {\n        \"identifier\": func.__name__,\n        \"name\": func.__name__.replace(\"_\", \" \").title(),\n        \"description_human\": main_description,\n        \"description_llm\": main_description,\n        \"input_schema\": input_schema,\n        \"output_schema\": output_schema,\n        \"key_requirements\": [],\n        \"tags\": [\"decorated_tool\"],\n        \"version\": \"1.0.0\",\n        \"cacheable\": False,\n    }\n\n    @wraps(func)\n    async def async_wrapper(*args, **kwargs):\n        return await func(*args, **kwargs)\n\n    @wraps(func)\n    def sync_wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n\n    chosen_wrapper = async_wrapper if inspect.iscoroutinefunction(func) else sync_wrapper\n\n    chosen_wrapper._tool_metadata_ = tool_metadata\n    chosen_wrapper._original_function_ = func\n\n    return chosen_wrapper\n</code></pre>"},{"location":"api/genie_facade/","title":"Genie Facade and Interfaces","text":""},{"location":"api/genie_facade/#genie-facade","title":"Genie Facade","text":""},{"location":"api/genie_facade/#genie_tooling.genie.Genie","title":"genie_tooling.genie.Genie","text":"<pre><code>Genie(\n    plugin_manager: PluginManager,\n    key_provider: KeyProvider,\n    config: MiddlewareConfig,\n    tool_manager: ToolManager,\n    tool_invoker: ToolInvoker,\n    rag_manager: RAGManager,\n    tool_lookup_service: ToolLookupService,\n    llm_provider_manager: LLMProviderManager,\n    command_processor_manager: CommandProcessorManager,\n    tracing_manager: InteractionTracingManager,\n    hitl_manager: HITLManager,\n    token_usage_manager: TokenUsageManager,\n    guardrail_manager: GuardrailManager,\n    prompt_manager: PromptManager,\n    conversation_manager: ConversationStateManager,\n    llm_output_parser_manager: LLMOutputParserManager,\n    task_queue_manager: DistributedTaskQueueManager,\n    llm_interface: LLMInterface,\n    rag_interface: RAGInterface,\n    observability_interface: ObservabilityInterface,\n    hitl_interface: HITLInterface,\n    usage_tracking_interface: UsageTrackingInterface,\n    prompt_interface: PromptInterface,\n    conversation_interface: ConversationInterface,\n    task_queue_interface: TaskQueueInterface,\n)\n</code></pre> Source code in <code>src/genie_tooling/genie.py</code> <pre><code>def __init__(\n    self, plugin_manager: PluginManager, key_provider: KeyProvider,\n    config: MiddlewareConfig, tool_manager: ToolManager,\n    tool_invoker: ToolInvoker, rag_manager: RAGManager,\n    tool_lookup_service: ToolLookupService, llm_provider_manager: LLMProviderManager,\n    command_processor_manager: CommandProcessorManager,\n    tracing_manager: InteractionTracingManager, hitl_manager: HITLManager,\n    token_usage_manager: TokenUsageManager, guardrail_manager: GuardrailManager,\n    prompt_manager: PromptManager, conversation_manager: ConversationStateManager,\n    llm_output_parser_manager: LLMOutputParserManager,\n    task_queue_manager: DistributedTaskQueueManager, # Added for P2.5.D\n    llm_interface: LLMInterface, rag_interface: RAGInterface,\n    observability_interface: ObservabilityInterface, hitl_interface: HITLInterface,\n    usage_tracking_interface: UsageTrackingInterface,\n    prompt_interface: PromptInterface, conversation_interface: ConversationInterface,\n    task_queue_interface: TaskQueueInterface # Added for P2.5.D\n):\n    self._plugin_manager = plugin_manager\n    self._key_provider = key_provider\n    self._config = config\n    self._tool_manager = tool_manager\n    self._tool_invoker = tool_invoker\n    self._rag_manager = rag_manager\n    self._tool_lookup_service = tool_lookup_service\n    self._llm_provider_manager = llm_provider_manager\n    self._command_processor_manager = command_processor_manager\n    self._tracing_manager = tracing_manager\n    self._hitl_manager = hitl_manager\n    self._token_usage_manager = token_usage_manager\n    self._guardrail_manager = guardrail_manager\n    self._prompt_manager = prompt_manager\n    self._conversation_manager = conversation_manager\n    self._llm_output_parser_manager = llm_output_parser_manager\n    self._task_queue_manager = task_queue_manager # Added for P2.5.D\n\n    self.llm = llm_interface\n    self.rag = rag_interface\n    self.observability = observability_interface\n    self.human_in_loop = hitl_interface\n    self.usage = usage_tracking_interface\n    self.prompts = prompt_interface\n    self.conversation = conversation_interface\n    self.task_queue = task_queue_interface # Added for P2.5.D\n\n    self._config._genie_instance = self # type: ignore\n    logger.info(\"Genie facade initialized with resolved configuration.\")\n</code></pre>"},{"location":"api/genie_facade/#core-interfaces","title":"Core Interfaces","text":"<p>These interfaces are accessed via attributes on the <code>Genie</code> facade instance (e.g., <code>genie.llm</code>, <code>genie.rag</code>).</p>"},{"location":"api/genie_facade/#genie_tooling.interfaces.LLMInterface","title":"genie_tooling.interfaces.LLMInterface","text":"<pre><code>LLMInterface(\n    llm_provider_manager: LLMProviderManager,\n    default_provider_id: Optional[str],\n    output_parser_manager: LLMOutputParserManager,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n    guardrail_manager: Optional[GuardrailManager] = None,\n    token_usage_manager: Optional[TokenUsageManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(\n    self,\n    llm_provider_manager: \"LLMProviderManager\",\n    default_provider_id: Optional[str],\n    output_parser_manager: \"LLMOutputParserManager\",\n    tracing_manager: Optional[\"InteractionTracingManager\"] = None,\n    guardrail_manager: Optional[\"GuardrailManager\"] = None,\n    token_usage_manager: Optional[\"TokenUsageManager\"] = None\n):\n    self._llm_provider_manager = llm_provider_manager\n    self._default_provider_id = default_provider_id\n    self._output_parser_manager = output_parser_manager\n    self._tracing_manager = tracing_manager\n    self._guardrail_manager = guardrail_manager\n    self._token_usage_manager = token_usage_manager\n    logger.debug(f\"LLMInterface initialized with default provider ID: {self._default_provider_id}\")\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.RAGInterface","title":"genie_tooling.interfaces.RAGInterface","text":"<pre><code>RAGInterface(\n    rag_manager: RAGManager,\n    config: MiddlewareConfig,\n    key_provider: KeyProvider,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, rag_manager: \"RAGManager\", config: \"MiddlewareConfig\", key_provider: \"KeyProvider\", tracing_manager: Optional[\"InteractionTracingManager\"] = None):\n    self._rag_manager = rag_manager\n    self._config = config\n    self._key_provider = key_provider\n    self._tracing_manager = tracing_manager\n    logger.debug(\"RAGInterface initialized.\")\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.ObservabilityInterface","title":"genie_tooling.interfaces.ObservabilityInterface","text":"<pre><code>ObservabilityInterface(tracing_manager: InteractionTracingManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, tracing_manager: \"InteractionTracingManager\"): self._tracing_manager = tracing_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.HITLInterface","title":"genie_tooling.interfaces.HITLInterface","text":"<pre><code>HITLInterface(hitl_manager: HITLManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, hitl_manager: \"HITLManager\"): self._hitl_manager = hitl_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.UsageTrackingInterface","title":"genie_tooling.interfaces.UsageTrackingInterface","text":"<pre><code>UsageTrackingInterface(token_usage_manager: TokenUsageManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, token_usage_manager: \"TokenUsageManager\"): self._token_usage_manager = token_usage_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.PromptInterface","title":"genie_tooling.interfaces.PromptInterface","text":"<pre><code>PromptInterface(prompt_manager: PromptManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, prompt_manager: \"PromptManager\"): self._prompt_manager = prompt_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.ConversationInterface","title":"genie_tooling.interfaces.ConversationInterface","text":"<pre><code>ConversationInterface(conversation_manager: Optional[ConversationStateManager])\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, conversation_manager: Optional[\"ConversationStateManager\"]): # Allow Optional manager\n    self._conversation_manager = conversation_manager\n    if not self._conversation_manager:\n        logger.warning(\"ConversationInterface initialized without a ConversationStateManager. Operations will be no-ops or return defaults.\")\n</code></pre>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>Genie Tooling is configured at runtime using a <code>MiddlewareConfig</code> object. For ease of use, especially for common setups, <code>MiddlewareConfig</code> integrates a <code>FeatureSettings</code> model.</p>"},{"location":"guides/configuration/#simplified-configuration-with-featuresettings","title":"Simplified Configuration with <code>FeatureSettings</code>","text":"<p>The recommended way to start configuring Genie is by using the <code>features</code> attribute of <code>MiddlewareConfig</code>. <code>FeatureSettings</code> provides high-level toggles and default choices for major components like LLM providers, RAG components, caching, tool lookup, observability, HITL, token usage, guardrails, prompt system, conversation state, and distributed task queues.</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",  # Use Ollama as the default LLM\n        llm_ollama_model_name=\"mistral:latest\", # Specify the Ollama model\n\n        command_processor=\"llm_assisted\", # Use LLM-assisted tool selection\n        tool_lookup=\"embedding\",          # Use embedding-based tool lookup for the LLM processor\n\n        rag_embedder=\"sentence_transformer\", # Embedder for RAG\n        rag_vector_store=\"faiss\",            # Vector store for RAG\n\n        cache=\"in-memory\", # Use in-memory cache\n\n        observability_tracer=\"console_tracer\", # Log traces to console\n        hitl_approver=\"cli_hitl_approver\",       # Use CLI for human approvals\n        token_usage_recorder=\"in_memory_token_recorder\", # Track token usage in memory\n        input_guardrails=[\"keyword_blocklist_guardrail\"], # Enable a keyword blocklist for inputs\n\n        prompt_registry=\"file_system_prompt_registry\",\n        prompt_template_engine=\"jinja2_chat_formatter\",\n        conversation_state_provider=\"in_memory_convo_provider\",\n        default_llm_output_parser=\"json_output_parser\",\n\n        task_queue=\"celery\", # Example: Use Celery for distributed tasks\n        task_queue_celery_broker_url=\"redis://localhost:6379/1\",\n        task_queue_celery_backend_url=\"redis://localhost:6379/2\"\n    ),\n    # Enable and configure tools explicitly\n    tool_configurations={\n        \"calculator_tool\": {}, # Enable calculator tool (no specific config needed)\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": \"./my_agent_workspace_feature_example\"}\n    },\n    # Configure the keyword blocklist guardrail\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": { # Canonical ID\n            \"blocklist\": [\"sensitive_data_pattern\", \"forbidden_command\"],\n            \"action_on_match\": \"block\"\n        }\n    },\n    # Configure prompt registry if file_system_prompt_registry is used\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\"base_path\": \"./my_prompts\"}\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#how-featuresettings-works","title":"How <code>FeatureSettings</code> Works","text":"<p>When you initialize <code>Genie</code> with a <code>MiddlewareConfig</code> containing <code>FeatureSettings</code>, an internal <code>ConfigResolver</code> processes these settings. It translates your high-level choices into specific plugin IDs and default configurations for those plugins.</p> <p>For example, setting <code>features.llm = \"ollama\"</code> will make the resolver: 1.  Set <code>default_llm_provider_id</code> to the canonical ID of the Ollama LLM provider (e.g., <code>\"ollama_llm_provider_v1\"</code>). 2.  Populate a basic configuration for this provider in <code>llm_provider_configurations</code>, including the specified <code>llm_ollama_model_name</code>.</p> <p>Similarly, <code>features.input_guardrails=[\"keyword_blocklist_guardrail\"]</code> will add the canonical ID of the <code>KeywordBlocklistGuardrailPlugin</code> to <code>default_input_guardrail_ids</code>.</p> <p>Setting <code>features.rag_vector_store = \"qdrant\"</code> along with <code>rag_vector_store_qdrant_url</code> and <code>rag_vector_store_qdrant_embedding_dim</code> would set <code>default_rag_vector_store_id</code> to <code>\"qdrant_vector_store_v1\"</code> and populate its configuration in <code>vector_store_configurations</code> with the URL, collection name, and embedding dimension.</p> <p>This layered approach (Features -&gt; Explicit Defaults -&gt; Explicit Plugin Configs) provides both ease of use for common cases and fine-grained control when needed.</p>"},{"location":"guides/configuration/#aliases","title":"Aliases","text":"<p>The <code>ConfigResolver</code> uses a system of aliases to map short, user-friendly names (like \"ollama\", \"st_embedder\", \"console_tracer\") to their full canonical plugin IDs. This makes configuration more concise.</p> <p>For a full list of available aliases and more details on simplified configuration, please see the Simplified Configuration Guide.</p>"},{"location":"guides/configuration/#explicit-overrides-and-detailed-configuration","title":"Explicit Overrides and Detailed Configuration","text":"<p>While <code>FeatureSettings</code> provides a convenient starting point, you can always provide more detailed, explicit configurations that will override or augment the settings derived from features.</p> <p>You can directly set default plugin IDs for any component:</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(llm=\"openai\"), # Base feature\n    default_llm_provider_id=\"my_custom_openai_provider_v2\", # Override default ID\n    default_observability_tracer_id=\"my_custom_tracer_v1\" \n)\n</code></pre> <p>You can also provide specific configurations for individual plugins using the various <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code>. These dictionaries are keyed by the canonical plugin ID (or a recognized alias, which the resolver will convert).</p> <p>Crucially, for tools to be active and usable by <code>genie.execute_tool</code> or <code>genie.run_command</code>, their plugin ID (canonical or alias) must be a key in the <code>tool_configurations</code> dictionary. If a tool requires no specific settings, an empty dictionary <code>{}</code> as its value is sufficient to enable it.</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\", \n        observability_tracer=\"console_tracer\"\n    ),\n    # Override configuration for the OpenAI LLM provider\n    llm_provider_configurations={\n        \"openai_llm_provider_v1\": { # Canonical ID\n            \"model_name\": \"gpt-4-turbo-preview\", \n            \"request_timeout_seconds\": 120\n        },\n        # Example for Llama.cpp Internal Provider\n        \"llama_cpp_internal_llm_provider_v1\": {\n            \"model_path\": \"/path/to/your/model.gguf\",\n            \"n_gpu_layers\": -1,\n            \"chat_format\": \"mistral\"\n        }\n    },\n    # Enable and configure specific tools\n    tool_configurations={\n        \"calculator_tool\": {}, # Enable calculator, no specific config needed\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": \"./agent_workspace\"}\n    },\n    # Configure a specific observability tracer\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": {\"log_level\": \"DEBUG\"}\n    },\n    # Configure a specific guardrail\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": {\n            \"blocklist\": [\"confidential_project_alpha\"],\n            \"case_sensitive\": True,\n            \"action_on_match\": \"warn\"\n        }\n    },\n    # Configure a specific prompt registry\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\"base_path\": \"path/to/prompts\", \"template_suffix\": \".txt\"}\n    },\n    # Configure a specific conversation state provider\n    conversation_state_provider_configurations={\n        \"redis_conversation_state_v1\": {\"redis_url\": \"redis://localhost:6379/1\"}\n    },\n    # Configure a specific LLM output parser\n    llm_output_parser_configurations={\n        \"json_output_parser_v1\": {\"strict_parsing\": False}\n    },\n    # Configure a specific task queue\n    distributed_task_queue_configurations={\n        \"celery_task_queue_v1\": {\n            \"celery_app_name\": \"genie_worker_app_explicit\",\n            \"celery_broker_url\": \"amqp://guest:guest@localhost:5672//\", # Example AMQP\n            \"celery_backend_url\": \"redis://localhost:6379/3\"\n        }\n    }\n)\n</code></pre> <p>Key Points for Explicit Configuration:</p> <ul> <li>Precedence: Explicit configurations in <code>default_*_id</code> fields or within the <code>*_configurations</code> dictionaries take precedence over settings derived from <code>FeatureSettings</code>.</li> <li>Tool Enablement: A tool plugin is only loaded and made active if its ID (or alias) is present as a key in the <code>tool_configurations</code> dictionary.</li> <li>Canonical IDs vs. Aliases: When providing explicit configurations in dictionaries like <code>llm_provider_configurations</code> or <code>tool_configurations</code>, you can use either the canonical plugin ID (e.g., <code>\"ollama_llm_provider_v1\"</code>) or a recognized alias (e.g., <code>\"ollama\"</code>). The <code>ConfigResolver</code> will map aliases to their canonical IDs.</li> <li>KeyProvider: API keys are managed by a <code>KeyProvider</code> implementation. Genie defaults to <code>EnvironmentKeyProvider</code> (alias <code>\"env_keys\"</code>) if no <code>key_provider_instance</code> is passed to <code>Genie.create()</code> and <code>key_provider_id</code> is not set. Plugins requiring keys (like OpenAI or Gemini providers) will receive the configured <code>KeyProvider</code> instance.</li> </ul>"},{"location":"guides/configuration/#plugin-development-directories","title":"Plugin Development Directories","text":"<p>If you have custom plugins located outside your main Python path or not installed as entry points, you can specify their location:</p> <p><pre><code>app_config = MiddlewareConfig(\n    plugin_dev_dirs=[\"/path/to/my/custom_plugins\", \"./project_plugins\"]\n)\n</code></pre> The <code>PluginManager</code> will scan these directories for valid plugin classes. Discovered plugins still need to be explicitly enabled via their respective configuration sections (e.g., <code>tool_configurations</code> for tools) to be loaded by <code>Genie</code>.</p>"},{"location":"guides/creating_other_plugins/","title":"Creating Other Plugins","text":"<p>Beyond tools and RAG components, Genie Tooling's pluggable architecture allows for customization of many other functionalities. This guide provides an overview of how to create various other types of plugins.</p>"},{"location":"guides/creating_other_plugins/#general-plugin-principles","title":"General Plugin Principles","text":"<p>All plugins in Genie Tooling typically adhere to the <code>genie_tooling.core.types.Plugin</code> protocol:</p> <pre><code>from typing import Protocol, Optional, Dict, Any\n\nclass Plugin(Protocol):\n    @property\n    def plugin_id(self) -&gt; str:\n        ... # Unique identifier for the plugin\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n        pass # Optional async setup\n\n    async def teardown(self) -&gt; None:\n        pass # Optional async teardown\n</code></pre> <p>Your custom plugin class should: 1.  Define a unique <code>plugin_id</code> as a class attribute. 2.  Implement the specific protocol for the type of plugin you are creating (e.g., <code>LLMProviderPlugin</code>, <code>CacheProviderPlugin</code>). 3.  Implement <code>async def setup(self, config: Optional[Dict[str, Any]] = None)</code> if your plugin requires initialization with configuration. 4.  Implement <code>async def teardown(self)</code> if your plugin needs to release resources.</p>"},{"location":"guides/creating_other_plugins/#registering-your-plugin","title":"Registering Your Plugin","text":"<p>Make your plugin discoverable by Genie: *   Entry Points: Define an entry point in your <code>pyproject.toml</code> under the <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> group.     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_custom_cache_v1\" = \"my_package.my_module:MyCustomCacheProvider\"\n</code></pre> *   Plugin Development Directories: Place your plugin's Python file in a directory specified in <code>MiddlewareConfig.plugin_dev_dirs</code>.</p>"},{"location":"guides/creating_other_plugins/#common-plugin-categories-and-their-protocols","title":"Common Plugin Categories and Their Protocols","text":"<p>Refer to the <code>src/genie_tooling/</code> subdirectories for the specific abstract base classes (ABCs) or protocols for each plugin type. Key examples include:</p> <ul> <li>Key Providers: <code>genie_tooling.security.key_provider.KeyProvider</code><ul> <li>Implement <code>async def get_key(self, key_name: str) -&gt; Optional[str]</code></li> </ul> </li> <li>LLM Providers: <code>genie_tooling.llm_providers.abc.LLMProviderPlugin</code><ul> <li>Implement <code>async def generate(...)</code> and/or <code>async def chat(...)</code>.</li> </ul> </li> <li>Command Processors: <code>genie_tooling.command_processors.abc.CommandProcessorPlugin</code><ul> <li>Implement <code>async def process_command(...)</code>.</li> </ul> </li> <li>Definition Formatters: <code>genie_tooling.definition_formatters.abc.DefinitionFormatter</code><ul> <li>Implement <code>def format(...)</code>.</li> </ul> </li> <li>Caching Providers: <code>genie_tooling.cache_providers.abc.CacheProvider</code><ul> <li>Implement <code>async def get(...)</code>, <code>async def set(...)</code>, <code>async def delete(...)</code>, etc.</li> </ul> </li> <li>Invocation Strategies: <code>genie_tooling.invocation_strategies.abc.InvocationStrategy</code><ul> <li>Implement <code>async def invoke(...)</code>.</li> </ul> </li> <li>Input Validators: <code>genie_tooling.input_validators.abc.InputValidator</code><ul> <li>Implement <code>def validate(...)</code>.</li> </ul> </li> <li>Output Transformers: <code>genie_tooling.output_transformers.abc.OutputTransformer</code><ul> <li>Implement <code>def transform(...)</code>.</li> </ul> </li> <li>Error Handlers &amp; Formatters: <code>genie_tooling.error_handlers.abc.ErrorHandler</code>, <code>genie_tooling.error_formatters.abc.ErrorFormatter</code></li> <li>Log Adapters &amp; Redactors: <code>genie_tooling.log_adapters.abc.LogAdapter</code>, <code>genie_tooling.redactors.abc.Redactor</code></li> <li>Code Executors: <code>genie_tooling.code_executors.abc.CodeExecutor</code></li> <li>Observability Tracers: <code>genie_tooling.observability.abc.InteractionTracerPlugin</code></li> <li>HITL Approvers: <code>genie_tooling.hitl.abc.HumanApprovalRequestPlugin</code></li> <li>Token Usage Recorders: <code>genie_tooling.token_usage.abc.TokenUsageRecorderPlugin</code></li> <li>Guardrail Plugins: <code>genie_tooling.guardrails.abc.InputGuardrailPlugin</code>, <code>OutputGuardrailPlugin</code>, <code>ToolUsageGuardrailPlugin</code></li> <li>Prompt System Plugins: <code>genie_tooling.prompts.abc.PromptRegistryPlugin</code>, <code>PromptTemplatePlugin</code></li> <li>Conversation State Providers: <code>genie_tooling.prompts.conversation.impl.abc.ConversationStateProviderPlugin</code></li> <li>LLM Output Parsers: <code>genie_tooling.prompts.llm_output_parsers.abc.LLMOutputParserPlugin</code></li> <li>Distributed Task Queues: <code>genie_tooling.task_queues.abc.DistributedTaskQueuePlugin</code></li> </ul>"},{"location":"guides/creating_other_plugins/#configuration","title":"Configuration","text":"<p>Once your plugin is created and registered, you can configure it in <code>MiddlewareConfig</code> using the appropriate <code>*_configurations</code> dictionary, keyed by your plugin's <code>plugin_id</code>.</p> <pre><code># In your MiddlewareConfig\napp_config = MiddlewareConfig(\n    # ...\n    cache_provider_configurations={\n        \"my_custom_cache_v1\": {\n            \"connection_string\": \"...\",\n            \"default_ttl\": 300\n        }\n    },\n    default_cache_provider_id=\"my_custom_cache_v1\" # If it should be the default\n    # ...\n)\n</code></pre> <p>By following the relevant protocol and registering your plugin, you can extend Genie Tooling to meet your specific application needs.</p>"},{"location":"guides/creating_plugins/","title":"Creating Plugins (Overview)","text":"<p>Genie Tooling is designed to be highly extensible through its plugin architecture. Almost every functional component can be replaced or augmented with custom implementations.</p> <p>This page provides a general overview. For specific plugin types, refer to: *   Creating Tool Plugins *   Creating RAG Plugins *   Creating Other Plugins (for caches, LLM providers, etc.)</p>"},{"location":"guides/creating_plugins/#core-plugin-protocol","title":"Core Plugin Protocol","text":"<p>All plugins in Genie Tooling should ideally adhere to the <code>genie_tooling.core.types.Plugin</code> protocol:</p> <pre><code>from typing import Protocol, Optional, Dict, Any\n\nclass Plugin(Protocol):\n    @property\n    def plugin_id(self) -&gt; str:\n        \"\"\"A unique identifier for this plugin instance/type.\"\"\"\n        ...\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Optional asynchronous setup method.\n        Called after the plugin is instantiated, with its specific configuration.\n        \"\"\"\n        pass # Default implementation\n\n    async def teardown(self) -&gt; None:\n        \"\"\"\n        Optional asynchronous teardown method.\n        Called when the Genie facade is closing down, to release resources.\n        \"\"\"\n        pass # Default implementation\n</code></pre> <p>Key Requirements for a Plugin Class:</p> <ol> <li> <p><code>plugin_id</code> (Class Attribute):</p> <ul> <li>A unique string identifier for your plugin.</li> <li>Convention: <code>your_plugin_name_v1</code> (e.g., <code>my_custom_llm_provider_v1</code>, <code>advanced_calculator_tool_v1</code>).</li> <li>This ID is used in configuration files and for discovery.</li> </ul> </li> <li> <p>Implement the Specific Protocol:</p> <ul> <li>Your plugin class must implement the methods defined by the protocol for its type (e.g., <code>Tool</code>, <code>LLMProviderPlugin</code>, <code>CacheProviderPlugin</code>).</li> <li>These protocols are typically found in the <code>abc.py</code> file of the relevant submodule (e.g., <code>genie_tooling.tools.abc.Tool</code>).</li> </ul> </li> <li> <p><code>async def setup(self, config: Optional[Dict[str, Any]] = None)</code> (Optional):</p> <ul> <li>If your plugin needs initialization with configuration values, implement this asynchronous method.</li> <li>The <code>config</code> dictionary will contain the settings provided for your plugin's <code>plugin_id</code> in the <code>MiddlewareConfig</code> (e.g., in <code>llm_provider_configurations[\"my_custom_llm_v1\"]</code>).</li> </ul> </li> <li> <p><code>async def teardown(self)</code> (Optional):</p> <ul> <li>If your plugin acquires resources (e.g., network connections, file handles) that need to be released, implement this asynchronous method.</li> </ul> </li> </ol>"},{"location":"guides/creating_plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>Genie's <code>PluginManager</code> discovers plugins through two primary mechanisms:</p> <ol> <li> <p>Entry Points (Recommended for distributable plugins):</p> <ul> <li>Define an entry point in your package's <code>pyproject.toml</code> under the group <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code>.</li> <li>Example:     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_awesome_tool_v1\" = \"my_package.my_module:MyAwesomeToolClass\"\n\"custom_cache_provider_alpha\" = \"my_other_package.cache:CustomCache\"\n</code></pre></li> <li>The key is the <code>plugin_id</code> Genie will use to refer to your plugin.</li> <li>The value is the import path to your plugin class.</li> </ul> </li> <li> <p>Plugin Development Directories (For local/project-specific plugins):</p> <ul> <li>Specify a list of directories in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> <li>The <code>PluginManager</code> will scan these directories for Python files (<code>*.py</code>).</li> <li>It will attempt to import these files as modules and look for classes that implement the <code>Plugin</code> protocol and have a <code>plugin_id</code>.</li> <li>Files starting with <code>_</code> or <code>.</code> (e.g., <code>__init__.py</code>, <code>_internal_utils.py</code>) are ignored.</li> </ul> </li> </ol>"},{"location":"guides/creating_plugins/#configuration-and-usage","title":"Configuration and Usage","text":"<p>Once your plugin is discoverable, you can: *   Configure it: Provide settings in the appropriate <code>*_configurations</code> dictionary within <code>MiddlewareConfig</code>, keyed by your plugin's <code>plugin_id</code>. *   Set it as a default: If applicable, set its <code>plugin_id</code> in fields like <code>default_llm_provider_id</code>, <code>default_cache_provider_id</code>, etc., in <code>MiddlewareConfig</code> or via <code>FeatureSettings</code>. *   Use it explicitly: Some <code>Genie</code> facade methods allow specifying a <code>plugin_id</code> at runtime (e.g., <code>genie.llm.chat(..., provider_id=\"my_llm_v1\")</code>).</p> <p>By adhering to these principles, you can create robust and reusable extensions for Genie Tooling.</p>"},{"location":"guides/creating_rag_plugins/","title":"Creating RAG Plugins","text":"<p>Genie Tooling's Retrieval Augmented Generation (RAG) system is composed of several pluggable components. You can create custom implementations for each to tailor the RAG pipeline to your specific data sources and needs.</p> <p>The core RAG component protocols are:</p> <ul> <li><code>DocumentLoaderPlugin</code>: Loads raw data from a source (e.g., files, web pages, databases) into <code>Document</code> objects.<ul> <li>Located in: <code>genie_tooling.document_loaders.abc</code></li> <li>Key method: <code>async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]</code></li> </ul> </li> <li><code>TextSplitterPlugin</code>: Splits <code>Document</code> objects into smaller <code>Chunk</code> objects.<ul> <li>Located in: <code>genie_tooling.text_splitters.abc</code></li> <li>Key method: <code>async def split(self, documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Chunk]</code></li> </ul> </li> <li><code>EmbeddingGeneratorPlugin</code>: Generates embedding vectors for <code>Chunk</code> objects.<ul> <li>Located in: <code>genie_tooling.embedding_generators.abc</code></li> <li>Key method: <code>async def embed(self, chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]</code></li> </ul> </li> <li><code>VectorStorePlugin</code>: Stores, manages, and searches <code>Chunk</code> embeddings.<ul> <li>Located in: <code>genie_tooling.vector_stores.abc</code></li> <li>Key methods: <code>async def add(...)</code>, <code>async def search(...)</code>, <code>async def delete(...)</code>.</li> </ul> </li> <li><code>RetrieverPlugin</code>: Orchestrates the process of taking a query, embedding it, and searching a vector store to retrieve relevant chunks. Often composes an <code>EmbeddingGeneratorPlugin</code> and a <code>VectorStorePlugin</code>.<ul> <li>Located in: <code>genie_tooling.retrievers.abc</code></li> <li>Key method: <code>async def retrieve(self, query: str, top_k: int, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]</code></li> </ul> </li> </ul>"},{"location":"guides/creating_rag_plugins/#steps-to-create-a-rag-plugin","title":"Steps to Create a RAG Plugin","text":"<ol> <li>Identify the Component: Determine which part of the RAG pipeline you want to customize (e.g., a new document loader for a specific API, a different text splitting strategy).</li> <li>Implement the Protocol:<ul> <li>Create a Python class that inherits from the relevant plugin protocol (e.g., <code>DocumentLoaderPlugin</code>).</li> <li>Your class must also implicitly or explicitly adhere to the base <code>genie_tooling.core.types.Plugin</code> protocol (by having a <code>plugin_id</code> and optional <code>setup</code>/<code>teardown</code> methods).</li> <li>Implement all required methods from the chosen RAG component protocol. <pre><code># Example: Custom Document Loader\nfrom genie_tooling.core.types import Document\nfrom genie_tooling.document_loaders.abc import DocumentLoaderPlugin\nfrom typing import AsyncIterable, Dict, Any, Optional\n\nclass MyCustomAPILoader(DocumentLoaderPlugin):\n    plugin_id: str = \"my_custom_api_loader_v1\"\n    description: str = \"Loads documents from MyCustomAPI.\"\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None):\n        # Initialize API clients, etc.\n        self.api_endpoint = (config or {}).get(\"api_endpoint\", \"https://api.example.com/data\")\n        # self.key_provider = (config or {}).get(\"key_provider\") # If API key needed\n\n    async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]:\n        # source_uri might be an entity ID or query for your API\n        # api_key = await self.key_provider.get_key(\"MY_CUSTOM_API_KEY\")\n        # ... fetch data from API ...\n        # for item in fetched_data:\n        #     yield Document(content=item['text'], metadata={\"source\": \"MyCustomAPI\", \"id\": item['id']})\n        if False: # Make it an async generator\n             yield\n        pass # Replace with actual implementation\n</code></pre></li> </ul> </li> <li>Register Your Plugin:<ul> <li>Use entry points in <code>pyproject.toml</code> or place it in a <code>plugin_dev_dirs</code> directory.</li> </ul> </li> <li>Configure Genie to Use Your Plugin:<ul> <li>In <code>MiddlewareConfig</code>, update the relevant <code>features</code> settings (e.g., <code>features.rag_loader = \"my_custom_api_loader_v1\"</code>) or explicitly set the default ID (e.g., <code>default_rag_loader_id = \"my_custom_api_loader_v1\"</code>).</li> <li>Provide any necessary configuration for your plugin in the corresponding <code>*_configurations</code> dictionary (e.g., <code>document_loader_configurations[\"my_custom_api_loader_v1\"] = {\"api_endpoint\": \"...\"}</code>).</li> </ul> </li> </ol>"},{"location":"guides/creating_rag_plugins/#example-using-a-custom-rag-component","title":"Example: Using a Custom RAG Component","text":"<p><pre><code># In your MiddlewareConfig\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        rag_loader=\"my_custom_api_loader_v1\", # Use your custom loader\n        rag_embedder=\"st_embedder\",\n        rag_vector_store=\"faiss_vs\"\n    ),\n    document_loader_configurations={\n        \"my_custom_api_loader_v1\": {\n            \"api_endpoint\": \"https://my.service.com/data_source\",\n            # \"key_provider\": my_key_provider_instance (if needed and passed to Genie.create)\n        }\n    }\n)\n\n# genie = await Genie.create(config=app_config, key_provider_instance=my_key_provider_instance)\n# await genie.rag.index_directory(source_uri=\"query_for_my_api\", collection_name=\"custom_data\")\n</code></pre> The <code>RAGManager</code> (used internally by <code>genie.rag</code>) will then pick up and use your custom plugin based on the configuration.</p>"},{"location":"guides/creating_tool_plugins/","title":"Creating Tool Plugins","text":"<p>Tools are fundamental to Genie Tooling, representing actions an agent can perform. You can create custom tools by implementing the <code>Tool</code> protocol.</p>"},{"location":"guides/creating_tool_plugins/#the-tool-protocol","title":"The <code>Tool</code> Protocol","text":"<p>Located in <code>genie_tooling.tools.abc.Tool</code>, the protocol requires:</p> <p><pre><code>from typing import Protocol, Any, Dict, Optional\nfrom genie_tooling.core.types import Plugin # For base Plugin behavior\nfrom genie_tooling.security.key_provider import KeyProvider\n\nclass Tool(Plugin, Protocol):\n    @property\n    def identifier(self) -&gt; str:\n        \"\"\"A unique string identifier for this tool.\"\"\"\n        ...\n\n    async def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns comprehensive metadata about the tool.\n        Expected structure:\n        {\n            \"identifier\": str,\n            \"name\": str, (Human-friendly)\n            \"description_human\": str, (Detailed for UI/developers)\n            \"description_llm\": str, (Concise for LLM prompts)\n            \"input_schema\": Dict[str, Any], (JSON Schema for parameters)\n            \"output_schema\": Dict[str, Any], (JSON Schema for result)\n            \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", ...}])\n            \"tags\": List[str],\n            \"version\": str,\n            \"cacheable\": bool, (Optional, default False)\n            \"cache_ttl_seconds\": Optional[int] (Optional)\n        }\n        \"\"\"\n        ...\n\n    async def execute(\n        self,\n        params: Dict[str, Any],\n        key_provider: KeyProvider,\n        context: Optional[Dict[str, Any]] = None\n    ) -&gt; Any:\n        \"\"\"Executes the tool with validated parameters.\"\"\"\n        ...\n</code></pre> Your tool class must also have a <code>plugin_id</code> attribute (usually the same as <code>identifier</code>).</p>"},{"location":"guides/creating_tool_plugins/#steps-to-create-a-tool-plugin","title":"Steps to Create a Tool Plugin","text":"<ol> <li> <p>Define Your Class:     <pre><code>from genie_tooling.tools.abc import Tool\nfrom genie_tooling.security.key_provider import KeyProvider\nfrom typing import Dict, Any, Optional\n\nclass MyCustomSearchTool(Tool):\n    plugin_id: str = \"my_custom_search_tool_v1\" # Unique plugin ID\n\n    @property\n    def identifier(self) -&gt; str:\n        return self.plugin_id # Often same as plugin_id\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None):\n        self.api_base_url = (config or {}).get(\"api_base_url\", \"https://api.customsearch.com\")\n        # Initialize HTTP client, etc.\n\n    async def get_metadata(self) -&gt; Dict[str, Any]:\n        return {\n            \"identifier\": self.identifier,\n            \"name\": \"My Custom Search\",\n            \"description_human\": \"Searches my custom data source.\",\n            \"description_llm\": \"CustomSearch: Finds items in my data. Args: query (str, req), limit (int, opt, default 10).\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query.\"},\n                    \"limit\": {\"type\": \"integer\", \"default\": 10, \"description\": \"Max results.\"}\n                },\n                \"required\": [\"query\"]\n            },\n            \"output_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"results\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}},\n                    \"error\": {\"type\": [\"string\", \"null\"]}\n                },\n                \"required\": [\"results\"]\n            },\n            \"key_requirements\": [{\"name\": \"MY_CUSTOM_API_KEY\", \"description\": \"API key for custom search.\"}],\n            \"tags\": [\"search\", \"custom\"],\n            \"version\": \"1.0.0\"\n        }\n\n    async def execute(\n        self, \n        params: Dict[str, Any], \n        key_provider: KeyProvider, \n        context: Optional[Dict[str, Any]] = None\n    ) -&gt; Any:\n        query = params[\"query\"]\n        limit = params.get(\"limit\", 10)\n        api_key = await key_provider.get_key(\"MY_CUSTOM_API_KEY\")\n        if not api_key:\n            return {\"results\": [], \"error\": \"API key not found.\"}\n\n        # ... actual search logic using self.api_base_url, query, limit, api_key ...\n        # For example:\n        # response = await self._http_client.get(f\"{self.api_base_url}/search?q={query}&amp;limit={limit}&amp;key={api_key}\")\n        # search_results = response.json() \n        search_results = [{\"title\": f\"Mock result for {query}\"}] # Placeholder\n        return {\"results\": search_results, \"error\": None}\n\n    async def teardown(self):\n        # Close HTTP client, etc.\n        pass\n</code></pre></p> </li> <li> <p>Register Your Plugin:</p> <ul> <li>Add an entry point in <code>pyproject.toml</code>:     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_custom_search_tool_v1\" = \"my_package.tools:MyCustomSearchTool\"\n</code></pre></li> <li>Or, place the Python file in a directory listed in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> </ul> </li> <li> <p>Enable and Configure in <code>MiddlewareConfig</code>:     <pre><code>app_config = MiddlewareConfig(\n    # ...\n    tool_configurations={\n        \"my_custom_search_tool_v1\": { # This is the plugin_id\n            \"api_base_url\": \"https://prod.customsearch.com/v2\" # Configuration for its setup()\n        }\n    }\n    # ...\n)\n</code></pre> Important: Your tool will only be loaded and available if its <code>plugin_id</code> is a key in the <code>tool_configurations</code> dictionary. An empty dictionary <code>{}</code> as the value is sufficient if no specific configuration is needed for <code>setup()</code>.</p> </li> </ol> <p>Now, your <code>MyCustomSearchTool</code> can be used by <code>genie.execute_tool(\"my_custom_search_tool_v1\", ...)</code> or selected by a command processor.</p>"},{"location":"guides/distributed_tasks/","title":"Using Distributed Task Queues","text":"<p>Genie Tooling supports offloading tasks, such as long-running tool executions, to distributed task queues like Celery or RQ. This allows your main agent loop to remain responsive.</p>"},{"location":"guides/distributed_tasks/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>TaskQueueInterface</code> (<code>genie.task_queue</code>): Facade for submitting and managing distributed tasks.</li> <li><code>DistributedTaskQueuePlugin</code>: Plugin for a specific task queue system.<ul> <li>Built-in: <code>CeleryTaskQueuePlugin</code> (alias: <code>celery_task_queue</code>), <code>RedisQueueTaskPlugin</code> (alias: <code>rq_task_queue</code>).</li> </ul> </li> <li><code>DistributedTaskInvocationStrategy</code>: An <code>InvocationStrategy</code> that uses a task queue plugin to execute tools. (Note: This strategy is currently more conceptual and may require further application-side setup for robust use.)</li> </ul>"},{"location":"guides/distributed_tasks/#configuration","title":"Configuration","text":"<p>Enable and configure your chosen task queue system via <code>FeatureSettings</code>:</p> <p><pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\n# Example for Celery\napp_config_celery = MiddlewareConfig(\n    features=FeatureSettings(\n        task_queue=\"celery\", \n        task_queue_celery_broker_url=\"redis://localhost:6379/1\", # Your Celery broker\n        task_queue_celery_backend_url=\"redis://localhost:6379/2\", # Your Celery result backend\n    ),\n    # Optional: Further Celery-specific configurations\n    distributed_task_queue_configurations={\n        \"celery_task_queue_v1\": { # Canonical ID\n            \"celery_app_name\": \"my_genie_worker_app\",\n            # \"celery_include_task_paths\": [\"my_project.worker_tasks\"] # Paths for Celery to find tasks\n        }\n    }\n)\n\n# Example for RQ (Redis Queue)\napp_config_rq = MiddlewareConfig(\n    features=FeatureSettings(\n        task_queue=\"rq\" \n        # RQ typically uses a single Redis connection, configured in its plugin settings\n    ),\n    distributed_task_queue_configurations={\n        \"redis_queue_task_plugin_v1\": { # Canonical ID\n            \"redis_url\": \"redis://localhost:6379/3\", # Redis for RQ\n            \"default_queue_name\": \"genie-rq-default\"\n        }\n    }\n)\n</code></pre> Ensure your chosen task queue broker (e.g., Redis, RabbitMQ) is running and accessible.</p>"},{"location":"guides/distributed_tasks/#using-genietask_queue-direct-task-submission","title":"Using <code>genie.task_queue</code> (Direct Task Submission)","text":"<p>This interface allows you to directly submit tasks to the configured queue. You need to have tasks defined and registered with your Celery/RQ worker environment.</p> <pre><code># Assuming 'genie' is initialized with a task queue configured (e.g., Celery)\n# and you have a Celery task defined as 'my_project.tasks.long_computation'\n\ntask_id = await genie.task_queue.submit_task(\n    task_name=\"my_project.tasks.long_computation\", \n    args=(10, 20),\n    kwargs={\"operation\": \"multiply\"},\n    task_options={\"countdown\": 5} # Example: Celery-specific option\n)\n\nif task_id:\n    print(f\"Task submitted with ID: {task_id}\")\n\n    # Poll for status and result (example polling loop)\n    for _ in range(30): # Try for 30 seconds\n        status = await genie.task_queue.get_task_status(task_id)\n        print(f\"Task {task_id} status: {status}\")\n        if status == \"success\":\n            result = await genie.task_queue.get_task_result(task_id)\n            print(f\"Task result: {result}\")\n            break\n        elif status in [\"failure\", \"revoked\"]:\n            print(f\"Task failed or was revoked.\")\n            # Optionally try to get error details if it failed\n            # result = await genie.task_queue.get_task_result(task_id) \n            break\n        await asyncio.sleep(1)\nelse:\n    print(\"Failed to submit task.\")\n</code></pre>"},{"location":"guides/distributed_tasks/#using-distributedtaskinvocationstrategy-for-tools-conceptual","title":"Using <code>DistributedTaskInvocationStrategy</code> for Tools (Conceptual)","text":"<p>The <code>DistributedTaskInvocationStrategy</code> aims to automatically offload tool executions to a task queue. This is a more advanced setup.</p> <ol> <li>Configure Task Queue: Ensure a task queue (e.g., Celery) is configured in <code>FeatureSettings</code>.</li> <li>Worker Task: Define a generic task in your Celery/RQ worker environment that can execute Genie tools. This task would typically:<ul> <li>Receive <code>tool_id</code>, <code>tool_params</code>, and potentially serialized key information or context.</li> <li>Instantiate a minimal Genie environment (or have one pre-configured).</li> <li>Execute the specified tool with the given parameters.</li> <li>Return the tool's result.</li> <li>Example task name: <code>genie_tooling.worker_tasks.execute_genie_tool_task</code> (this is a placeholder; you'd implement this).</li> </ul> </li> <li>Configure the Strategy: In <code>MiddlewareConfig</code>, you would configure the <code>DistributedTaskInvocationStrategy</code> to use your chosen task queue plugin and specify the name of your generic worker task.     <pre><code># In MiddlewareConfig:\n# invocation_strategy_configurations={\n#     \"distributed_task_invocation_strategy_v1\": {\n#         \"task_queue_plugin_id\": \"celery_task_queue_v1\", # Or \"redis_queue_task_plugin_v1\"\n#         \"worker_tool_execution_task_name\": \"my_project.worker_tasks.execute_genie_tool_task\"\n#     }\n# }\n</code></pre></li> <li>Execute Tool with Strategy:     <pre><code># result_or_task_ref = await genie.execute_tool(\n#     \"my_long_running_tool\",\n#     param1=\"value\",\n#     strategy_id=\"distributed_task_invocation_strategy_v1\" \n#     # The strategy might return a task ID or future immediately,\n#     # or poll internally and return the final result.\n# )\n</code></pre></li> </ol> <p>Note on Security and Context: Securely passing API keys and sensitive context to distributed workers requires careful design. Options include: *   Workers fetching credentials themselves from a secure vault. *   Using short-lived, task-specific credentials. *   Carefully serializing only necessary and safe context information.</p> <p>The direct <code>genie.task_queue.submit_task</code> method gives you more control over what gets sent to the worker.</p>"},{"location":"guides/installation/","title":"Installation","text":"<p>This guide explains how to install Genie Tooling and its dependencies.</p>"},{"location":"guides/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher.</li> <li>Poetry for dependency management and packaging.</li> </ul>"},{"location":"guides/installation/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the Repository (Optional, if contributing or running examples locally):     If you want to work with the source code or run the examples directly from the repository:     <pre><code>git clone https://github.com/genie-tooling/genie-tooling.git\ncd genie-tooling\n</code></pre></p> </li> <li> <p>Install using Poetry:</p> <ul> <li> <p>To install the core library and all optional dependencies (recommended for full functionality and running all examples):     Navigate to the cloned repository directory (if applicable) or where your project that depends on Genie Tooling is located.     <pre><code>poetry install --all-extras\n</code></pre></p> </li> <li> <p>To install only the core library dependencies: <pre><code>poetry install\n</code></pre>     This will install the essential components of Genie Tooling. Some plugins requiring external libraries (e.g., specific LLM providers, vector stores, or tools) will not be functional unless their respective optional dependencies are installed.</p> </li> <li> <p>To install specific optional dependencies (extras):     You can install only the extras you need. Extras are defined in the <code>pyproject.toml</code> file.     For example, to install support for Ollama, OpenAI, Qdrant, Celery, and the internal Llama.cpp provider:     <pre><code>poetry install --extras \"ollama openai qdrant celery llama_cpp_internal\"\n</code></pre>     Common extras include:</p> <ul> <li><code>web_tools</code>: For tools that interact with web pages (e.g., <code>WebPageLoader</code>, <code>GoogleSearchTool</code>).</li> <li><code>openai_services</code>: For the OpenAI LLM provider and embedding generator.</li> <li><code>local_rag</code>: For local RAG components like Sentence Transformers and FAISS.</li> <li><code>distributed_rag</code>: For distributed RAG components like ChromaDB and Qdrant clients.</li> <li><code>ollama</code>: For the Ollama LLM provider.</li> <li><code>gemini</code>: For the Google Gemini LLM provider.</li> <li><code>secure_exec</code>: For the <code>SecureDockerExecutor</code>.</li> <li><code>observability</code>: For OpenTelemetry tracing and metrics.</li> <li><code>prompts</code>: For advanced prompt templating engines like Jinja2.</li> <li><code>task_queues</code>: For distributed task queue support (Celery, RQ).</li> <li><code>llama_cpp_server</code>: For the Llama.cpp server-based LLM provider.</li> <li><code>llama_cpp_internal</code>: For the Llama.cpp internal (direct library use) LLM provider.</li> </ul> </li> <li> <p>Adding Genie Tooling as a dependency to your existing project:     If you are integrating Genie Tooling into your own Poetry-managed project, you can add it:     <pre><code>poetry add genie-tooling # For the core library\n\n# To add with specific extras:\npoetry add genie-tooling -E ollama -E local_rag \n# or\npoetry add genie-tooling[ollama,local_rag]\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"guides/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can try running one of the examples, such as the simple Ollama chat example (ensure Ollama is running):</p> <pre><code># From the root of the genie-tooling repository if cloned:\npoetry run python examples/E02_ollama_chat_example.py \n</code></pre> <p>If the example runs without import errors and interacts with Ollama successfully, your basic installation is working.</p>"},{"location":"guides/logging/","title":"Logging","text":"<p>Genie Tooling uses the standard Python <code>logging</code> module for its internal logging. This allows application developers to integrate Genie's logs into their existing logging infrastructure seamlessly.</p>"},{"location":"guides/logging/#library-logger-name","title":"Library Logger Name","text":"<p>The root logger name for all messages originating from the Genie Tooling library is:</p> <pre><code>genie_tooling\n</code></pre> <p>Submodules within the library will use child loggers of this root logger (e.g., <code>genie_tooling.tools.manager</code>, <code>genie_tooling.llm_providers.impl.ollama_provider</code>).</p>"},{"location":"guides/logging/#configuring-logging-in-your-application","title":"Configuring Logging in Your Application","text":"<p>By default, Genie Tooling adds a <code>logging.NullHandler()</code> to its root logger (<code>genie_tooling</code>). This prevents \"No handlers could be found for logger 'genie_tooling'\" warnings if your application doesn't explicitly configure logging for the library.</p> <p>To see logs from Genie Tooling, you need to configure a handler and set a log level for the <code>\"genie_tooling\"</code> logger (or one of its parent loggers, like the root logger) in your application code.</p> <p>Example: Basic Console Logging</p> <p>Here's how you can enable basic console logging for Genie Tooling messages:</p> <pre><code>import logging\n\n# Get the Genie Tooling library logger\nlibrary_logger = logging.getLogger(\"genie_tooling\")\n\n# Set the desired log level (e.g., DEBUG for verbose output, INFO for general, WARNING for issues)\nlibrary_logger.setLevel(logging.DEBUG) \n\n# Create a handler (e.g., StreamHandler to output to console)\nconsole_handler = logging.StreamHandler()\n\n# Optional: Create a formatter and add it to the handler\nformatter = logging.Formatter('%(asctime)s - %(name)s - [%(levelname)s] - %(message)s (%(module)s:%(lineno)d)')\nconsole_handler.setFormatter(formatter)\n\n# Add the handler to the library logger\n# Check if a similar handler already exists to avoid duplicates if this code runs multiple times\nif not any(isinstance(h, logging.StreamHandler) for h in library_logger.handlers):\n    library_logger.addHandler(console_handler)\n\n# Optional: Prevent Genie logs from propagating to the root logger if it also has handlers\n# library_logger.propagate = False \n\n# Now, when you use Genie, its logs will appear on the console.\n# from genie_tooling.genie import Genie\n# ... your Genie setup and usage ...\n</code></pre> <p>Common Log Levels:</p> <ul> <li><code>logging.DEBUG</code>: Detailed information, typically of interest only when diagnosing problems.</li> <li><code>logging.INFO</code>: Confirmation that things are working as expected, high-level operational messages.</li> <li><code>logging.WARNING</code>: An indication that something unexpected happened, or indicative of some problem in the near future (e.g., \u2018disk space low\u2019). The software is still working as expected.</li> <li><code>logging.ERROR</code>: Due to a more serious problem, the software has not been able to perform some function.</li> <li><code>logging.CRITICAL</code>: A serious error, indicating that the program itself may be unable to continue running.</li> </ul> <p>You can integrate Genie's logging with more advanced logging setups, such as logging to files, using structured logging (e.g., JSON format), or sending logs to external monitoring services, just as you would for any other Python library. The <code>DefaultLogAdapter</code> plugin, if configured, can also influence logging behavior, including redaction.</p>"},{"location":"guides/observability_tracing/","title":"Observability and Interaction Tracing (<code>genie.observability</code>)","text":"<p>Genie Tooling includes an observability interface, <code>genie.observability</code>, primarily for interaction tracing. This allows developers to record key events and data points as an agent processes requests, aiding in debugging, monitoring, and understanding agent behavior. Traces can be exported to various backends using OpenTelemetry.</p>"},{"location":"guides/observability_tracing/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>ObservabilityInterface</code> (<code>genie.observability</code>): The facade interface for tracing events.</li> <li><code>InteractionTracerPlugin</code>: A plugin responsible for handling trace events.<ul> <li>Built-in:<ul> <li><code>ConsoleTracerPlugin</code> (alias: <code>console_tracer</code>): Prints trace events to the console.</li> <li><code>OpenTelemetryTracerPlugin</code> (alias: <code>otel_tracer</code>): Exports traces using the OpenTelemetry SDK.</li> </ul> </li> </ul> </li> <li><code>TraceEvent</code> (TypedDict): The structure for trace data:     <pre><code>class TraceEvent(TypedDict):\n    event_name: str  # e.g., \"llm.chat.start\", \"tool.execute.success\"\n    data: Dict[str, Any] # Event-specific payload\n    timestamp: float # time.time() or loop.time()\n    component: Optional[str] # e.g., \"LLMInterface\", \"ToolInvoker:calculator_tool\"\n    correlation_id: Optional[str] # For linking related events\n</code></pre>     The <code>data</code> field may contain an <code>error_message</code>, <code>error_type</code>, and <code>error_stacktrace</code> if an error occurred. For LLM events, it may contain <code>llm.usage</code> with token counts (if <code>OpenTelemetryTracerPlugin</code> is used and token usage is recorded).</li> </ul>"},{"location":"guides/observability_tracing/#configuration","title":"Configuration","text":"<p>Configure the default interaction tracer via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code> settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        observability_tracer=\"console_tracer\", # Default: prints to console\n        # OR for OpenTelemetry:\n        # observability_tracer=\"otel_tracer\",\n        # observability_otel_endpoint=\"http://localhost:4318/v1/traces\" # For OTLP/HTTP\n    ),\n    # Example: Configure the ConsoleTracerPlugin\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": { # Canonical ID\n            \"log_level\": \"DEBUG\" # Log traces at DEBUG level\n        }\n    }\n    # Example: Configure OpenTelemetryTracerPlugin\n    # observability_tracer_configurations={\n    #     \"otel_tracer_plugin_v1\": {\n    #         \"otel_service_name\": \"my-genie-app\",\n    #         \"otel_service_version\": \"1.2.3\",\n    #         \"exporter_type\": \"otlp_http\", # \"console\", \"otlp_grpc\"\n    #         \"otlp_http_endpoint\": \"http://localhost:4318/v1/traces\",\n    #         # \"otlp_http_headers\": \"Authorization=Bearer mytoken,X-Custom-Header=value\",\n    #         # \"otlp_grpc_endpoint\": \"localhost:4317\",\n    #         # \"otlp_grpc_insecure\": True, # Use False if your collector uses TLS\n    #         \"resource_attributes\": {\"deployment.environment\": \"staging\"}\n    #     }\n    # }\n)\n</code></pre>"},{"location":"guides/observability_tracing/#opentelemetrytracerplugin-configuration-options","title":"<code>OpenTelemetryTracerPlugin</code> Configuration Options","text":"<p>When <code>observability_tracer=\"otel_tracer\"</code> is chosen, or <code>default_observability_tracer_id=\"otel_tracer_plugin_v1\"</code> is set, you can configure it in <code>observability_tracer_configurations[\"otel_tracer_plugin_v1\"]</code>:</p> <ul> <li><code>otel_service_name</code> (str, default: \"genie-tooling-application\"): The name of your service as it will appear in traces.</li> <li><code>otel_service_version</code> (str, default: Genie Tooling library version): Version of your service.</li> <li><code>exporter_type</code> (str, default: \"console\"):<ul> <li><code>\"console\"</code>: Prints spans to the console (useful for local debugging).</li> <li><code>\"otlp_http\"</code>: Exports spans via OTLP/HTTP protocol. Requires <code>opentelemetry-exporter-otlp-proto-http</code>.</li> <li><code>\"otlp_grpc\"</code>: Exports spans via OTLP/gRPC protocol. Requires <code>opentelemetry-exporter-otlp-proto-grpc</code>.</li> </ul> </li> <li><code>otlp_http_endpoint</code> (str, default: \"http://localhost:4318/v1/traces\"): Endpoint for OTLP/HTTP exporter.</li> <li><code>otlp_http_headers</code> (Optional[Dict[str,str]] or str like \"k1=v1,k2=v2\"): Custom headers for OTLP/HTTP.</li> <li><code>otlp_http_timeout</code> (int, default: 10): Timeout in seconds for OTLP/HTTP.</li> <li><code>otlp_grpc_endpoint</code> (str, default: \"localhost:4317\"): Endpoint for OTLP/gRPC exporter.</li> <li><code>otlp_grpc_insecure</code> (bool, default: <code>False</code>): Whether to use an insecure gRPC connection.</li> <li><code>otlp_grpc_timeout</code> (int, default: 10): Timeout in seconds for OTLP/gRPC.</li> <li><code>resource_attributes</code> (Optional[Dict[str, Any]]): Additional attributes to add to the OTel Resource (e.g., <code>{\"deployment.environment\": \"production\"}</code>).</li> </ul>"},{"location":"guides/observability_tracing/#automatic-tracing","title":"Automatic Tracing","text":"<p>Core <code>Genie</code> methods are automatically instrumented. Key attributes often included: *   <code>component</code>: The Genie component emitting the trace (e.g., <code>LLMInterface</code>, <code>ToolInvoker:my_tool</code>). *   <code>correlation_id</code>: Links events within a single logical operation (e.g., one <code>genie.run_command()</code> call). *   <code>llm.usage.prompt_tokens</code>, <code>llm.usage.completion_tokens</code>, <code>llm.usage.total_tokens</code>: For LLM call success events (when using <code>OpenTelemetryTracerPlugin</code> and token usage is available from the LLM provider). *   <code>error.type</code>, <code>error.message</code>, <code>error.stacktrace</code>: If an error occurs during the traced operation.</p>"},{"location":"guides/observability_tracing/#manual-tracing","title":"Manual Tracing","text":"<p>Use <code>await genie.observability.trace_event(...)</code> for custom application events.</p> <pre><code>import uuid\n# await genie.observability.trace_event(\n#     event_name=\"my_app.custom_process.start\",\n#     data={\"input_id\": \"123\", \"user_category\": \"premium\"},\n#     component=\"MyCustomModule\",\n#     correlation_id=str(uuid.uuid4())\n# )\n</code></pre>"},{"location":"guides/observability_tracing/#viewing-traces","title":"Viewing Traces","text":"<p>When using OTLP exporters, configure them to point to an OpenTelemetry collector (e.g., OTel Collector, Jaeger All-in-One, Grafana Agent, SigNoz, or a cloud vendor's OTel endpoint). The collector will then process and forward traces to your chosen backend (Jaeger, Prometheus, Zipkin, etc.) for visualization and analysis.</p>"},{"location":"guides/plugin_architecture/","title":"Plugin Architecture","text":"<p>Genie Tooling is built upon a highly modular and extensible plugin architecture. This design principle allows developers to easily swap, customize, or add new functionalities without altering the core framework.</p>"},{"location":"guides/plugin_architecture/#core-idea","title":"Core Idea","text":"<p>The central idea is that most significant functionalities within Genie are implemented as plugins. Each plugin adheres to a specific protocol (an interface defined using Python's <code>typing.Protocol</code> or an Abstract Base Class). This ensures that different implementations of a particular functionality can be used interchangeably as long as they conform to the defined contract.</p>"},{"location":"guides/plugin_architecture/#key-components","title":"Key Components","text":"<ol> <li> <p><code>Plugin</code> Protocol (<code>genie_tooling.core.types.Plugin</code>):     This is the base protocol that all plugins should ideally implement. It defines:</p> <ul> <li><code>plugin_id</code> (property): A unique string identifier for the plugin.</li> <li><code>async def setup(config)</code> (optional): For initialization with configuration.</li> <li><code>async def teardown()</code> (optional): For resource cleanup.</li> </ul> </li> <li> <p>Specific Plugin Protocols:     For each type of extensible functionality, there's a more specific protocol that inherits from or includes the base <code>Plugin</code> requirements. Examples:</p> <ul> <li><code>Tool</code> (<code>genie_tooling.tools.abc.Tool</code>)</li> <li><code>LLMProviderPlugin</code> (<code>genie_tooling.llm_providers.abc.LLMProviderPlugin</code>)</li> <li><code>CacheProvider</code> (<code>genie_tooling.cache_providers.abc.CacheProvider</code>)</li> <li><code>DocumentLoaderPlugin</code> (<code>genie_tooling.document_loaders.abc.DocumentLoaderPlugin</code>)</li> <li>And many more (see <code>src/genie_tooling/.../abc.py</code> files).</li> </ul> </li> <li> <p><code>PluginManager</code> (<code>genie_tooling.core.plugin_manager.PluginManager</code>):</p> <ul> <li>Discovery: Responsible for finding available plugin classes. It searches:<ul> <li>Entry Points: Defined in <code>pyproject.toml</code> under the <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> group. This is the standard way for third-party libraries or separate modules to provide plugins.</li> <li>Plugin Development Directories: Specified in <code>MiddlewareConfig.plugin_dev_dirs</code>. Useful for project-specific plugins or during development.</li> </ul> </li> <li>Instantiation &amp; Setup: When a plugin is requested (e.g., by a manager or the <code>Genie</code> facade), the <code>PluginManager</code> instantiates the plugin class and calls its <code>async setup(config)</code> method, passing any relevant configuration.</li> <li>Caching: It typically caches instantiated plugins to avoid re-creating and re-setting them up on every request.</li> <li>Teardown: Manages the <code>async teardown()</code> lifecycle of loaded plugins.</li> </ul> </li> <li> <p>Managers (e.g., <code>ToolManager</code>, <code>LLMProviderManager</code>, <code>RAGManager</code>):</p> <ul> <li>These components orchestrate plugins of a specific type.</li> <li>They use the <code>PluginManager</code> to get instances of the plugins they need based on configuration.</li> <li>For example, <code>LLMProviderManager</code> uses <code>PluginManager</code> to load the configured <code>LLMProviderPlugin</code> (like <code>OllamaLLMProviderPlugin</code> or <code>OpenAILLMProviderPlugin</code>).</li> </ul> </li> <li> <p><code>Genie</code> Facade:</p> <ul> <li>The <code>Genie</code> facade simplifies interaction with the underlying managers and, by extension, the plugins.</li> <li>When you configure <code>Genie</code> (e.g., <code>features.llm = \"ollama\"</code>), it internally directs the <code>LLMProviderManager</code> to load and use the \"ollama\" LLM provider plugin.</li> </ul> </li> </ol>"},{"location":"guides/plugin_architecture/#benefits-of-the-plugin-architecture","title":"Benefits of the Plugin Architecture","text":"<ul> <li>Extensibility: Easily add new LLM providers, tools, data sources, caching backends, etc., without modifying Genie's core code.</li> <li>Customization: Replace default implementations with your own specialized versions.</li> <li>Modularity: Keeps different functionalities decoupled, making the system easier to understand, maintain, and test.</li> <li>Community Contributions: Facilitates sharing and using plugins developed by the community.</li> <li>Flexibility: Allows applications to select and configure only the components they need.</li> </ul>"},{"location":"guides/plugin_architecture/#how-to-create-a-plugin","title":"How to Create a Plugin","text":"<ol> <li>Identify the Protocol: Find the relevant plugin protocol in <code>genie_tooling</code> (e.g., <code>Tool</code> for a new tool, <code>CacheProvider</code> for a new cache).</li> <li>Implement the Class: Create a Python class that:<ul> <li>Defines a unique <code>plugin_id</code> class attribute.</li> <li>Implements all methods and properties required by the chosen protocol.</li> <li>Optionally implements <code>async setup(config)</code> and <code>async teardown()</code>.</li> </ul> </li> <li>Register the Plugin:<ul> <li>For distributable plugins: Add an entry point in your <code>pyproject.toml</code>.</li> <li>For local plugins: Place the file in a directory specified in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> </ul> </li> <li>Configure Genie: Update your <code>MiddlewareConfig</code> to use your new plugin, either by setting it as a default or by providing its configuration in the relevant <code>*_configurations</code> dictionary. For tools, ensure they are enabled in <code>tool_configurations</code>.</li> </ol> <p>Refer to the specific \"Creating ... Plugins\" guides for more detailed instructions on particular plugin types.</p>"},{"location":"guides/simplified_configuration/","title":"Simplified Configuration with FeatureSettings and Aliases","text":"<p>Genie Tooling aims to make common configurations straightforward through <code>FeatureSettings</code> and a system of plugin ID aliases. This guide explains these concepts in detail.</p>"},{"location":"guides/simplified_configuration/#featuresettings","title":"<code>FeatureSettings</code>","text":"<p>The <code>FeatureSettings</code> model, part of <code>MiddlewareConfig</code>, provides high-level toggles for major functionalities. When you use <code>Genie.create(config=MiddlewareConfig(features=...))</code>, the internal <code>ConfigResolver</code> uses these settings to:</p> <ol> <li>Set default plugin IDs for various components (e.g., <code>default_llm_provider_id</code>, <code>default_rag_embedder_id</code>).</li> <li>Populate basic configurations for these default plugins in the respective <code>*_configurations</code> dictionaries (e.g., setting the model name for the chosen LLM provider).</li> </ol> <p>Example of <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.features import FeatureSettings\n\nfeatures = FeatureSettings(\n    # LLM Provider\n    llm=\"ollama\",                             # Chooses OllamaLLMProviderPlugin\n    llm_ollama_model_name=\"mistral:7b-instruct-q4_K_M\", # Sets model for Ollama\n    # Or for Llama.cpp Internal:\n    # llm=\"llama_cpp_internal\",\n    # llm_llama_cpp_internal_model_path=\"/path/to/your/model.gguf\",\n    # llm_llama_cpp_internal_n_gpu_layers=-1,\n    # llm_llama_cpp_internal_chat_format=\"mistral\",\n\n    # Command Processing\n    command_processor=\"llm_assisted\",         # Chooses LLMAssistedToolSelectionProcessorPlugin\n    command_processor_formatter_id_alias=\"compact_text_formatter\", # Formatter for LLM prompt\n\n    # Tool Lookup\n    tool_lookup=\"embedding\",                  # Uses EmbeddingSimilarityLookupProvider\n    tool_lookup_embedder_id_alias=\"st_embedder\", # Embedder for tool lookup\n    tool_lookup_formatter_id_alias=\"compact_text_formatter\", # Formatter for tool indexing\n    # tool_lookup_chroma_path=\"./my_tool_lookup_db\", # Optional: Use ChromaDB for tool lookup embeddings\n\n    # RAG\n    rag_embedder=\"openai\",                    # Uses OpenAIEmbeddingGenerator for RAG\n    rag_vector_store=\"chroma\",                # Uses ChromaDBVectorStore for RAG\n    rag_vector_store_chroma_path=\"./my_rag_vector_db\",\n    rag_vector_store_chroma_collection_name=\"main_rag_docs\",\n\n    # Caching\n    cache=\"redis\",                            # Uses RedisCacheProvider\n    cache_redis_url=\"redis://localhost:6379/1\",\n\n    # Observability &amp; Monitoring\n    observability_tracer=\"otel_tracer\",       # Use OpenTelemetry for traces\n    observability_otel_endpoint=\"http://localhost:4318/v1/traces\", # OTLP/HTTP endpoint\n    token_usage_recorder=\"otel_metrics_recorder\", # Use OpenTelemetry for token metrics\n\n    # Human-in-the-Loop\n    hitl_approver=\"cli_hitl_approver\",        # Use CLI for approvals\n\n    # Guardrails\n    input_guardrails=[\"keyword_blocklist_guardrail\"], # List of guardrail aliases/IDs\n\n    # Prompt System &amp; Conversation\n    prompt_registry=\"file_system_prompt_registry\",\n    prompt_template_engine=\"jinja2_chat_formatter\",\n    conversation_state_provider=\"in_memory_convo_provider\",\n    default_llm_output_parser=\"pydantic_output_parser\",\n\n    # Distributed Tasks\n    task_queue=\"celery\",                      # Use Celery\n    task_queue_celery_broker_url=\"redis://localhost:6379/3\",\n    task_queue_celery_backend_url=\"redis://localhost:6379/4\",\n    # Or for RQ:\n    # task_queue=\"rq\",\n    # (RQ typically uses the same Redis for broker/backend, configured via redis_url in its plugin config)\n)\n\n# This 'features' object would be passed to MiddlewareConfig:\n# from genie_tooling.config.models import MiddlewareConfig\n# app_config = MiddlewareConfig(features=features)\n</code></pre>"},{"location":"guides/simplified_configuration/#plugin-id-aliases","title":"Plugin ID Aliases","text":"<p>To make <code>FeatureSettings</code> and explicit configurations more readable, Genie uses a system of aliases. The <code>ConfigResolver</code> maps these short aliases to their full, canonical plugin IDs.</p> <p>Commonly Used Aliases (Examples):</p> <ul> <li>LLM Providers:<ul> <li><code>\"ollama\"</code>: <code>\"ollama_llm_provider_v1\"</code></li> <li><code>\"openai\"</code>: <code>\"openai_llm_provider_v1\"</code></li> <li><code>\"gemini\"</code>: <code>\"gemini_llm_provider_v1\"</code></li> <li><code>\"llama_cpp\"</code>: <code>\"llama_cpp_llm_provider_v1\"</code> (for server)</li> <li><code>\"llama_cpp_internal\"</code>: <code>\"llama_cpp_internal_llm_provider_v1\"</code></li> </ul> </li> <li>Key Provider:<ul> <li><code>\"env_keys\"</code>: <code>\"environment_key_provider_v1\"</code> (Default if no KeyProvider instance/ID is given)</li> </ul> </li> <li>Caching Providers:<ul> <li><code>\"in_memory_cache\"</code>: <code>\"in_memory_cache_provider_v1\"</code></li> <li><code>\"redis_cache\"</code>: <code>\"redis_cache_provider_v1\"</code></li> </ul> </li> <li>Embedding Generators (Embedders):<ul> <li><code>\"st_embedder\"</code>: <code>\"sentence_transformer_embedder_v1\"</code></li> <li><code>\"openai_embedder\"</code>: <code>\"openai_embedding_generator_v1\"</code></li> </ul> </li> <li>Vector Stores:<ul> <li><code>\"faiss_vs\"</code>: <code>\"faiss_vector_store_v1\"</code></li> <li><code>\"chroma_vs\"</code>: <code>\"chromadb_vector_store_v1\"</code></li> <li><code>\"qdrant_vs\"</code>: <code>\"qdrant_vector_store_v1\"</code></li> </ul> </li> <li>Tool Lookup Providers:<ul> <li><code>\"embedding_lookup\"</code>: <code>\"embedding_similarity_lookup_v1\"</code></li> <li><code>\"keyword_lookup\"</code>: <code>\"keyword_match_lookup_v1\"</code></li> </ul> </li> <li>Definition Formatters:<ul> <li><code>\"compact_text_formatter\"</code>: <code>\"compact_text_formatter_plugin_v1\"</code></li> <li><code>\"openai_func_formatter\"</code>: <code>\"openai_function_formatter_plugin_v1\"</code></li> <li><code>\"hr_json_formatter\"</code>: <code>\"human_readable_json_formatter_plugin_v1\"</code></li> </ul> </li> <li>Command Processors:<ul> <li><code>\"llm_assisted_cmd_proc\"</code>: <code>\"llm_assisted_tool_selection_processor_v1\"</code></li> <li><code>\"simple_keyword_cmd_proc\"</code>: <code>\"simple_keyword_processor_v1\"</code></li> </ul> </li> <li>Observability &amp; Monitoring:<ul> <li><code>\"console_tracer\"</code>: <code>\"console_tracer_plugin_v1\"</code></li> <li><code>\"otel_tracer\"</code>: <code>\"otel_tracer_plugin_v1\"</code></li> <li><code>\"in_memory_token_recorder\"</code>: <code>\"in_memory_token_usage_recorder_v1\"</code></li> <li><code>\"otel_metrics_recorder\"</code>: <code>\"otel_metrics_token_recorder_v1\"</code></li> </ul> </li> <li>HITL &amp; Guardrails:<ul> <li><code>\"cli_hitl_approver\"</code>: <code>\"cli_approval_plugin_v1\"</code></li> <li><code>\"keyword_blocklist_guardrail\"</code>: <code>\"keyword_blocklist_guardrail_v1\"</code></li> </ul> </li> <li>Prompt System &amp; Conversation:<ul> <li><code>\"file_system_prompt_registry\"</code>: <code>\"file_system_prompt_registry_v1\"</code></li> <li><code>\"basic_string_formatter\"</code>: <code>\"basic_string_format_template_v1\"</code></li> <li><code>\"jinja2_chat_formatter\"</code>: <code>\"jinja2_chat_template_v1\"</code></li> <li><code>\"in_memory_convo_provider\"</code>: <code>\"in_memory_conversation_state_v1\"</code></li> <li><code>\"redis_convo_provider\"</code>: <code>\"redis_conversation_state_v1\"</code></li> </ul> </li> <li>LLM Output Parsers:<ul> <li><code>\"json_output_parser\"</code>: <code>\"json_output_parser_v1\"</code></li> <li><code>\"pydantic_output_parser\"</code>: <code>\"pydantic_output_parser_v1\"</code></li> </ul> </li> <li>Task Queues:<ul> <li><code>\"celery_task_queue\"</code>: <code>\"celery_task_queue_v1\"</code></li> <li><code>\"rq_task_queue\"</code>: <code>\"redis_queue_task_plugin_v1\"</code></li> </ul> </li> </ul> <p>(This list is not exhaustive. Refer to <code>src/genie_tooling/config/resolver.py</code> for the complete <code>PLUGIN_ID_ALIASES</code> dictionary.)</p>"},{"location":"guides/simplified_configuration/#overriding-feature-derived-settings","title":"Overriding Feature-Derived Settings","text":"<p>You can always provide more specific configurations that will take precedence over what <code>FeatureSettings</code> implies.</p> <p>1. Overriding Default Plugin IDs:</p> <p>If a feature sets a default (e.g., <code>features.llm = \"openai\"</code> sets <code>default_llm_provider_id</code> to <code>\"openai_llm_provider_v1\"</code>), you can override this directly:</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(llm=\"openai\"),\n    default_llm_provider_id=\"my_special_openai_provider_v3\" # Explicit override\n)\n</code></pre> <p>2. Overriding Plugin Configurations:</p> <p>Use the <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code> to provide settings for specific plugins. You can use either the canonical ID or an alias as the key.</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\" # Feature sets this model\n    ),\n    llm_provider_configurations={\n        \"ollama\": { # Using alias 'ollama'\n            \"model_name\": \"llama3:8b-instruct-fp16\", # Override the model\n            \"request_timeout_seconds\": 300.0,\n            # Other Ollama-specific settings\n        },\n        \"openai_llm_provider_v1\": { # Using canonical ID\n            \"model_name\": \"gpt-4o\",\n            # This config will be available if you switch to OpenAI or use it explicitly\n        }\n    }\n)\n</code></pre> <p>In this example: *   The default LLM is Ollama. *   The <code>mistral:latest</code> model set by <code>FeatureSettings</code> for Ollama is overridden by <code>llama3:8b-instruct-fp16</code> from the explicit configuration. *   The <code>request_timeout_seconds</code> is also set specifically for the Ollama provider.</p> <p>This layered approach (Features -&gt; Explicit Defaults -&gt; Explicit Plugin Configs) provides both ease of use for common cases and fine-grained control when needed.</p>"},{"location":"guides/token_usage_tracking/","title":"Token Usage Tracking (<code>genie.usage</code>)","text":"<p>Genie Tooling provides an interface for tracking token usage by LLM providers, accessible via <code>genie.usage</code>. This helps in monitoring costs and understanding LLM consumption patterns.</p>"},{"location":"guides/token_usage_tracking/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>UsageTrackingInterface</code> (<code>genie.usage</code>): The facade interface for recording and summarizing token usage.</li> <li><code>TokenUsageRecorderPlugin</code>: A plugin responsible for storing or exporting token usage records.<ul> <li>Built-in:<ul> <li><code>InMemoryTokenUsageRecorderPlugin</code> (alias: <code>in_memory_token_recorder</code>): Stores records in memory.</li> <li><code>OpenTelemetryMetricsTokenRecorderPlugin</code> (alias: <code>otel_metrics_recorder</code>): Emits token counts as OpenTelemetry metrics.</li> </ul> </li> </ul> </li> <li><code>TokenUsageRecord</code> (TypedDict): The structure for a single token usage event:     <pre><code>class TokenUsageRecord(TypedDict, total=False):\n    provider_id: str\n    model_name: str\n    prompt_tokens: Optional[int]\n    completion_tokens: Optional[int]\n    total_tokens: Optional[int]\n    timestamp: float # time.time()\n    call_type: Optional[str] # \"chat\", \"generate\", \"generate_stream_end\", \"chat_stream_end\"\n    user_id: Optional[str]\n    session_id: Optional[str]\n    custom_tags: Optional[dict]\n</code></pre></li> </ul>"},{"location":"guides/token_usage_tracking/#configuration","title":"Configuration","text":"<p>Configure the default token usage recorder via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code>.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        token_usage_recorder=\"in_memory_token_recorder\" # Default in-memory\n        # OR to use OTel Metrics:\n        # token_usage_recorder=\"otel_metrics_recorder\",\n        # observability_tracer=\"otel_tracer\" # OTel SDK needs to be initialized\n    )\n    # Example: Configure the OpenTelemetryMetricsTokenRecorderPlugin if chosen\n    # (This plugin itself has no specific config options, but relies on OTel SDK setup)\n    # token_usage_recorder_configurations={\n    #     \"otel_metrics_token_recorder_v1\": {}\n    # },\n    # observability_tracer_configurations={ # Ensure OTel SDK is initialized\n    #     \"otel_tracer_plugin_v1\": {\n    #         \"otel_service_name\": \"my-app-with-token-metrics\",\n    #         \"exporter_type\": \"console\" # Or your preferred OTel exporter\n    #     }\n    # }\n)\n</code></pre>"},{"location":"guides/token_usage_tracking/#automatic-recording","title":"Automatic Recording","text":"<p>When a <code>TokenUsageManager</code> is active (i.e., <code>features.token_usage_recorder</code> is not <code>\"none\"</code>), token usage is automatically recorded by: *   <code>genie.llm.chat()</code> *   <code>genie.llm.generate()</code></p> <p>The LLM provider plugins are responsible for returning <code>LLMUsageInfo</code> in their responses (or in the final chunk of a stream). The <code>LLMInterface</code> then uses this information to create and record <code>TokenUsageRecord</code>s. The <code>call_type</code> will distinguish between regular calls and the end of streaming calls (e.g., <code>generate_stream_end</code>, <code>chat_stream_end</code>).</p>"},{"location":"guides/token_usage_tracking/#manual-usage","title":"Manual Usage","text":""},{"location":"guides/token_usage_tracking/#1-getting-a-usage-summary-for-in_memory_token_recorder","title":"1. Getting a Usage Summary (for <code>in_memory_token_recorder</code>)","text":"<p>You can retrieve a summary of recorded token usage if using a recorder that supports it (like <code>InMemoryTokenUsageRecorderPlugin</code>).</p> <p><pre><code># Assuming genie is initialized with token_usage_recorder=\"in_memory_token_recorder\"\nsummary = await genie.usage.get_summary()\n# summary = await genie.usage.get_summary(recorder_id=\"my_custom_recorder\") # If using a specific recorder\n# summary = await genie.usage.get_summary(filter_criteria={\"user_id\": \"user123\"}) # Filter\n\nprint(f\"Total records: {summary.get('total_records')}\")\nprint(f\"Total tokens overall: {summary.get('total_tokens_overall')}\")\nif summary.get(\"by_model\"):\n    for model, data in summary[\"by_model\"].items():\n        print(f\"  Model: {model}, Total Tokens: {data['total']}, Count: {data['count']}\")\n</code></pre> The <code>OpenTelemetryMetricsTokenRecorderPlugin</code> will log a warning if <code>get_summary()</code> is called, as metrics are viewed in an OTel backend.</p>"},{"location":"guides/token_usage_tracking/#2-manually-recording-usage-advanced","title":"2. Manually Recording Usage (Advanced)","text":"<p>While most recording is automatic, you can manually record usage if needed: <pre><code>from genie_tooling.token_usage.types import TokenUsageRecord\nimport time\n\nmanual_record = TokenUsageRecord(\n    provider_id=\"my_external_llm_service\",\n    model_name=\"external_model_vX\",\n    prompt_tokens=1000,\n    completion_tokens=500,\n    total_tokens=1500,\n    timestamp=time.time(),\n    call_type=\"custom_batch_job\",\n    user_id=\"batch_processor\"\n)\nawait genie.usage.record_usage(manual_record)\n</code></pre></p>"},{"location":"guides/token_usage_tracking/#using-opentelemetrymetricstokenrecorderplugin","title":"Using <code>OpenTelemetryMetricsTokenRecorderPlugin</code>","text":"<p>When <code>token_usage_recorder=\"otel_metrics_recorder\"</code> is configured, this plugin will emit the following OTel metrics: *   <code>llm.request.tokens.prompt</code> (Counter, unit: <code>1</code> {token}) *   <code>llm.request.tokens.completion</code> (Counter, unit: <code>1</code> {token}) *   <code>llm.request.tokens.total</code> (Counter, unit: <code>1</code> {token})</p> <p>These metrics will have attributes like <code>llm.provider.id</code>, <code>llm.model.name</code>, <code>llm.call_type</code>, <code>genie.client.user_id</code>, <code>genie.client.session_id</code>, and any <code>genie.tag.*</code> from custom tags. Configure an OTel collector (e.g., with Prometheus exporter) to scrape and visualize these metrics.</p> <p>Example PromQL Queries: *   Total prompt tokens per model (rate over 5m): <code>sum(rate(llm_request_tokens_prompt_total[5m])) by (llm_model_name)</code> *   Total completion tokens per provider (rate over 5m): <code>sum(rate(llm_request_tokens_completion_total[5m])) by (llm_provider_id)</code></p>"},{"location":"guides/token_usage_tracking/#creating-custom-token-usage-recorders","title":"Creating Custom Token Usage Recorders","text":"<p>Implement the <code>TokenUsageRecorderPlugin</code> protocol, defining <code>record_usage</code>, <code>get_summary</code>, and <code>clear_records</code> methods to interact with your chosen storage backend (e.g., database, logging service). Register your plugin via entry points or <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/tool_lookup/","title":"Tool Lookup","text":"<p>Tool Lookup in Genie Tooling refers to the process of finding relevant tools based on a natural language query. This is primarily used by the <code>LLMAssistedToolSelectionProcessorPlugin</code> to narrow down the list of tools presented to the LLM, making the LLM's selection task more efficient and accurate.</p>"},{"location":"guides/tool_lookup/#how-tool-lookup-is-used","title":"How Tool Lookup is Used","text":"<p>When you configure the <code>Genie</code> facade to use the <code>llm_assisted</code> command processor, you can also enable and configure tool lookup:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\", # LLM for the command processor\n        command_processor=\"llm_assisted\",\n\n        # Tool Lookup Configuration\n        tool_lookup=\"embedding\", # Enable embedding-based tool lookup\n        # tool_lookup=\"keyword\", # Or enable keyword-based tool lookup\n        # tool_lookup=\"none\",    # Disable tool lookup (LLM sees all tools)\n\n        # If tool_lookup=\"embedding\":\n        tool_lookup_embedder_id_alias=\"st_embedder\", # e.g., \"sentence_transformer_embedder_v1\"\n        # tool_lookup_embedder_id_alias=\"openai_embedder\", # Or \"openai_embedding_generator_v1\"\n\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\", # Formatter for tool text before embedding\n\n        # Optional: Configure ChromaDB for persistent tool lookup embeddings\n        # tool_lookup_chroma_path=\"./my_tool_lookup_db\",\n        # tool_lookup_chroma_collection_name=\"agent_tool_embeddings\"\n    ),\n    # Further configure the LLM-assisted processor\n    command_processor_configurations={\n        \"llm_assisted_tool_selection_processor_v1\": {\n            \"tool_lookup_top_k\": 3 # How many top tools from lookup to show the LLM\n        }\n    },\n    tool_configurations={ # Tools must be enabled to be considered by lookup/processor\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {} \n        # Add other tools that might be relevant\n    }\n)\n</code></pre> <p>The <code>LLMAssistedToolSelectionProcessorPlugin</code> internally uses the <code>ToolLookupService</code> to: 1.  Index Tools: When first needed (or if the index is invalidated), it takes all enabled tools (from <code>tool_configurations</code>), formats their definitions (using the <code>tool_lookup_formatter_id_alias</code>), and indexes them using the configured <code>ToolLookupProvider</code> (e.g., <code>EmbeddingSimilarityLookupProvider</code> or <code>KeywordMatchLookupProvider</code>). 2.  Find Tools: When processing a user command, it queries the <code>ToolLookupService</code> with the command. The service returns a ranked list of potentially relevant tools. 3.  Filter Tools: The processor then takes the top N tools (defined by <code>tool_lookup_top_k</code>) from this list and presents only their definitions to the LLM for final selection.</p>"},{"location":"guides/tool_lookup/#available-tool-lookup-providers","title":"Available Tool Lookup Providers","text":"<p>Genie Tooling includes built-in providers:</p> <ul> <li><code>EmbeddingSimilarityLookupProvider</code> (alias: <code>\"embedding_lookup\"</code>):<ul> <li>Embeds tool definitions (formatted text) and user queries using a configured <code>EmbeddingGeneratorPlugin</code> (e.g., Sentence Transformers, OpenAI Embeddings).</li> <li>Finds tools based on cosine similarity in the embedding space.</li> <li>Can use an in-memory NumPy index or a persistent <code>VectorStorePlugin</code> (like ChromaDB) to store tool embeddings.</li> </ul> </li> <li><code>KeywordMatchLookupProvider</code> (alias: <code>\"keyword_lookup\"</code>):<ul> <li>Performs simple keyword matching between the query and tool definitions (name, description, tags, formatted text).</li> <li>Stateless and does not require an embedder or vector store.</li> </ul> </li> </ul>"},{"location":"guides/tool_lookup/#configuring-tool-lookup-components","title":"Configuring Tool Lookup Components","text":"<ul> <li><code>features.tool_lookup</code>: Sets the default <code>ToolLookupProviderPlugin</code> (<code>\"embedding\"</code>, <code>\"keyword\"</code>, or <code>\"none\"</code>).</li> <li><code>features.tool_lookup_formatter_id_alias</code>: Specifies the alias of the <code>DefinitionFormatterPlugin</code> used to create the textual representation of tools for indexing (e.g., <code>\"compact_text_formatter\"</code>).</li> <li><code>features.tool_lookup_embedder_id_alias</code>: If <code>tool_lookup=\"embedding\"</code>, this sets the alias of the <code>EmbeddingGeneratorPlugin</code> used for embedding tool definitions and queries (e.g., <code>\"st_embedder\"</code>, <code>\"openai_embedder\"</code>).</li> <li><code>features.tool_lookup_chroma_path</code> / <code>tool_lookup_chroma_collection_name</code>: If using embedding lookup and you want ChromaDB for persistence, these configure the <code>EmbeddingSimilarityLookupProvider</code> to use a <code>ChromaDBVectorStorePlugin</code> internally.</li> </ul> <p>You can provide more detailed configurations for the chosen <code>ToolLookupProviderPlugin</code> or its sub-components (like its embedder or vector store) via the <code>tool_lookup_provider_configurations</code>, <code>embedding_generator_configurations</code>, or <code>vector_store_configurations</code> dictionaries in <code>MiddlewareConfig</code>.</p> <p>Example: Overriding the embedder model for tool lookup: <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        tool_lookup=\"embedding\",\n        tool_lookup_embedder_id_alias=\"st_embedder\" # Default ST embedder\n    ),\n    embedding_generator_configurations={\n        # This config applies if 'st_embedder' (sentence_transformer_embedder_v1)\n        # is used by ANY component, including tool lookup.\n        \"sentence_transformer_embedder_v1\": {\n            \"model_name\": \"paraphrase-MiniLM-L3-v2\" # Use a different ST model\n        }\n    }\n)\n</code></pre></p> <p>Or, to configure the <code>EmbeddingSimilarityLookupProvider</code> directly to use a specific embedder and its config: <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(tool_lookup=\"embedding\"), # Selects embedding_similarity_lookup_v1\n    tool_lookup_provider_configurations={\n        \"embedding_similarity_lookup_v1\": {\n            \"embedder_id\": \"openai_embedding_generator_v1\", # Override to use OpenAI\n            \"embedder_config\": {\"model_name\": \"text-embedding-3-small\"},\n            # vector_store_id and vector_store_config can also be set here\n        }\n    }\n)\n</code></pre></p> <p>The <code>ToolLookupService</code> handles the re-indexing of tools automatically if new tools are registered (via <code>genie.register_tool_functions</code>) or if its index is explicitly invalidated (e.g., <code>genie._tool_lookup_service.invalidate_index()</code>).</p>"},{"location":"guides/using_command_processors/","title":"Using Command Processors","text":"<p>Command Processors in Genie Tooling are responsible for interpreting a user's natural language command, selecting an appropriate tool, and extracting the necessary parameters for that tool. The primary way to use a command processor is via the <code>genie.run_command()</code> method.</p>"},{"location":"guides/using_command_processors/#genierun_command","title":"<code>genie.run_command()</code>","text":"<pre><code>async def run_command(\n    self, \n    command: str, \n    processor_id: Optional[str] = None,\n    conversation_history: Optional[List[ChatMessage]] = None\n) -&gt; Any:\n</code></pre> <ul> <li><code>command</code>: The natural language command string from the user.</li> <li><code>processor_id</code>: Optional. The ID of the <code>CommandProcessorPlugin</code> to use. If <code>None</code>, the default command processor configured in <code>MiddlewareConfig</code> (via <code>features.command_processor</code> or <code>default_command_processor_id</code>) is used.</li> <li><code>conversation_history</code>: Optional list of previous <code>ChatMessage</code> dictionaries to provide context.</li> </ul> <p>The method returns a dictionary, typically including: *   <code>tool_result</code>: The result from the executed tool, if a tool was chosen and run successfully. *   <code>thought_process</code>: An explanation from the processor (especially LLM-based ones). *   <code>error</code>: An error message if processing or tool execution failed. *   <code>message</code>: A message if, for example, no tool was selected. *   <code>hitl_decision</code>: If HITL was triggered, this contains the approval response.</p> <p>Important: Any tool that a command processor might select must be enabled in <code>MiddlewareConfig.tool_configurations</code>.</p>"},{"location":"guides/using_command_processors/#configuring-command-processors","title":"Configuring Command Processors","text":"<p>You select and configure command processors using <code>FeatureSettings</code> or explicit configurations in <code>MiddlewareConfig</code>.</p>"},{"location":"guides/using_command_processors/#1-simple-keyword-processor-simple_keyword","title":"1. Simple Keyword Processor (<code>simple_keyword</code>)","text":"<p>This processor matches keywords in the user's command against a predefined map to select a tool. It then prompts the user for parameters if the selected tool requires them.</p> <p>Configuration via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        command_processor=\"simple_keyword\"\n    ),\n    # Explicit configuration for the simple_keyword_processor_v1\n    command_processor_configurations={\n        \"simple_keyword_processor_v1\": { # Canonical ID\n            \"keyword_map\": {\n                \"calculate\": \"calculator_tool\",\n                \"math\": \"calculator_tool\",\n                \"weather\": \"open_weather_map_tool\"\n            },\n            \"keyword_priority\": [\"calculate\", \"math\", \"weather\"] # Order for matching\n        }\n    },\n    tool_configurations={ # Tools must be enabled\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {}\n    }\n)\n# genie = await Genie.create(config=app_config)\n# result = await genie.run_command(\"calculate 10 + 5\") \n# The processor will prompt for num1, num2, operation for calculator_tool.\n</code></pre>"},{"location":"guides/using_command_processors/#2-llm-assisted-processor-llm_assisted","title":"2. LLM-Assisted Processor (<code>llm_assisted</code>)","text":"<p>This processor uses an LLM to understand the command, select the most appropriate tool from a list of available tools, and extract its parameters.</p> <p>Configuration via <code>FeatureSettings</code>:</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\", # The LLM to be used by the processor\n        llm_ollama_model_name=\"mistral:latest\",\n\n        command_processor=\"llm_assisted\",\n\n        # Configure how tools are presented to the LLM\n        command_processor_formatter_id_alias=\"compact_text_formatter\", \n\n        # Configure tool lookup to help the LLM (optional but recommended)\n        tool_lookup=\"embedding\", # Use embedding-based lookup\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\", # Formatter for indexing tools\n        tool_lookup_embedder_id_alias=\"st_embedder\" # Embedder for tool descriptions\n    ),\n    # Optionally, provide specific settings for the LLM-assisted processor\n    command_processor_configurations={\n        \"llm_assisted_tool_selection_processor_v1\": { # Canonical ID\n            \"tool_lookup_top_k\": 3, # Show top 3 tools from lookup to the LLM\n            # \"system_prompt_template\": \"Your custom system prompt...\" # Override default prompt\n        }\n    },\n    tool_configurations={ # Any tool the LLM might pick must be enabled\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {} \n        # Add other tools the LLM might be expected to use\n    }\n)\n# genie = await Genie.create(config=app_config)\n# result = await genie.run_command(\"What's the weather like in Berlin tomorrow?\")\n</code></pre> <p>Key aspects for <code>llm_assisted</code> processor: *   LLM Dependency: It requires a configured LLM provider (<code>features.llm</code>). *   Tool Formatting: It uses a <code>DefinitionFormatterPlugin</code> (specified by <code>command_processor_formatter_id_alias</code> or an explicit <code>tool_formatter_id</code> in its configuration) to format tool definitions for the LLM prompt. *   Tool Lookup (Optional but Recommended): If <code>features.tool_lookup</code> is enabled (e.g., <code>\"embedding\"</code> or <code>\"keyword\"</code>), the processor first uses the <code>ToolLookupService</code> to find a smaller set of relevant tools. Only these candidate tools are then presented to the LLM. This improves efficiency and accuracy. The <code>tool_lookup_top_k</code> parameter in its configuration controls how many tools from the lookup are passed to the LLM.</p> <p>See the Tool Lookup Guide for more on configuring tool lookup.</p>"},{"location":"guides/using_conversation_state/","title":"Managing Conversation State (<code>genie.conversation</code>)","text":"<p>Genie Tooling provides an interface for managing conversation state, primarily the history of messages in a chat interaction. This is accessible via <code>genie.conversation</code>.</p>"},{"location":"guides/using_conversation_state/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>ConversationInterface</code> (<code>genie.conversation</code>): The facade interface for all conversation state operations.</li> <li><code>ConversationStateProviderPlugin</code>: Responsible for storing, retrieving, and deleting conversation states. Each state is typically identified by a unique <code>session_id</code>.<ul> <li>Built-in: <code>InMemoryStateProviderPlugin</code> (alias: <code>in_memory_convo_provider</code>), <code>RedisStateProviderPlugin</code> (alias: <code>redis_convo_provider</code>).</li> </ul> </li> <li><code>ConversationState</code> (TypedDict): The structure for conversation data:     <pre><code>from typing import List, Dict, Optional, Any\nfrom genie_tooling.llm_providers.types import ChatMessage # Assuming ChatMessage is defined\n\nclass ConversationState(TypedDict):\n    session_id: str\n    history: List[ChatMessage]\n    metadata: Optional[Dict[str, Any]] \n</code></pre></li> </ul>"},{"location":"guides/using_conversation_state/#configuration","title":"Configuration","text":"<p>You configure the default conversation state provider via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code> settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        conversation_state_provider=\"in_memory_convo_provider\" # Default\n        # OR for Redis:\n        # conversation_state_provider=\"redis_convo_provider\" \n    ),\n    # Example: Configure the RedisStateProviderPlugin if chosen\n    # conversation_state_provider_configurations={\n    #     \"redis_conversation_state_v1\": { # Canonical ID\n    #         \"redis_url\": \"redis://localhost:6379/2\",\n    #         \"key_prefix\": \"my_app_cs:\",\n    #         \"default_ttl_seconds\": 7200 # 2 hours\n    #     }\n    # }\n)\n</code></pre> <p>Explicit Configuration:</p> <pre><code>app_config = MiddlewareConfig(\n    default_conversation_state_provider_id=\"in_memory_conversation_state_v1\",\n    # conversation_state_provider_configurations={ ... }\n)\n</code></pre>"},{"location":"guides/using_conversation_state/#using-genieconversation","title":"Using <code>genie.conversation</code>","text":""},{"location":"guides/using_conversation_state/#1-loading-conversation-state","title":"1. Loading Conversation State","text":"<pre><code># Assuming 'genie' is an initialized Genie instance\nsession_id = \"user123_chat_abc\"\nstate = await genie.conversation.load_state(session_id)\n# state = await genie.conversation.load_state(session_id, provider_id=\"custom_store\") # Optional\n\nif state:\n    print(f\"Loaded history for {session_id}: {state['history']}\")\nelse:\n    print(f\"No existing state for {session_id}.\")\n</code></pre>"},{"location":"guides/using_conversation_state/#2-adding-a-message-to-a-session","title":"2. Adding a Message to a Session","text":"<p>This method handles loading the existing state (or creating a new one if it doesn't exist), appending the message, and saving the updated state.</p> <p><pre><code>from genie_tooling.llm_providers.types import ChatMessage\n\nsession_id = \"user123_chat_abc\"\nnew_user_message: ChatMessage = {\"role\": \"user\", \"content\": \"What's new?\"}\n\nawait genie.conversation.add_message(session_id, new_user_message)\n# await genie.conversation.add_message(session_id, new_user_message, provider_id=\"custom_store\")\n\n# If you then get an assistant response:\n# assistant_response: ChatMessage = {\"role\": \"assistant\", \"content\": \"Not much, how about you?\"}\n# await genie.conversation.add_message(session_id, assistant_response)\n</code></pre> The <code>add_message</code> method automatically updates a <code>last_updated</code> timestamp in the state's metadata. If it's a new session, it also adds a <code>created_at</code> timestamp.</p>"},{"location":"guides/using_conversation_state/#3-saving-conversation-state-explicitly","title":"3. Saving Conversation State (Explicitly)","text":"<p>While <code>add_message</code> handles saving, you can explicitly save a state if you've modified it directly (e.g., adding custom metadata).</p> <pre><code>from genie_tooling.prompts.conversation.types import ConversationState # Correct import\n\nsession_id = \"user123_chat_abc\"\ncurrent_state = await genie.conversation.load_state(session_id)\nif not current_state:\n    current_state = ConversationState(session_id=session_id, history=[], metadata={})\n\n# Modify state\ncurrent_state[\"history\"].append({\"role\": \"system\", \"content\": \"Context updated.\"})\nif current_state.get(\"metadata\") is None: current_state[\"metadata\"] = {}\ncurrent_state[\"metadata\"][\"custom_flag\"] = True \n\nawait genie.conversation.save_state(current_state)\n</code></pre>"},{"location":"guides/using_conversation_state/#4-deleting-conversation-state","title":"4. Deleting Conversation State","text":"<pre><code>session_id = \"user123_chat_to_delete\"\nwas_deleted = await genie.conversation.delete_state(session_id)\nif was_deleted:\n    print(f\"State for session {session_id} deleted.\")\nelse:\n    print(f\"State for session {session_id} not found or delete failed.\")\n</code></pre>"},{"location":"guides/using_conversation_state/#creating-custom-conversation-state-providers","title":"Creating Custom Conversation State Providers","text":"<p>Implement the <code>ConversationStateProviderPlugin</code> protocol, defining <code>load_state</code>, <code>save_state</code>, and <code>delete_state</code> methods to interact with your chosen backend (database, file, etc.). Register your plugin via entry points or <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_guardrails/","title":"Using Guardrails","text":"<p>Guardrails in Genie Tooling provide a mechanism to enforce policies and safety checks on inputs, outputs, and tool usage attempts. They are implemented as plugins and integrated into the core <code>Genie</code> facade operations.</p>"},{"location":"guides/using_guardrails/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>GuardrailManager</code>: Orchestrates the execution of different types of guardrail plugins.</li> <li>Guardrail Plugin Types:<ul> <li><code>InputGuardrailPlugin</code>: Checks data provided to the system or an LLM (e.g., user prompts, chat messages).</li> <li><code>OutputGuardrailPlugin</code>: Checks data produced by the system or an LLM (e.g., LLM responses, tool execution results before final output).</li> <li><code>ToolUsageGuardrailPlugin</code>: Checks if a specific tool usage attempt (tool + parameters) is permissible before execution.</li> </ul> </li> <li><code>GuardrailViolation</code> (TypedDict): The result of a guardrail check:     <pre><code>from typing import Literal, Optional, Dict, Any, TypedDict\n\nclass GuardrailViolation(TypedDict):\n    action: Literal[\"allow\", \"block\", \"warn\"]\n    reason: Optional[str]\n    guardrail_id: Optional[str]\n    details: Optional[Dict[str, Any]]\n</code></pre><ul> <li><code>allow</code>: The check passed.</li> <li><code>block</code>: The operation should be prevented.</li> <li><code>warn</code>: The operation can proceed, but a warning should be logged or noted.</li> </ul> </li> </ul>"},{"location":"guides/using_guardrails/#configuration","title":"Configuration","text":"<p>Guardrails are configured in <code>MiddlewareConfig</code>, primarily through <code>FeatureSettings</code> for enabling lists of guardrails, and <code>guardrail_configurations</code> for specific plugin settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        input_guardrails=[\"keyword_blocklist_guardrail\"], # Enable by alias\n        output_guardrails=[\"keyword_blocklist_guardrail\"],\n        # tool_usage_guardrails=[\"my_custom_tool_usage_policy_v1\"] # Example\n    ),\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": { # Canonical ID\n            \"blocklist\": [\"unsafe_topic\", \"banned_phrase\"],\n            \"case_sensitive\": False,\n            \"action_on_match\": \"block\" # or \"warn\"\n        },\n        # \"my_custom_tool_usage_policy_v1\": { ... }\n    }\n)\n</code></pre> <ul> <li><code>features.input_guardrails</code>, <code>features.output_guardrails</code>, <code>features.tool_usage_guardrails</code>: Lists of plugin IDs or aliases for guardrails to activate for each category.</li> <li><code>guardrail_configurations</code>: A dictionary where keys are canonical guardrail plugin IDs (or aliases) and values are their specific configuration dictionaries.</li> </ul>"},{"location":"guides/using_guardrails/#implicit-integration","title":"Implicit Integration","text":"<p>Guardrails are automatically invoked by relevant <code>Genie</code> facade methods:</p> <ul> <li>Input Guardrails:<ul> <li>Checked by <code>genie.llm.chat()</code> and <code>genie.llm.generate()</code> before sending data to the LLM.</li> <li>Checked by <code>genie.run_command()</code> on the user's command string before processing.</li> </ul> </li> <li>Output Guardrails:<ul> <li>Checked by <code>genie.llm.chat()</code> and <code>genie.llm.generate()</code> on the LLM's response before returning it.</li> <li>Checked by <code>genie.execute_tool()</code> (via the invocation strategy) on the raw tool result before transformation.</li> </ul> </li> <li>Tool Usage Guardrails:<ul> <li>Checked by <code>genie.execute_tool()</code> (via the invocation strategy) before the tool's <code>execute()</code> method is called.</li> <li>Checked by <code>genie.run_command()</code> after a tool and its parameters have been determined by the command processor, but before execution and before HITL (if HITL is also active).</li> </ul> </li> </ul> <p>Behavior on \"block\": *   If an input guardrail blocks, the operation (e.g., LLM call) is prevented, and a <code>PermissionError</code> is typically raised by the <code>Genie</code> facade method. *   If an output guardrail blocks, the original output is replaced with a message indicating it was blocked (e.g., <code>\"[RESPONSE BLOCKED: Reason]\"</code>). *   If a tool usage guardrail blocks, the tool execution is prevented, and an error is typically returned by <code>genie.run_command()</code> or <code>genie.execute_tool()</code>.</p>"},{"location":"guides/using_guardrails/#built-in-guardrails","title":"Built-in Guardrails","text":"<ul> <li><code>KeywordBlocklistGuardrailPlugin</code> (alias: <code>keyword_blocklist_guardrail</code>):<ul> <li>Implements <code>InputGuardrailPlugin</code> and <code>OutputGuardrailPlugin</code>.</li> <li>Checks text data against a configurable list of keywords.</li> <li>Configuration:<ul> <li><code>blocklist</code> (List[str]): Keywords to block/warn on.</li> <li><code>case_sensitive</code> (bool, default: <code>False</code>): Whether matching is case-sensitive.</li> <li><code>action_on_match</code> (Literal[\"block\", \"warn\"], default: <code>\"block\"</code>): Action to take if a keyword is found.</li> </ul> </li> </ul> </li> </ul>"},{"location":"guides/using_guardrails/#creating-custom-guardrail-plugins","title":"Creating Custom Guardrail Plugins","text":"<ol> <li> <p>Choose the Base Protocol:</p> <ul> <li><code>InputGuardrailPlugin</code>: Implement <code>async def check_input(self, data: Any, context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>.</li> <li><code>OutputGuardrailPlugin</code>: Implement <code>async def check_output(self, data: Any, context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>.</li> <li><code>ToolUsageGuardrailPlugin</code>: Implement <code>async def check_tool_usage(self, tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>. A single plugin class can implement multiple of these protocols if it's designed to check different types of data.</li> </ul> </li> <li> <p>Implement the Check Logic: Your method should analyze the <code>data</code> (and <code>context</code> or <code>tool</code>/<code>params</code>) and return a <code>GuardrailViolation</code> dictionary.</p> </li> <li> <p>Register Your Plugin: Use entry points in <code>pyproject.toml</code> or place it in a directory specified by <code>plugin_dev_dirs</code> in <code>MiddlewareConfig</code>.</p> </li> <li> <p>Configure It: Add its ID to the appropriate list in <code>features</code> (e.g., <code>features.input_guardrails</code>) and provide any necessary configuration in <code>guardrail_configurations</code>.</p> </li> </ol>"},{"location":"guides/using_human_in_loop/","title":"Human-in-the-Loop (HITL) Approvals (<code>genie.human_in_loop</code>)","text":"<p>Genie Tooling supports Human-in-the-Loop (HITL) workflows, allowing critical actions or ambiguous decisions to be paused for human review and approval before proceeding. This is primarily managed via the <code>genie.human_in_loop</code> interface.</p>"},{"location":"guides/using_human_in_loop/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>HITLInterface</code> (<code>genie.human_in_loop</code>): The facade interface for requesting human approval.</li> <li><code>HumanApprovalRequestPlugin</code>: A plugin responsible for presenting an approval request to a human and collecting their response.<ul> <li>Built-in: <code>CliApprovalPlugin</code> (alias: <code>cli_hitl_approver</code>) - Prompts on the command line.</li> </ul> </li> <li><code>ApprovalRequest</code> (TypedDict): Data structure for an approval request:     <pre><code>from typing import Literal, Optional, Dict, Any, TypedDict\n\nclass ApprovalRequest(TypedDict):\n    request_id: str\n    prompt: str # Message shown to the human\n    data_to_approve: Dict[str, Any] # Data needing approval\n    context: Optional[Dict[str, Any]]\n    timeout_seconds: Optional[int]\n</code></pre></li> <li><code>ApprovalResponse</code> (TypedDict): Data structure for the human's response:     <pre><code>class ApprovalResponse(TypedDict):\n    request_id: str\n    status: Literal[\"pending\", \"approved\", \"denied\", \"timeout\", \"error\"]\n    approver_id: Optional[str]\n    reason: Optional[str] # Reason for denial or comments\n    timestamp: Optional[float]\n</code></pre></li> </ul>"},{"location":"guides/using_human_in_loop/#configuration","title":"Configuration","text":"<p>Configure the default HITL approval plugin via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code>.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        hitl_approver=\"cli_hitl_approver\" # Use CLI for approvals\n    ),\n    # Example: Configure the CliApprovalPlugin (though it has no specific config by default)\n    # hitl_approver_configurations={\n    #     \"cli_approval_plugin_v1\": {} \n    # }\n)\n</code></pre>"},{"location":"guides/using_human_in_loop/#integration-with-genierun_command","title":"Integration with <code>genie.run_command()</code>","text":"<p>The most common use case for HITL is to require approval before a tool selected by <code>genie.run_command()</code> is executed. If an <code>HITLManager</code> is active (i.e., <code>features.hitl_approver</code> is not <code>\"none\"</code>), <code>genie.run_command()</code> will automatically: 1.  Determine the tool and parameters. 2.  Create an <code>ApprovalRequest</code> with the tool ID and parameters. 3.  Call <code>genie.human_in_loop.request_approval()</code>. 4.  If approved, execute the tool. If denied or timed out, return an error/message.</p> <p>Example: If <code>hitl_approver</code> is set to <code>\"cli_hitl_approver\"</code> in <code>FeatureSettings</code> and <code>calculator_tool</code> is enabled in <code>tool_configurations</code>: <pre><code># genie = await Genie.create(config=app_config_with_hitl)\n# command_result = await genie.run_command(\"What is 15 times 7?\") \n\n# The CLI will prompt:\n# --- HUMAN APPROVAL REQUIRED ---\n# Request ID: &lt;uuid&gt;\n# Prompt: Approve execution of tool 'calculator_tool' with params: {'num1': 15, 'num2': 7, 'operation': 'multiply'} for goal 'What is 15 times 7?'?\n# Data/Action: {'tool_id': 'calculator_tool', 'params': {'num1': 15, 'num2': 7, 'operation': 'multiply'}, 'step_reasoning': None}\n# Approve? (yes/no/y/n): \n</code></pre> If the user types \"no\" and provides a reason, <code>command_result</code> might look like: <pre><code>{\n  \"error\": \"Tool execution denied by HITL: User intervention required.\",\n  \"thought_process\": \"The LLM selected calculator_tool for 15 times 7.\",\n  \"hitl_decision\": {\n    \"request_id\": \"...\", \"status\": \"denied\", \"approver_id\": \"cli_user\", \n    \"reason\": \"User intervention required.\", \"timestamp\": ...\n  }\n}\n</code></pre></p>"},{"location":"guides/using_human_in_loop/#manual-approval-requests-with-geniehuman_in_looprequest_approval","title":"Manual Approval Requests with <code>genie.human_in_loop.request_approval()</code>","text":"<p>You can also manually trigger an approval request from your custom logic:</p> <pre><code>from genie_tooling.hitl.types import ApprovalRequest\nimport uuid\n\nrequest_details = ApprovalRequest(\n    request_id=str(uuid.uuid4()),\n    prompt=\"Is it okay to proceed with this sensitive operation on customer data?\",\n    data_to_approve={\"customer_id\": \"cust_789\", \"operation\": \"data_wipe\"},\n    timeout_seconds=300 # 5 minutes\n)\n\napproval_response = await genie.human_in_loop.request_approval(request_details)\n# approval_response = await genie.human_in_loop.request_approval(request_details, approver_id=\"my_specific_hitl_plugin\")\n\n\nif approval_response[\"status\"] == \"approved\":\n    print(f\"Operation approved by {approval_response.get('approver_id', 'N/A')}. Reason: {approval_response.get('reason')}\")\n    # ... proceed with sensitive operation ...\nelse:\n    print(f\"Operation {approval_response['status']}. Reason: {approval_response.get('reason')}\")\n</code></pre>"},{"location":"guides/using_human_in_loop/#creating-custom-hitl-approval-plugins","title":"Creating Custom HITL Approval Plugins","text":"<p>Implement the <code>HumanApprovalRequestPlugin</code> protocol, defining the <code>request_approval(request: ApprovalRequest)</code> method. This method should handle the presentation of the request to a human (e.g., via a web UI, messaging platform, ticketing system) and return an <code>ApprovalResponse</code>. Register your plugin via entry points or <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_llm_providers/","title":"Using LLM Providers","text":"<p>Genie Tooling allows you to easily interact with various Large Language Models (LLMs) through a unified interface: <code>genie.llm</code>. This interface abstracts the specifics of different LLM provider APIs.</p>"},{"location":"guides/using_llm_providers/#the-geniellm-interface","title":"The <code>genie.llm</code> Interface","text":"<p>Once you have a <code>Genie</code> instance, you can access LLM functionalities:</p> <ul> <li><code>async genie.llm.generate(prompt: str, provider_id: Optional[str] = None, stream: bool = False, **kwargs) -&gt; Union[LLMCompletionResponse, AsyncIterable[LLMCompletionChunk]]</code>:     For text completion or generation tasks.<ul> <li><code>prompt</code>: The input prompt string.</li> <li><code>provider_id</code>: Optional. The ID of the LLM provider plugin to use. If <code>None</code>, the default LLM provider configured in <code>MiddlewareConfig</code> (via <code>features.llm</code> or <code>default_llm_provider_id</code>) is used.</li> <li><code>stream</code>: Optional (default <code>False</code>). If <code>True</code>, returns an async iterable of <code>LLMCompletionChunk</code> objects.</li> <li><code>**kwargs</code>: Additional provider-specific parameters (e.g., <code>temperature</code>, <code>max_tokens</code>, <code>model</code> to override the default for that provider, <code>output_schema</code> for structured output).</li> </ul> </li> <li><code>async genie.llm.chat(messages: List[ChatMessage], provider_id: Optional[str] = None, stream: bool = False, **kwargs) -&gt; Union[LLMChatResponse, AsyncIterable[LLMChatChunk]]</code>:     For conversational interactions.<ul> <li><code>messages</code>: A list of <code>ChatMessage</code> dictionaries (see Types).</li> <li><code>provider_id</code>: Optional. The ID of the LLM provider plugin to use.</li> <li><code>stream</code>: Optional (default <code>False</code>). If <code>True</code>, returns an async iterable of <code>LLMChatChunk</code> objects.</li> <li><code>**kwargs</code>: Additional provider-specific parameters (e.g., <code>temperature</code>, <code>tools</code>, <code>tool_choice</code>, <code>output_schema</code> for structured output).</li> </ul> </li> <li><code>async genie.llm.parse_output(response: Union[LLMChatResponse, LLMCompletionResponse], parser_id: Optional[str] = None, schema: Optional[Any] = None) -&gt; ParsedOutput</code>:     Parses the text content from an LLM response (either <code>LLMChatResponse</code> or <code>LLMCompletionResponse</code>) using a configured <code>LLMOutputParserPlugin</code>.<ul> <li><code>response</code>: The LLM response object.</li> <li><code>parser_id</code>: Optional. The ID of the <code>LLMOutputParserPlugin</code> to use. If <code>None</code>, the default configured parser is used.</li> <li><code>schema</code>: Optional. A schema (e.g., Pydantic model class, JSON schema dict) to guide parsing, if supported by the parser.</li> <li>Returns the parsed data (e.g., a dictionary, a Pydantic model instance).</li> <li>Raises <code>ValueError</code> if parsing fails or content cannot be extracted.</li> </ul> </li> </ul>"},{"location":"guides/using_llm_providers/#configuring-llm-providers","title":"Configuring LLM Providers","text":"<p>LLM providers are primarily configured using <code>FeatureSettings</code> in your <code>MiddlewareConfig</code>.</p>"},{"location":"guides/using_llm_providers/#example-using-ollama","title":"Example: Using Ollama","text":"<p><pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\nfrom genie_tooling.llm_providers.types import ChatMessage\nimport asyncio\n\nasync def main():\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            llm=\"ollama\",  # Select Ollama as the default\n            llm_ollama_model_name=\"mistral:latest\" # Specify the model for Ollama\n        )\n    )\n    genie = await Genie.create(config=app_config)\n\n    response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hi from Genie!\"}])\n    print(response['message']['content'])\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> Ensure Ollama is running (<code>ollama serve</code>) and the specified model (<code>mistral:latest</code>) is pulled (<code>ollama pull mistral</code>).</p>"},{"location":"guides/using_llm_providers/#example-using-openai","title":"Example: Using OpenAI","text":"<pre><code># Requires OPENAI_API_KEY environment variable to be set.\n# Genie's default EnvironmentKeyProvider will pick it up.\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\"\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-gemini","title":"Example: Using Gemini","text":"<pre><code># Requires GOOGLE_API_KEY environment variable to be set.\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"gemini\",\n        llm_gemini_model_name=\"gemini-1.5-flash-latest\"\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-llamacpp-server-mode","title":"Example: Using Llama.cpp (Server Mode)","text":"<pre><code># Assumes a Llama.cpp server is running at the specified URL.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp\",\n        llm_llama_cpp_model_name=\"your-model-alias-on-server\", # Alias/model server uses\n        llm_llama_cpp_base_url=\"http://localhost:8080\" # Default, adjust if needed\n        # llm_llama_cpp_api_key_name=\"MY_LLAMA_SERVER_KEY\" # If server requires API key\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-llamacpp-internal-mode","title":"Example: Using Llama.cpp (Internal Mode)","text":"<pre><code># Requires llama-cpp-python library and a GGUF model file.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp_internal\",\n        llm_llama_cpp_internal_model_path=\"/path/to/your/model.gguf\", # IMPORTANT: Set this path\n        llm_llama_cpp_internal_n_gpu_layers=-1, # Offload all possible layers to GPU\n        llm_llama_cpp_internal_n_ctx=4096,      # Example context size\n        llm_llama_cpp_internal_chat_format=\"mistral\" # Or \"llama-2\", \"chatml\", etc.\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#overriding-provider-settings","title":"Overriding Provider Settings","text":"<p>You can override settings for specific LLM providers using the <code>llm_provider_configurations</code> dictionary in <code>MiddlewareConfig</code>. Keys can be the canonical plugin ID or a recognized alias.</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\", # Default is OpenAI\n        llm_openai_model_name=\"gpt-3.5-turbo\" # Default model for OpenAI\n    ),\n    llm_provider_configurations={\n        \"openai_llm_provider_v1\": { # Canonical ID for OpenAI provider\n            \"model_name\": \"gpt-4-turbo-preview\", # Override the model for OpenAI\n            \"request_timeout_seconds\": 180 \n        },\n        \"ollama\": { # Alias for Ollama provider\n            \"model_name\": \"llama3:latest\",\n            \"request_timeout_seconds\": 240\n        },\n        \"llama_cpp_internal_llm_provider_v1\": { # Canonical ID for internal Llama.cpp\n            \"model_path\": \"/another/model.gguf\",\n            \"n_gpu_layers\": 0, # CPU only for this specific override\n            \"chat_format\": \"chatml\",\n            \"model_name_for_logging\": \"custom_internal_llama\"\n        }\n    }\n)\n# genie = await Genie.create(config=app_config)\n\n# This will use OpenAI with gpt-4-turbo-preview\n# await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello OpenAI!\"}])\n\n# This will use Ollama with llama3:latest\n# await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Ollama!\"}], provider_id=\"ollama\")\n\n# This will use the internal Llama.cpp provider with /another/model.gguf\n# await genie.llm.generate(\"Test internal Llama.cpp\", provider_id=\"llama_cpp_internal\")\n</code></pre>"},{"location":"guides/using_llm_providers/#api-keys-and-keyprovider","title":"API Keys and <code>KeyProvider</code>","text":"<p>LLM providers that require API keys (like OpenAI, Gemini, or a secured Llama.cpp server) will attempt to fetch them using the configured <code>KeyProvider</code>. By default, Genie uses <code>EnvironmentKeyProvider</code>, which reads keys from environment variables (e.g., <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>). You can provide a custom <code>KeyProvider</code> instance to <code>Genie.create()</code> for more sophisticated key management. See the Configuration Guide for details. The internal Llama.cpp provider does not use API keys managed via <code>KeyProvider</code>.</p>"},{"location":"guides/using_llm_providers/#structured-output-gbnf-with-llamacpp-providers","title":"Structured Output (GBNF with Llama.cpp Providers)","text":"<p>Both the Llama.cpp server provider (<code>llama_cpp_llm_provider_v1</code>) and the internal Llama.cpp provider (<code>llama_cpp_internal_llm_provider_v1</code>) support GBNF grammar for constrained, structured output. You can pass a Pydantic model class or a JSON schema dictionary to the <code>output_schema</code> parameter of <code>genie.llm.generate()</code> or <code>genie.llm.chat()</code>.</p> <p><pre><code>from pydantic import BaseModel\n\nclass MyData(BaseModel):\n    name: str\n    value: int\n\n# Assuming 'genie' is configured with a Llama.cpp provider (server or internal)\n# For Llama.cpp server, ensure it's started with GBNF support enabled.\n# For Llama.cpp internal, ensure the model is GBNF-compatible.\n\n# Using with generate:\n# response_gen = await genie.llm.generate(\n#     prompt=\"Extract name and value: Name is Alpha, Value is 10. Output JSON.\",\n#     output_schema=MyData,\n#     # Llama.cpp server might need n_predict for GBNF with /v1/completions\n#     # n_predict=256 \n# )\n# if response_gen['text']:\n#     parsed_gen = await genie.llm.parse_output(response_gen, schema=MyData)\n\n# Using with chat:\n# response_chat = await genie.llm.chat(\n#     messages=[{\"role\": \"user\", \"content\": \"User: Name is Beta, Value is 20. Output JSON.\"}],\n#     output_schema=MyData\n# )\n# if response_chat['message']['content']:\n#     parsed_chat = await genie.llm.parse_output(response_chat, schema=MyData)\n</code></pre> The provider will attempt to convert the Pydantic model/JSON schema into a GBNF grammar string and pass it to the Llama.cpp backend. Ensure your prompt instructs the LLM to output JSON matching the schema.</p>"},{"location":"guides/using_llm_providers/#parsing-llm-output","title":"Parsing LLM Output","text":"<p>Often, you'll want an LLM to produce structured output (e.g., JSON). The <code>genie.llm.parse_output()</code> method helps with this.</p> <p>Example: Parsing JSON output <pre><code># Assuming 'genie' is initialized and an LLM provider is configured.\n# And a JSONOutputParserPlugin (json_output_parser_v1) is available.\n\n# Configure default output parser (optional, can also specify per call)\n# app_config = MiddlewareConfig(\n#     features=FeatureSettings(llm=\"ollama\", default_llm_output_parser=\"json_output_parser\")\n# )\n# genie = await Genie.create(config=app_config)\n\n\nprompt_for_json = \"Generate a JSON object with keys 'name' and 'city'.\"\nllm_response = await genie.llm.generate(prompt_for_json)\n# llm_response['text'] might be: 'Sure, here is the JSON: {\"name\": \"Test User\", \"city\": \"Genieville\"}'\n\ntry:\n    # Uses default parser if configured, or specify with parser_id=\"json_output_parser_v1\"\n    parsed_data = await genie.llm.parse_output(llm_response) \n    print(f\"Parsed data: {parsed_data}\")\n    # Output: Parsed data: {'name': 'Test User', 'city': 'Genieville'}\nexcept ValueError as e:\n    print(f\"Failed to parse LLM output: {e}\")\n</code></pre></p> <p>Example: Parsing into a Pydantic model <pre><code>from pydantic import BaseModel\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n# Configure Pydantic parser (pydantic_output_parser_v1)\n# app_config = MiddlewareConfig(\n#     features=FeatureSettings(llm=\"ollama\", default_llm_output_parser=\"pydantic_output_parser\")\n# )\n# genie = await Genie.create(config=app_config)\n\nprompt_for_pydantic = \"Create a JSON for a user named Bob, age 42.\"\nllm_response = await genie.llm.generate(prompt_for_pydantic)\n# llm_response['text'] might be: '```json\\n{\"name\": \"Bob\", \"age\": 42}\\n```'\n\ntry:\n    user_instance = await genie.llm.parse_output(llm_response, schema=UserInfo)\n    if isinstance(user_instance, UserInfo):\n        print(f\"User: {user_instance.name}, Age: {user_instance.age}\")\nexcept ValueError as e:\n    print(f\"Failed to parse into Pydantic model: {e}\")\n</code></pre> See the specific <code>LLMOutputParserPlugin</code> documentation for details on their capabilities and configuration (e.g., <code>JSONOutputParserPlugin</code>, <code>PydanticOutputParserPlugin</code>).</p>"},{"location":"guides/using_llm_providers/#chatmessage-type","title":"<code>ChatMessage</code> Type","text":"<p>The <code>messages</code> parameter for <code>genie.llm.chat()</code> expects a list of <code>ChatMessage</code> dictionaries:</p> <pre><code>from genie_tooling.llm_providers.types import ChatMessage, ToolCall\n\n# User message\nuser_message: ChatMessage = {\"role\": \"user\", \"content\": \"What's the weather in London?\"}\n\n# Assistant message (simple text response)\nassistant_text_response: ChatMessage = {\"role\": \"assistant\", \"content\": \"The weather in London is pleasant.\"}\n\n# Assistant message requesting a tool call\nassistant_tool_call_request: ChatMessage = {\n    \"role\": \"assistant\",\n    \"content\": None, # Content can be None if only tool_calls are present\n    \"tool_calls\": [\n        {\n            \"id\": \"call_weather_london_123\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"get_weather\", \"arguments\": '{\"city\": \"London\", \"units\": \"celsius\"}'}\n        }\n    ]\n}\n\n# Tool message (response from executing a tool)\ntool_response_message: ChatMessage = {\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_weather_london_123\", # Matches the ID from assistant's request\n    \"name\": \"get_weather\", # Name of the function that was called\n    \"content\": '{\"temperature\": 15, \"condition\": \"Cloudy\"}' # JSON string of the tool's output\n}\n</code></pre> <p>The <code>LLMChatResponse</code> from <code>genie.llm.chat()</code> will contain an assistant's message in this format.</p>"},{"location":"guides/using_prompts/","title":"Using the Prompt Management System (<code>genie.prompts</code>)","text":"<p>Genie Tooling provides a flexible prompt management system accessible via <code>genie.prompts</code>. This allows you to register, retrieve, and render prompt templates, separating prompt engineering concerns from your application logic.</p>"},{"location":"guides/using_prompts/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>PromptInterface</code> (<code>genie.prompts</code>): The facade interface for all prompt-related operations.</li> <li><code>PromptRegistryPlugin</code>: Responsible for storing and retrieving raw prompt template content (e.g., from a file system, database).<ul> <li>Built-in: <code>FileSystemPromptRegistryPlugin</code> (alias: <code>file_system_prompt_registry</code>).</li> </ul> </li> <li><code>PromptTemplatePlugin</code>: Responsible for rendering a raw template string with provided data.<ul> <li>Built-in: <code>BasicStringFormatTemplatePlugin</code> (alias: <code>basic_string_formatter</code>), <code>Jinja2ChatTemplatePlugin</code> (alias: <code>jinja2_chat_formatter</code>).</li> </ul> </li> </ul>"},{"location":"guides/using_prompts/#configuration","title":"Configuration","text":"<p>You configure the default prompt registry and template engine via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code> settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        prompt_registry=\"file_system_prompt_registry\", # Default\n        prompt_template_engine=\"jinja2_chat_formatter\"   # Default\n    ),\n    # Configure the FileSystemPromptRegistryPlugin\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": { # Canonical ID\n            \"base_path\": \"./my_application_prompts\",\n            \"template_suffix\": \".j2\" # If using Jinja2 templates\n        }\n    }\n    # No specific config needed for Jinja2ChatTemplatePlugin by default\n)\n</code></pre> <p>Explicit Configuration:</p> <pre><code>app_config = MiddlewareConfig(\n    default_prompt_registry_id=\"file_system_prompt_registry_v1\",\n    default_prompt_template_plugin_id=\"jinja2_chat_template_v1\",\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\"base_path\": \"prompts\"}\n    }\n)\n</code></pre>"},{"location":"guides/using_prompts/#using-genieprompts","title":"Using <code>genie.prompts</code>","text":""},{"location":"guides/using_prompts/#1-getting-raw-template-content","title":"1. Getting Raw Template Content","text":"<pre><code># Assuming 'my_application_prompts/my_task.j2' exists\ntemplate_content = await genie.prompts.get_prompt_template_content(\n    name=\"my_task\", \n    # version=\"v1\", # Optional version\n    # registry_id=\"custom_registry\" # Optional, if not using default\n)\nif template_content:\n    print(f\"Raw template: {template_content}\")\n</code></pre>"},{"location":"guides/using_prompts/#2-rendering-a-string-prompt","title":"2. Rendering a String Prompt","text":"<p>This is useful for prompts that result in a single string, often for completion LLMs.</p> <pre><code>from genie_tooling.prompts.types import PromptData\n\nprompt_data: PromptData = {\"topic\": \"AI ethics\", \"length\": \"short\"}\n\n# Uses default registry and default template engine\nrendered_string = await genie.prompts.render_prompt(\n    name=\"summarization_prompt\", # e.g., 'my_application_prompts/summarization_prompt.txt'\n    data=prompt_data\n)\nif rendered_string:\n    print(f\"Rendered string prompt: {rendered_string}\")\n    # Example: \"Summarize AI ethics in a short paragraph.\"\n</code></pre>"},{"location":"guides/using_prompts/#3-rendering-a-chat-prompt","title":"3. Rendering a Chat Prompt","text":"<p>This is used for prompts that result in a list of <code>ChatMessage</code> dictionaries, suitable for chat-based LLMs. The <code>Jinja2ChatTemplatePlugin</code> is particularly useful here as Jinja2 can easily render structured JSON/YAML.</p> <p>Example Jinja2 template (<code>my_application_prompts/chat_style_prompt.j2</code>): <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful {{ persona }}.\"},\n    {\"role\": \"user\", \"content\": \"Tell me about {{ subject }}.\"}\n]\n</code></pre></p> <p>Python code: <pre><code>from genie_tooling.prompts.types import PromptData\n\nchat_data: PromptData = {\"persona\": \"historian\", \"subject\": \"the Roman Empire\"}\n\n# Uses default registry and default template engine (configured to jinja2_chat_formatter)\nchat_messages = await genie.prompts.render_chat_prompt(\n    name=\"chat_style_prompt\", \n    data=chat_data\n)\nif chat_messages:\n    print(f\"Rendered chat messages: {chat_messages}\")\n    # Output:\n    # [\n    #   {'role': 'system', 'content': 'You are a helpful historian.'},\n    #   {'role': 'user', 'content': 'Tell me about the Roman Empire.'}\n    # ]\n    # response = await genie.llm.chat(chat_messages)\n</code></pre></p>"},{"location":"guides/using_prompts/#4-listing-available-templates","title":"4. Listing Available Templates","text":"<pre><code>available_templates = await genie.prompts.list_templates()\n# Or for a specific registry:\n# available_templates = await genie.prompts.list_templates(registry_id=\"my_other_registry\")\n\nfor template_id in available_templates:\n    print(f\"- Name: {template_id['name']}, Version: {template_id.get('version', 'N/A')}, Desc: {template_id.get('description')}\")\n</code></pre>"},{"location":"guides/using_prompts/#creating-custom-prompt-plugins","title":"Creating Custom Prompt Plugins","text":"<ul> <li><code>PromptRegistryPlugin</code>: Implement <code>get_template_content</code> and <code>list_available_templates</code>.</li> <li><code>PromptTemplatePlugin</code>: Implement <code>render</code> (for string output) and <code>render_chat_messages</code> (for <code>List[ChatMessage]</code> output).</li> </ul> <p>Register your custom plugins via entry points in <code>pyproject.toml</code> or ensure they are discoverable via <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_rag/","title":"Using Retrieval Augmented Generation (RAG)","text":"<p>Genie Tooling provides a flexible RAG system through the <code>genie.rag</code> interface, allowing you to index data from various sources and perform semantic searches.</p>"},{"location":"guides/using_rag/#core-rag-operations-via-genierag","title":"Core RAG Operations via <code>genie.rag</code>","text":"<p>The <code>genie.rag</code> interface simplifies common RAG tasks:</p> <ul> <li><code>genie.rag.index_directory(path, collection_name, ...)</code>: Indexes all supported files within a local directory.</li> <li><code>genie.rag.index_web_page(url, collection_name, ...)</code>: Fetches content from a web URL, extracts text, and indexes it.</li> <li><code>genie.rag.search(query, collection_name, top_k, ...)</code>: Performs a semantic search against an indexed collection.</li> </ul>"},{"location":"guides/using_rag/#configuring-rag-components","title":"Configuring RAG Components","text":"<p>RAG components (Document Loaders, Text Splitters, Embedding Generators, Vector Stores, Retrievers) are configured primarily using <code>FeatureSettings</code> within your <code>MiddlewareConfig</code>.</p> <pre><code>import asyncio\nfrom pathlib import Path\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            # RAG Embedder (e.g., for creating embeddings of your documents)\n            rag_embedder=\"sentence_transformer\", # Alias for \"sentence_transformer_embedder_v1\"\n            # rag_embedder_st_model_name=\"all-MiniLM-L6-v2\", # Default ST model\n\n            # RAG Vector Store (e.g., for storing and searching embeddings)\n            rag_vector_store=\"faiss\", # Alias for \"faiss_vector_store_v1\" (in-memory)\n            # Or for persistent ChromaDB:\n            # rag_vector_store=\"chroma\",\n            # rag_vector_store_chroma_path=\"./my_rag_db\",\n            # rag_vector_store_chroma_collection_name=\"my_documents\",\n            # Or for Qdrant:\n            # rag_vector_store=\"qdrant\", # Example using Qdrant\n            # rag_vector_store_qdrant_url=\"http://localhost:6333\", # Or path for local Qdrant\n            # rag_vector_store_qdrant_collection_name=\"my_qdrant_docs\",\n            # rag_vector_store_qdrant_embedding_dim=384, # Set to your embedder's dimension (e.g., 384 for all-MiniLM-L6-v2)\n\n            # Default RAG Loader (used by index_directory if not specified)\n            rag_loader=\"file_system\", # Alias for \"file_system_loader_v1\" (default for index_directory)\n            # For index_web_page, it defaults to \"web_page_loader_v1\" internally if rag_loader is \"web_page\" or not set.\n        )\n    )\n    genie = await Genie.create(config=app_config)\n\n    # Create a dummy directory and file for indexing\n    data_path = Path(\"./rag_data_example\")\n    data_path.mkdir(exist_ok=True)\n    (data_path / \"sample.txt\").write_text(\"Genie makes RAG easy and configurable.\")\n\n    # Index the directory\n    collection = \"my_sample_collection\"\n    await genie.rag.index_directory(str(data_path), collection_name=collection)\n    print(f\"Indexed documents from '{data_path}' into '{collection}'.\")\n\n    # Search the collection\n    query = \"How is RAG with Genie?\"\n    results = await genie.rag.search(query, collection_name=collection, top_k=1)\n    if results:\n        print(f\"Search for '{query}':\")\n        for res in results:\n            print(f\"  - Score: {res.score:.4f}, Content: {res.content[:100]}...\")\n    else:\n        print(f\"No results found for '{query}'.\")\n\n    # Clean up dummy data\n    (data_path / \"sample.txt\").unlink(missing_ok=True) # Added missing_ok=True\n    if data_path.exists(): data_path.rmdir() # Only rmdir if it's empty and exists\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/using_rag/#overriding-rag-component-configurations","title":"Overriding RAG Component Configurations","text":"<p>You can override configurations for specific RAG component plugins (Document Loaders, Text Splitters, Embedding Generators, Vector Stores, Retrievers) using their respective <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code>.</p> <p>For example, to configure the <code>WebPageLoader</code> to use Trafilatura for better content extraction when <code>genie.rag.index_web_page()</code> is called (and <code>rag_loader</code> feature is set to <code>\"web_page\"</code> or not set, allowing the default):</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        rag_loader=\"web_page\", # Explicitly or implicitly uses web_page_loader_v1\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\"\n    ),\n    document_loader_configurations={\n        \"web_page_loader_v1\": { # Canonical ID of the WebPageLoader\n            \"use_trafilatura\": True,\n            # \"trafilatura_include_comments\": False # Other loader-specific settings\n        }\n    },\n    embedding_generator_configurations={\n        \"sentence_transformer_embedder_v1\": {\n            \"model_name\": \"paraphrase-multilingual-MiniLM-L12-v2\" # Use a different ST model\n        }\n    }\n)\n</code></pre> <p>When calling <code>genie.rag.index_directory()</code> or <code>genie.rag.search()</code>, you can also pass <code>loader_id</code>, <code>splitter_id</code>, <code>embedder_id</code>, <code>vector_store_id</code>, <code>retriever_id</code>, and their corresponding <code>*_config</code> dictionaries to override the defaults for that specific call.</p>"},{"location":"guides/using_rag/#advanced-usage","title":"Advanced Usage","text":"<p>The <code>genie.rag</code> interface internally uses the <code>RAGManager</code>. For advanced scenarios requiring direct interaction with the <code>RAGManager</code> or individual RAG component plugins, you can access it via <code>genie._rag_manager</code> (though this is generally not needed for typical use cases).</p> <p>Refer to Creating RAG Plugins for information on developing your own RAG components.</p>"},{"location":"guides/using_tools/","title":"Using Tools","text":"<p>Genie Tooling provides robust mechanisms for defining, discovering, and executing tools. Tools represent discrete actions an AI agent can perform.</p> <p>The primary way to interact with tools is through the <code>Genie</code> facade.</p>"},{"location":"guides/using_tools/#enabling-tools","title":"Enabling Tools","text":"<p>Crucially, for a tool to be available for execution (either directly or via <code>run_command</code>), its plugin ID must be included as a key in the <code>tool_configurations</code> dictionary within your <code>MiddlewareConfig</code>. If a tool requires no specific configuration, an empty dictionary <code>{}</code> as its value is sufficient to enable it.</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\n\napp_config = MiddlewareConfig(\n    # ... other settings ...\n    tool_configurations={\n        \"calculator_tool\": {}, # Enables the built-in calculator tool\n        \"sandboxed_fs_tool_v1\": { # Enables and configures the FS tool\n            \"sandbox_base_path\": \"./my_agent_sandbox\"\n        },\n        \"my_custom_tool_id\": {} # Enables your custom tool\n    }\n)\n</code></pre>"},{"location":"guides/using_tools/#executing-tools-directly-with-genieexecute_tool","title":"Executing Tools Directly with <code>genie.execute_tool()</code>","text":"<p>If you know which tool you want to use (and it's enabled in <code>tool_configurations</code>) and have its parameters, you can execute it directly:</p> <pre><code>import asyncio\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(llm=\"none\", command_processor=\"none\"),\n        tool_configurations={\n            \"calculator_tool\": {}, # Enable calculator\n            \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": \"./my_agent_sandbox\"}\n        }\n    )\n    genie = await Genie.create(config=app_config)\n\n    # Example: Using the built-in CalculatorTool\n    calc_result = await genie.execute_tool(\n        \"calculator_tool\", \n        num1=10, \n        num2=5, \n        operation=\"multiply\"\n    )\n    print(f\"Calculator Result: {calc_result}\")\n    # Output: Calculator Result: {'result': 50.0, 'error_message': None}\n\n    # Example: Using the SandboxedFileSystemTool (if configured)\n    await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"write_file\",\n        path=\"example.txt\",\n        content=\"Hello from Genie!\"\n    )\n    print(\"Wrote to sandbox via execute_tool.\")\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/using_tools/#processing-natural-language-commands-with-genierun_command","title":"Processing Natural Language Commands with <code>genie.run_command()</code>","text":"<p>For more agentic behavior, where the system needs to interpret a natural language command to select and parameterize an enabled tool, use <code>genie.run_command()</code>. This method leverages a configured Command Processor plugin.</p> <pre><code>import asyncio\nimport json\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    # Configure Genie to use an LLM-assisted command processor\n    # Ensure Ollama is running and mistral model is pulled for this example\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            llm=\"ollama\", \n            llm_ollama_model_name=\"mistral:latest\",\n            command_processor=\"llm_assisted\",\n            tool_lookup=\"embedding\" # Enable tool lookup for the LLM processor\n        ),\n        tool_configurations={\n            \"calculator_tool\": {} # Ensure calculator is enabled\n        }\n    )\n    genie = await Genie.create(config=app_config)\n\n    command_text = \"What is the result of 75 divided by 3?\"\n    print(f\"Running command: '{command_text}'\")\n\n    command_output = await genie.run_command(command_text)\n    print(f\"Command Output: {json.dumps(command_output, indent=2)}\")\n    # Expected output might include:\n    # {\n    #   \"tool_result\": { \"result\": 25.0, \"error_message\": null },\n    #   \"thought_process\": \"The user wants to divide 75 by 3. I will use the calculator_tool...\"\n    # }\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Refer to the Using Command Processors guide for more details on configuring different command processors.</p>"},{"location":"guides/using_tools/#defining-tools","title":"Defining Tools","text":"<p>Genie supports two main ways to define tools:</p> <ol> <li>Plugin-based Tools: Create a class that inherits from <code>genie_tooling.tools.abc.Tool</code> and implements the required methods (<code>identifier</code>, <code>get_metadata</code>, <code>execute</code>). These tools are discovered via entry points or plugin development directories. See Creating Tool Plugins. Remember to enable them via <code>tool_configurations</code>.</li> <li> <p>Decorator-based Tools: Use the <code>@genie_tooling.tool</code> decorator on your Python functions. Genie can then register these decorated functions as tools using <code>await genie.register_tool_functions([...])</code>. After registration, their identifiers (typically the function names) must also be added to <code>tool_configurations</code> to be active for <code>genie.execute_tool</code> or <code>genie.run_command</code>.</p> <pre><code>from genie_tooling import tool\nfrom genie_tooling.config.models import MiddlewareConfig # For app_config\nfrom genie_tooling.genie import Genie # For Genie\nimport asyncio # For async main\n\n@tool\nasync def my_custom_utility(text: str, uppercase: bool = False) -&gt; str:\n    \"\"\"\n    A custom utility function.\n    Args:\n        text (str): The input text.\n        uppercase (bool): Whether to convert the text to uppercase.\n    Returns:\n        str: The processed text.\n    \"\"\"\n    if uppercase:\n        return text.upper()\n    return text\n\nasync def run_decorated_tool_example():\n    app_config = MiddlewareConfig(\n        tool_configurations={\n            \"my_custom_utility\": {} # Enable the decorated tool\n        }\n    )\n    genie = await Genie.create(config=app_config)\n    await genie.register_tool_functions([my_custom_utility])\n\n    result = await genie.execute_tool(\"my_custom_utility\", text=\"hello\", uppercase=True)\n    print(result) # Output: {'result': 'HELLO'}\n    await genie.close()\n\n# if __name__ == \"__main__\":\n#     asyncio.run(run_decorated_tool_example())\n</code></pre> </li> </ol>"},{"location":"guides/using_tools/#configuring-tools","title":"Configuring Tools","text":"<p>Specific tools might require configuration (e.g., the base path for <code>SandboxedFileSystemTool</code>). This is done via the <code>tool_configurations</code> dictionary in <code>MiddlewareConfig</code>, which also serves to enable the tool:</p> <p><pre><code>app_config = MiddlewareConfig(\n    # ... other settings ...\n    tool_configurations={\n        \"sandboxed_fs_tool_v1\": { # Canonical plugin ID of the tool\n            \"sandbox_base_path\": \"./agent_data_sandbox\"\n        },\n        \"my_custom_api_tool_v1\": { # Example for a hypothetical custom tool\n            \"api_endpoint\": \"https://api.example.com/custom\",\n            \"default_timeout\": 30\n        },\n        \"calculator_tool\": {} # Enable calculator with no specific config\n    }\n)\n</code></pre> The configuration dictionary provided here will be passed to the tool's <code>setup()</code> method.</p>"},{"location":"guides/using_tools/#advanced-usage","title":"Advanced Usage","text":"<p>For more fine-grained control, you can interact directly with the <code>ToolManager</code> and <code>ToolInvoker</code> components, which are accessible via the <code>Genie</code> instance (e.g., <code>genie._tool_manager</code>) if needed, though direct interaction is typically for advanced scenarios or extending Genie's core behavior.</p>"},{"location":"tutorials/E15_prompt_management_example/","title":"Tutorial: Prompt Management Example","text":"<p>This tutorial corresponds to the example file <code>examples/E15_prompt_management_example.py</code>.</p> <p>It demonstrates how to: - Configure <code>FileSystemPromptRegistryPlugin</code> and <code>Jinja2ChatTemplatePlugin</code>. - List available prompt templates. - Retrieve raw template content. - Render string-based prompts. - Render chat-based prompts suitable for LLM conversation.</p> <pre><code># Full code from examples/E15_prompt_management_example.py\n# (This will be auto-filled by your documentation generation process if configured,\n# or you can paste the example code here manually.)\n</code></pre> <p>Key Takeaways: - Use <code>genie.prompts.get_prompt_template_content()</code> to fetch raw templates. - Use <code>genie.prompts.render_prompt()</code> for simple string templates. - Use <code>genie.prompts.render_chat_prompt()</code> for structured chat message lists, especially with Jinja2 templates.</p>"},{"location":"tutorials/E16_conversation_state_example/","title":"Tutorial: Conversation State Example","text":"<p>This tutorial corresponds to the example file <code>examples/E16_conversation_state_example.py</code>.</p> <p>It demonstrates how to: - Configure a <code>ConversationStateProviderPlugin</code> (e.g., <code>InMemoryStateProviderPlugin</code>). - Load existing conversation state for a session ID. - Add new user and assistant messages to the conversation history. - Observe how metadata like <code>created_at</code> and <code>last_updated</code> is managed. - Delete conversation state.</p> <pre><code># Full code from examples/E16_conversation_state_example.py\n</code></pre> <p>Key Takeaways: - <code>genie.conversation.load_state()</code> retrieves history. - <code>genie.conversation.add_message()</code> appends to history and updates/saves the state. - <code>genie.conversation.delete_state()</code> removes a session's history.</p>"},{"location":"tutorials/E17_observability_tracing_example/","title":"Tutorial: Observability &amp; Tracing Example","text":"<p>This tutorial corresponds to the example file <code>examples/E17_observability_tracing_example.py</code>.</p> <p>It demonstrates how to: - Enable and configure an <code>InteractionTracerPlugin</code> (e.g., <code>ConsoleTracerPlugin</code>). - Observe automatic trace events generated by core <code>Genie</code> operations. - Manually emit custom trace events using <code>genie.observability.trace_event()</code>.</p> <pre><code># Full code from examples/E17_observability_tracing_example.py\n</code></pre> <p>Key Takeaways: - Core Genie operations are auto-instrumented for tracing. - <code>genie.observability.trace_event()</code> allows for custom application-level tracing. - Correlation IDs can be used to link related trace events.</p>"},{"location":"tutorials/E18_human_in_loop_example/","title":"Tutorial: Human-in-the-Loop (HITL) Example","text":"<p>This tutorial corresponds to the example file <code>examples/E18_human_in_loop_example.py</code>.</p> <p>It demonstrates how to: - Configure a <code>HumanApprovalRequestPlugin</code> (e.g., <code>CliApprovalPlugin</code>). - See HITL automatically triggered by <code>genie.run_command()</code> before tool execution. - Understand the structure of <code>ApprovalRequest</code> and <code>ApprovalResponse</code>.</p> <pre><code># Full code from examples/E18_human_in_loop_example.py\n</code></pre> <p>Key Takeaways: - HITL provides a checkpoint for critical or ambiguous agent actions. - The <code>CliApprovalPlugin</code> allows for simple console-based approvals during development. - <code>genie.run_command()</code> integrates HITL seamlessly if an approver is configured.</p>"},{"location":"tutorials/E19_llm_output_parsing_example/","title":"Tutorial: LLM Output Parsing Example","text":"<p>This tutorial corresponds to the example file <code>examples/E19_llm_output_parsing_example.py</code>.</p> <p>It demonstrates how to: - Use <code>genie.llm.parse_output()</code> to convert LLM text responses into structured data. - Parse LLM output into Python dictionaries (from JSON). - Parse LLM output directly into Pydantic model instances. - Configure and use <code>JSONOutputParserPlugin</code> and <code>PydanticOutputParserPlugin</code>.</p> <pre><code># Full code from examples/E19_llm_output_parsing_example.py\n</code></pre> <p>Key Takeaways: - <code>genie.llm.parse_output()</code> simplifies extracting structured data from LLMs. - Provide a Pydantic model class to the <code>schema</code> argument for direct model instantiation.</p>"},{"location":"tutorials/E20_token_usage_example/","title":"Tutorial: Token Usage Tracking Example","text":"<p>This tutorial corresponds to the example file <code>examples/E20_token_usage_example.py</code>.</p> <p>It demonstrates how to: - Enable and configure a <code>TokenUsageRecorderPlugin</code> (e.g., <code>InMemoryTokenUsageRecorderPlugin</code>). - Observe automatic token usage recording from <code>genie.llm</code> calls. - Retrieve and interpret token usage summaries using <code>genie.usage.get_summary()</code>.</p> <pre><code># Full code from examples/E20_token_usage_example.py\n</code></pre> <p>Key Takeaways: - Token usage is automatically tracked for supported LLM provider operations. - <code>genie.usage.get_summary()</code> provides insights into token consumption.</p>"},{"location":"tutorials/E21_guardrails_example/","title":"Tutorial: Guardrails Example","text":"<p>This tutorial corresponds to the example file <code>examples/E21_guardrails_example.py</code>.</p> <p>It demonstrates how to: - Configure and enable <code>GuardrailPlugin</code>s (e.g., <code>KeywordBlocklistGuardrailPlugin</code>). - See how input guardrails can block or warn on data sent to LLMs or command processors. - See how output guardrails can block or warn on data received from LLMs or tools. - Understand the <code>GuardrailViolation</code> structure and <code>action_on_match</code> behavior.</p> <pre><code># Full code from examples/E21_guardrails_example.py\n</code></pre> <p>Key Takeaways: - Guardrails are implicitly integrated into core <code>Genie</code> operations. - The <code>KeywordBlocklistGuardrailPlugin</code> offers a simple way to filter content. - Guardrails can be configured to either \"block\" or \"warn\" upon detecting a violation.</p>"}]}