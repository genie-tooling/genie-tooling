{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Genie Tooling","text":"<p>Genie Tooling is a hyper-pluggable Python middleware designed to empower developers in building sophisticated Agentic AI and LLM-powered applications. It provides a modular, async-first framework that emphasizes clear interfaces and interchangeable components, allowing for rapid iteration and customization of agent capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simplified Development with the <code>Genie</code> Facade: The primary entry point for most applications, offering a high-level API for common agentic tasks like LLM interaction, Retrieval Augmented Generation (RAG), tool execution, and natural language command processing.</li> <li>Hyper-Pluggable Architecture: Almost every piece of functionality\u2014from LLM providers and tools to data loaders and caching mechanisms\u2014is a plugin. This allows you to easily swap, extend, or create custom components.</li> <li>Simplified Configuration: Utilize <code>FeatureSettings</code> within <code>MiddlewareConfig</code> for quick setup of common features, with the flexibility for detailed overrides.</li> <li>Async First: Built with <code>asyncio</code> for high-performance, I/O-bound operations common in AI applications.</li> <li><code>@tool</code> Decorator: Effortlessly convert your existing Python functions into Genie-compatible tools with automatic metadata and schema generation. By default, these tools are automatically enabled upon registration, but it is strongly recommended to explicitly enable them via <code>tool_configurations</code> in production for enhanced security.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running quickly with the <code>Genie</code> facade:</p> <pre><code>import asyncio\nimport logging\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    logging.basicConfig(level=logging.INFO)\n\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            llm=\"ollama\", # Default: ollama, ensure it's running\n            llm_ollama_model_name=\"mistral:latest\",\n            command_processor=\"llm_assisted\",\n            tool_lookup=\"embedding\" \n        ),\n        # Explicitly enable the built-in calculator tool.\n        # If auto_enable_registered_tools=False, any custom @tool\n        # would also need to be listed here after registration.\n        tool_configurations={\n            \"calculator_tool\": {} \n        }\n    )\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    # LLM Chat\n    chat_response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Tell me about Genie Tooling.\"}])\n    print(f\"LLM: {chat_response['message']['content']}\")\n\n    # Command Execution (e.g., calculator tool is built-in)\n    cmd_result = await genie.run_command(\"What is 5 times 12?\")\n    print(f\"Command Result: {cmd_result.get('tool_result')}\")\n\n    await genie.close()\n    print(\"Genie torn down.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li>User Guide: Learn how to install, configure, and use Genie Tooling for your projects.<ul> <li>Installation</li> <li>Configuration &amp; Simplified Configuration</li> <li>Using LLM Providers</li> <li>Using Tools</li> <li>Using RAG</li> <li>Using Command Processors</li> <li>Tool Lookup</li> <li>Logging</li> <li>Prompt Management</li> <li>Conversation State</li> <li>Observability &amp; Tracing</li> <li>Human-in-the-Loop (HITL)</li> <li>Token Usage Tracking</li> <li>Guardrails</li> <li>Distributed Tasks</li> </ul> </li> <li>Developer Guide: Understand the plugin architecture and learn how to create your own custom plugins and tools.<ul> <li>Plugin Architecture</li> <li>Creating Tool Plugins</li> <li>Creating RAG Plugins</li> <li>Creating Other Plugins</li> </ul> </li> <li>API Reference: Detailed reference for all public modules and classes.</li> <li>Tutorials &amp; Examples: Step-by-step guides and practical examples to get you started.</li> </ul> <p>We encourage you to explore the documentation and examples to unlock the full potential of Genie Tooling!</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#010-yyyy-mm-dd","title":"[0.1.0] - YYYY-MM-DD","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release of Genie Tooling.</li> <li>Core components: PluginManager, Genie Facade, ToolManager, RAGManager.</li> <li>Support for various plugins: LLM Providers (Ollama, OpenAI, Gemini, Llama.cpp), Tools, RAG components, etc.</li> <li>Agent implementations: ReActAgent, PlanAndExecuteAgent.</li> <li>New features including Hybrid Search, Pyvider logging, and Distributed Task Queues.</li> </ul>"},{"location":"LLM_code_assist_instructions/LLM_api_guide/","title":"LLM api guide","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#docsllm_code_assist_instructionsllm_api_guidemd","title":"docs/LLM_code_assist_instructions/LLM_api_guide.md","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#llm_guide_metadata_v05","title":"LLM_GUIDE_METADATA_V0.5","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#this-file-is-a-hyper-compressed-semantic-representation-of-genie-tooling-for-llm-consumption","title":"THIS FILE IS A HYPER-COMPRESSED SEMANTIC REPRESENTATION OF GENIE TOOLING FOR LLM CONSUMPTION.","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#llm-if-you-update-the-genie-tooling-project-code-classes-methods-params-core-logic","title":"LLM: IF YOU UPDATE THE GENIE TOOLING PROJECT CODE (CLASSES, METHODS, PARAMS, CORE LOGIC),","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#you-must-update-this-file-to-reflect-those-changes-accurately","title":"YOU MUST UPDATE THIS FILE TO REFLECT THOSE CHANGES ACCURATELY.","text":""},{"location":"LLM_code_assist_instructions/LLM_api_guide/#maintain-the-compressed-format-human-readability-is-not-a-priority","title":"MAINTAIN THE COMPRESSED FORMAT. HUMAN READABILITY IS NOT A PRIORITY.","text":"<p>Project: Genie Tooling. Python Agentic/LLM Middleware. Async. MIT.</p> <p>Facade: <code>genie_tooling.genie.Genie</code> *   <code>init</code>: <code>Genie.create(cfg:MiddlewareConfig, kp_inst?:KeyProvider)</code> *   <code>llm</code>: <code>LLMInterface</code>     *   <code>chat(msgs:List[ChatMessage], prov_id?:str, stream?:bool, **kw) -&gt; LLMChatResp|AsyncIter[LLMChatChunk]</code> (kwargs can include <code>output_schema</code> for GBNF with Llama.cpp)     *   <code>generate(prompt:str, prov_id?:str, stream?:bool, **kw) -&gt; LLMCompResp|AsyncIter[LLMCompChunk]</code> (kwargs can include <code>output_schema</code> for GBNF with Llama.cpp)     *   <code>parse_output(resp:LLMChatResp|LLMCompResp, parser_id?:str, schema?:Any) -&gt; ParsedOutput</code> *   <code>rag</code>: <code>RAGInterface</code>     *   <code>index_directory(path:str, collection_name?:str, loader_id?:str, splitter_id?:str, embedder_id?:str, vector_store_id?:str, loader_config?:Dict, splitter_config?:Dict, embedder_config?:Dict, vector_store_config?:Dict, **kw) -&gt; Dict[str,Any]</code>     *   <code>index_web_page(url:str, collection_name?:str, loader_id?:str, splitter_id?:str, embedder_id?:str, vector_store_id?:str, loader_config?:Dict, splitter_config?:Dict, embedder_config?:Dict, vector_store_config?:Dict, **kw) -&gt; Dict[str,Any]</code>     *   <code>search(query:str, collection_name?:str, top_k?:int, retriever_id?:str, retriever_config?:Dict, **kw) -&gt; List[RetrievedChunk]</code> *   <code>tools</code>: (Methods on <code>Genie</code> instance)     *   <code>execute_tool(tool_identifier:str, **params:Any) -&gt; Any</code> (Tool must be enabled)     *   <code>run_command(command:str, processor_id?:str, conversation_history?:List[ChatMessage]) -&gt; CmdProcResp</code> (Integrates HITL; tools used must be enabled) *   <code>tool_reg</code>: (Methods on <code>Genie</code> instance)     *   <code>@genie_tooling.tool</code> (Decorator for functions. Auto-generates metadata).     *   <code>await genie.register_tool_functions(functions:List[Callable])</code> (Registers decorated functions. Enablement depends on <code>auto_enable_registered_tools</code> flag). *   <code>prompts</code>: <code>PromptInterface</code>     *   <code>get_prompt_template_content(name:str, version?:str, registry_id?:str) -&gt; str?</code>     *   <code>render_prompt(name:str, data:PromptData, version?:str, registry_id?:str, template_engine_id?:str) -&gt; FormattedPrompt?</code>     *   <code>render_chat_prompt(name:str, data:PromptData, version?:str, registry_id?:str, template_engine_id?:str) -&gt; List[ChatMessage]?</code>     *   <code>list_templates(registry_id?:str) -&gt; List[PromptIdentifier]</code> *   <code>conversation</code>: <code>ConversationInterface</code>     *   <code>load_state(session_id:str, provider_id?:str) -&gt; ConversationState?</code>     *   <code>save_state(state:ConversationState, provider_id?:str)</code>     *   <code>add_message(session_id:str, message:ChatMessage, provider_id?:str)</code>     *   <code>delete_state(session_id:str, provider_id?:str) -&gt; bool</code> *   <code>observability</code>: <code>ObservabilityInterface</code>     *   <code>trace_event(event_name:str, data:Dict, component?:str, correlation_id?:str)</code> *   <code>human_in_loop</code>: <code>HITLInterface</code>     *   <code>request_approval(request:ApprovalRequest, approver_id?:str) -&gt; ApprovalResponse</code> *   <code>usage</code>: <code>UsageTrackingInterface</code>     *   <code>record_usage(record:TokenUsageRecord)</code>     *   <code>get_summary(recorder_id?:str, filter_criteria?:Dict) -&gt; Dict</code> *   <code>task_queue</code>: <code>TaskQueueInterface</code>     *   <code>submit_task(task_name:str, args?:Tuple, kwargs?:Dict, queue_id?:str, task_options?:Dict) -&gt; str?</code>     *   <code>get_task_status(task_id:str, queue_id?:str) -&gt; TaskStatus?</code>     *   <code>get_task_result(task_id:str, queue_id?:str, timeout_seconds?:float) -&gt; Any?</code>     *   <code>revoke_task(task_id:str, queue_id?:str, terminate?:bool) -&gt; bool</code> *   <code>teardown</code>: <code>await genie.close()</code> (Cleans up all managers and plugins)</p> <p>Agent Classes (in <code>genie_tooling.agents</code>): *   <code>BaseAgent(genie:Genie, agent_cfg?:Dict)</code>     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Abstract method) *   <code>ReActAgent(BaseAgent)</code>     *   <code>cfg</code>: <code>max_iterations:int</code> (Def:7), <code>system_prompt_id:str</code> (Def: <code>react_agent_system_prompt_v1</code>), <code>llm_provider_id:str?</code>, <code>tool_formatter_id:str</code> (Def: <code>compact_text_formatter_plugin_v1</code>), <code>stop_sequences:List[str]</code> (Def: <code>[\"Observation:\"]</code>), <code>llm_retry_attempts:int</code> (Def:1), <code>llm_retry_delay_seconds:float</code> (Def:2.0)     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Implements ReAct loop: Thought, Action, Observation) *   <code>PlanAndExecuteAgent(BaseAgent)</code>     *   <code>cfg</code>: <code>planner_system_prompt_id:str</code> (Def: <code>plan_and_execute_planner_prompt_v1</code>), <code>planner_llm_provider_id:str?</code>, <code>tool_formatter_id:str</code> (Def: <code>compact_text_formatter_plugin_v1</code>), <code>max_plan_retries:int</code> (Def:1), <code>max_step_retries:int</code> (Def:0), <code>replan_on_step_failure:bool</code> (Def:False)     *   <code>async run(goal:str, **kw) -&gt; AgentOutput</code> (Implements Plan-then-Execute loop)</p> <p>Config: <code>genie_tooling.config.models.MiddlewareConfig</code> (<code>MWCfg</code>) *   <code>auto_enable_registered_tools: bool</code> (Default: <code>True</code>. IMPORTANT: Set to <code>False</code> for production security.) *   <code>features: FeatureSettings</code> -&gt; <code>ConfigResolver</code> (<code>CfgResolver</code>) populates <code>MWCfg</code>.     *   <code>llm: Literal[\"ollama\", \"openai\", \"gemini\", \"llama_cpp\", \"llama_cpp_internal\", \"none\"]</code>     *   <code>llm_ollama_model_name: str?</code>     *   <code>llm_openai_model_name: str?</code>     *   <code>llm_gemini_model_name: str?</code>     *   <code>llm_llama_cpp_model_name: str?</code>     *   <code>llm_llama_cpp_base_url: str?</code>     *   <code>llm_llama_cpp_api_key_name: str?</code>     *   <code>llm_llama_cpp_internal_model_path: str?</code>     *   <code>llm_llama_cpp_internal_n_gpu_layers: int</code> (Def: 0)     *   <code>llm_llama_cpp_internal_n_ctx: int</code> (Def: 2048)     *   <code>llm_llama_cpp_internal_chat_format: str?</code>     *   <code>llm_llama_cpp_internal_model_name_for_logging: str?</code>     *   <code>cache: Literal[\"in-memory\", \"redis\", \"none\"]</code>     *   <code>cache_redis_url: str?</code>     *   <code>rag_embedder: Literal[\"sentence_transformer\", \"openai\", \"none\"]</code>     *   <code>rag_embedder_st_model_name: str?</code>     *   <code>rag_vector_store: Literal[\"faiss\", \"chroma\", \"qdrant\", \"none\"]</code>     *   <code>rag_vector_store_chroma_path: str?</code>     *   <code>rag_vector_store_chroma_collection_name: str?</code>     *   <code>rag_vector_store_qdrant_url: str?</code>     *   <code>rag_vector_store_qdrant_path: str?</code>     *   <code>rag_vector_store_qdrant_api_key_name: str?</code>     *   <code>rag_vector_store_qdrant_collection_name: str?</code>     *   <code>rag_vector_store_qdrant_embedding_dim: int?</code>     *   <code>tool_lookup: Literal[\"embedding\", \"keyword\", \"hybrid\", \"none\"]</code>     *   <code>tool_lookup_formatter_id_alias: str?</code>     *   <code>tool_lookup_chroma_path: str?</code>     *   <code>tool_lookup_chroma_collection_name: str?</code>     *   <code>tool_lookup_embedder_id_alias: str?</code>     *   <code>command_processor: Literal[\"llm_assisted\", \"simple_keyword\", \"none\"]</code>     *   <code>command_processor_formatter_id_alias: str?</code>     *   <code>logging_adapter: Literal[\"default_log_adapter\", \"pyvider_log_adapter\", \"none\"]</code>     *   <code>logging_pyvider_service_name: str?</code>     *   <code>observability_tracer: Literal[\"console_tracer\", \"otel_tracer\", \"none\"]</code>     *   <code>observability_otel_endpoint: str?</code>     *   <code>hitl_approver: Literal[\"cli_hitl_approver\", \"none\"]</code>     *   <code>token_usage_recorder: Literal[\"in_memory_token_recorder\", \"otel_metrics_recorder\", \"none\"]</code>     *   <code>input_guardrails: List[str]</code>     *   <code>output_guardrails: List[str]</code>     *   <code>tool_usage_guardrails: List[str]</code>     *   <code>prompt_registry: Literal[\"file_system_prompt_registry\", \"none\"]</code>     *   <code>prompt_template_engine: Literal[\"basic_string_formatter\", \"jinja2_chat_formatter\", \"none\"]</code>     *   <code>conversation_state_provider: Literal[\"in_memory_convo_provider\", \"redis_convo_provider\", \"none\"]</code>     *   <code>default_llm_output_parser: Literal[\"json_output_parser\", \"pydantic_output_parser\", \"none\"]</code>     *   <code>task_queue: Literal[\"celery\", \"rq\", \"none\"]</code>     *   <code>task_queue_celery_broker_url: str?</code>     *   <code>task_queue_celery_backend_url: str?</code> *   <code>tool_configurations: Dict[str_id_or_alias, Dict[str, Any]]</code> (Provides tool-specific config. If <code>auto_enable_registered_tools=False</code>, this dict also serves as the explicit enablement list.) *   <code>ConfigResolver</code> (<code>genie_tooling.config.resolver.py</code>): <code>features</code> + aliases -&gt; canonical IDs &amp; cfgs. <code>PLUGIN_ID_ALIASES</code> dict. *   <code>key_provider_id: str?</code> Def: <code>env_keys</code> if no <code>key_provider_instance</code>. *   <code>key_provider_instance: KeyProvider?</code> -&gt; Passed to <code>Genie.create()</code>. *   <code>*_configurations: Dict[str_id_or_alias, Dict[str, Any]]</code> (e.g., <code>llm_provider_configurations</code>, <code>log_adapter_configurations</code>). *   <code>plugin_dev_dirs: List[str]</code>. *   <code>default_log_adapter_id: str?</code></p> <p>Plugins: <code>PluginManager</code>. IDs/paths: <code>pyproject.toml</code> -&gt; <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code>. Aliases: <code>genie_tooling.config.resolver.PLUGIN_ID_ALIASES</code>.</p> <p>Key Plugins (ID | Alias | Cfg/Notes): *   <code>KeyProv</code>: <code>environment_key_provider_v1</code>|<code>env_keys</code>. *   <code>LLMProv</code>:     *   <code>ollama_llm_provider_v1</code>|<code>ollama</code>. Cfg: <code>base_url</code>, <code>model_name</code>, <code>request_timeout_seconds</code>.     *   <code>openai_llm_provider_v1</code>|<code>openai</code>. Cfg: <code>model_name</code>, <code>api_key_name</code>, <code>openai_api_base</code>, <code>openai_organization</code>. Needs KP.     *   <code>gemini_llm_provider_v1</code>|<code>gemini</code>. Cfg: <code>model_name</code>, <code>api_key_name</code>, <code>system_instruction</code>, <code>safety_settings</code>. Needs KP.     *   <code>llama_cpp_llm_provider_v1</code>|<code>llama_cpp</code>. Cfg: <code>base_url</code>, <code>model_name</code>, <code>request_timeout_seconds</code>, <code>api_key_name</code>. Needs KP if <code>api_key_name</code> set.     *   <code>llama_cpp_internal_llm_provider_v1</code>|<code>llama_cpp_internal</code>. Cfg: <code>model_path</code>, <code>n_gpu_layers</code>, <code>n_ctx</code>, <code>chat_format</code>, <code>model_name_for_logging</code>, etc. Does not use KP for keys. *   <code>CmdProc</code>:     *   <code>simple_keyword_processor_v1</code>|<code>simple_keyword_cmd_proc</code>. Cfg: <code>keyword_map</code>, <code>keyword_priority</code>.     *   <code>llm_assisted_tool_selection_processor_v1</code>|<code>llm_assisted_cmd_proc</code>. Cfg: <code>llm_provider_id</code>, <code>tool_formatter_id</code>, <code>tool_lookup_top_k</code>, <code>system_prompt_template</code>, <code>max_llm_retries</code>. *   <code>Tools</code>: (Examples: <code>calculator_tool</code>, <code>sandboxed_fs_tool_v1</code>, etc. Enablement controlled by <code>auto_enable_registered_tools</code> flag and <code>tool_configurations</code> dict.) *   <code>LogAdapter</code>:     *   <code>default_log_adapter_v1</code>|<code>default_log_adapter</code>. Cfg: <code>log_level</code>, <code>library_logger_name</code>, <code>redactor_plugin_id</code>, <code>redactor_config</code>, <code>enable_schema_redaction</code>, <code>enable_key_name_redaction</code>.     *   <code>pyvider_telemetry_log_adapter_v1</code>|<code>pyvider_log_adapter</code>. Cfg: <code>service_name</code>, <code>default_level</code>, <code>module_levels</code>, <code>console_formatter</code>, emoji settings, <code>redactor_plugin_id</code>, etc. *   <code>Observability</code>:     *   <code>console_tracer_plugin_v1</code>|<code>console_tracer</code>. Cfg: <code>log_adapter_instance_for_console_tracer</code> (or similar for ID/PM), <code>log_level</code> (for its own direct logs if LogAdapter fails).     *   <code>otel_tracer_plugin_v1</code>|<code>otel_tracer</code>. Cfg: <code>otel_service_name</code>, <code>exporter_type</code> (console, otlp_http, otlp_grpc), endpoints, headers, etc. *   <code>TokenUsage</code>:     *   <code>in_memory_token_usage_recorder_v1</code>|<code>in_memory_token_recorder</code>.     *   <code>otel_metrics_token_recorder_v1</code>|<code>otel_metrics_recorder</code>. Emits OTel metrics. *   <code>ConversationStateProv</code>:     *   <code>in_memory_conversation_state_v1</code>|<code>in_memory_convo_provider</code>. Path: <code>genie_tooling.conversation.impl.in_memory_state_provider:InMemoryStateProviderPlugin</code>     *   <code>redis_conversation_state_v1</code>|<code>redis_convo_provider</code>. Path: <code>genie_tooling.conversation.impl.redis_state_provider:RedisStateProviderPlugin</code> *   <code>TaskQueues</code>:     *   <code>celery_task_queue_v1</code>|<code>celery_task_queue</code>. Cfg: <code>celery_app_name</code>, <code>celery_broker_url</code>, <code>celery_backend_url</code>.     *   <code>redis_queue_task_plugin_v1</code>|<code>rq_task_queue</code>. Cfg: <code>redis_url</code>, <code>default_queue_name</code>. *   (Other plugin categories like DefFormatters, RAG, ToolLookupProv, CodeExec, CacheProv, HITL, Guardrails, Prompts, LLMOutputParsers, InvocationStrategies remain structurally similar but their instances are loaded based on configuration.)</p> <p>Types: *   <code>ChatMessage</code>: <code>{role:Literal[\"system\"|\"user\"|\"assistant\"|\"tool\"], content?:str, tool_calls?:List[ToolCall], tool_call_id?:str, name?:str}</code> *   <code>ToolCall</code>: <code>{id:str, type:Literal[\"function\"], function:{name:str, arguments:str_json}}</code> *   <code>LLMCompResp</code>: <code>{text:str, finish_reason?:str, usage?:LLMUsageInfo, raw_response:Any}</code> *   <code>LLMChatResp</code>: <code>{message:ChatMessage, finish_reason?:str, usage?:LLMUsageInfo, raw_response:Any}</code> *   <code>LLMUsageInfo</code>: <code>{prompt_tokens?:int, completion_tokens?:int, total_tokens?:int}</code> *   <code>LLMCompChunk</code>: <code>{text_delta?:str, finish_reason?:str, usage_delta?:LLMUsageInfo, raw_chunk:Any}</code> *   <code>LLMChatChunkDeltaMsg</code>: <code>{role?:\"assistant\", content?:str, tool_calls?:List[ToolCall]}</code> *   <code>LLMChatChunk</code>: <code>{message_delta?:LLMChatChunkDeltaMsg, finish_reason?:str, usage_delta?:LLMUsageInfo, raw_chunk:Any}</code> *   <code>CmdProcResp</code>: <code>{chosen_tool_id?:str, extracted_params?:Dict, llm_thought_process?:str, error?:str, raw_response?:Any}</code> *   <code>RetrievedChunk</code>: <code>{content:str, metadata:Dict, id?:str, score:float, rank?:int}</code> *   <code>CodeExecRes</code>: <code>(stdout:str, stderr:str, result?:Any, error?:str, exec_time_ms:float)</code> (NamedTuple) *   <code>PromptData</code>: <code>Dict[str,Any]</code> *   <code>FormattedPrompt</code>: <code>Union[str, List[ChatMessage]]</code> *   <code>PromptIdentifier</code>: <code>{name:str, version?:str, description?:str}</code> *   <code>ConversationState</code>: <code>{session_id:str, history:List[ChatMessage], metadata?:Dict}</code> *   <code>TraceEvent</code>: <code>{event_name:str, data:Dict, timestamp:float, component?:str, correlation_id?:str}</code> *   <code>ApprovalRequest</code>: <code>{request_id:str, prompt:str, data_to_approve:Dict, context?:Dict, timeout_seconds?:int}</code> *   <code>ApprovalResponse</code>: <code>{request_id:str, status:ApprovalStatus, approver_id?:str, reason?:str, timestamp?:float}</code> *   <code>ApprovalStatus: Literal[\"pending\"|\"approved\"|\"denied\"|\"timeout\"|\"error\"]</code> *   <code>TokenUsageRecord</code>: <code>{provider_id:str, model_name:str, prompt_tokens?:int, completion_tokens?:int, total_tokens?:int, timestamp:float, call_type?:str, user_id?:str, session_id?:str, custom_tags?:dict}</code> *   <code>GuardrailViolation</code>: <code>{action:Literal[\"allow\"|\"block\"|\"warn\"], reason?:str, guardrail_id?:str, details?:Dict}</code> *   <code>ParsedOutput</code>: <code>Any</code> *   <code>AgentOutput</code>: <code>{status:Literal[\"success\"|\"error\"|\"max_iterations_reached\"|\"user_stopped\"], output:Any, history?:List[Any], plan?:List[PlannedStep]}</code> *   <code>PlannedStep</code>: <code>{step_number:int, tool_id:str, params:Dict, reasoning?:str}</code> *   <code>ReActObservation</code>: <code>{thought:str, action:str, observation:str}</code> *   <code>TaskStatus: Literal[\"pending\", \"running\", \"success\", \"failure\", \"revoked\", \"unknown\"]</code></p>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/","title":"LLM refactor guide","text":""},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#genie-tooling-refactoring-migration-guide-for-llms","title":"Genie Tooling Refactoring &amp; Migration Guide for LLMs","text":"<p>Primary LLM Directive: Your goal is to analyze an existing Python codebase and systematically refactor its components to utilize the <code>genie-tooling</code> middleware. The primary interaction point will be the <code>genie_tooling.genie.Genie</code> facade. Prioritize modularity, configurability, and leveraging Genie's built-in plugin ecosystem. Key Change: Tools are now explicitly enabled via <code>MiddlewareConfig.tool_configurations</code>.</p> <p>Core Reference Documents for LLM: 1.  <code>docs/LLM_code_assist_instructions/LLM_api_guide.md</code>: This is your primary technical schema for Genie Tooling's API, plugin types, configuration structure, and core concepts. Internalize this deeply, noting the change in tool enablement and new features like the internal Llama.cpp provider and distributed task queues. 2.  <code>README.md</code>: For high-level vision, core concepts (Genie facade, plugins, <code>@tool</code>, explicit tool enablement), and quick start examples. 3.  <code>pyproject.toml</code> (<code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> section): Canonical source for plugin IDs and their Python module paths. 4.  <code>src/genie_tooling/config/resolver.py</code> (<code>PLUGIN_ID_ALIASES</code> dict): Mapping of user-friendly aliases to canonical plugin IDs. 5.  <code>examples/</code> directory: Practical usage patterns and configuration examples, now reflecting explicit tool enablement and new features. 6.  <code>docs/guides/</code>: Detailed human-readable guides for specific features (e.g., RAG, LLM Providers, Configuration, Using Tools, Distributed Tasks, Observability).</p>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-0-deep-ingestion-semantic-understanding-of-genie-tooling","title":"Phase 0: Deep Ingestion &amp; Semantic Understanding of Genie Tooling","text":"<p>LLM Instructions:</p> <ol> <li>Master <code>docs/LLM_code_assist_instructions/LLM_api_guide.md</code>: This document is specifically structured for your consumption. Treat it as the ground truth for Genie's API, available plugin types, configuration parameters (via <code>MiddlewareConfig</code> and <code>FeatureSettings</code>), and core data structures (e.g., <code>ChatMessage</code>, <code>ToolCall</code>, <code>AgentOutput</code>). Pay special attention to the <code>tool_configurations</code> field in <code>MiddlewareConfig</code> as the method for enabling tools, and note the new <code>llama_cpp_internal</code> LLM provider and <code>task_queue</code> interface.</li> <li>Internalize Core Concepts (<code>README.md</code>):<ul> <li><code>Genie</code> Facade: Understand its role as the central interaction point.</li> <li>Plugin Architecture: Grasp that most functionalities are swappable plugins.</li> <li>Explicit Tool Enablement: Recognize that tools are only loaded if their ID is a key in <code>MiddlewareConfig.tool_configurations</code>.</li> <li>Managers: Note their role in orchestrating plugin types (though direct interaction is rare when using the facade).</li> <li><code>@tool</code> Decorator: Recognize its utility for quickly turning Python functions into tools. These decorated tools also need to be enabled via <code>tool_configurations</code> after registration.</li> <li>Configuration (<code>MiddlewareConfig</code>, <code>FeatureSettings</code>, <code>ConfigResolver</code>): Understand the layered configuration approach.</li> </ul> </li> <li>Plugin Identification &amp; Aliasing:<ul> <li>Correlate plugin IDs from <code>pyproject.toml</code> with aliases in <code>PLUGIN_ID_ALIASES</code> from <code>resolver.py</code>.</li> </ul> </li> <li>Facade &amp; Interface Study:<ul> <li>Analyze <code>src/genie_tooling/genie.py</code> to see how the <code>Genie</code> class initializes and exposes its sub-interfaces, including the new <code>TaskQueueInterface</code>.</li> <li>Thoroughly review <code>src/genie_tooling/interfaces.py</code>.</li> </ul> </li> <li>Practical Application Patterns:<ul> <li>Examine all files in the <code>examples/</code> directory. Pay close attention to how <code>MiddlewareConfig</code> (especially <code>tool_configurations</code>) and <code>FeatureSettings</code> are used, including examples for new features like <code>E17_observability_tracing_example.py</code>, <code>E20_token_usage_example.py</code>, and <code>E22_distributed_task_example.py</code>.</li> </ul> </li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-1-target-project-analysis-genie-capability-mapping","title":"Phase 1: Target Project Analysis &amp; Genie Capability Mapping","text":"<p>LLM Instructions:</p> <ol> <li>Deconstruct Target Project: (Same as before)</li> <li>Create a Refactoring Map: (Same as before, but when mapping to a Genie Tool, note that it will need to be added to <code>tool_configurations</code>).</li> <li>Assess API Key Handling: (Same as before)</li> <li>Identify Custom Plugin Candidates: (Same as before)</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-2-core-genie-integration-configuration-setup","title":"Phase 2: Core Genie Integration &amp; Configuration Setup","text":"<p>LLM Instructions:</p> <ol> <li>Add Dependency: (Same as before)</li> <li>Design <code>MiddlewareConfig</code>:<ul> <li>Create a central configuration point.</li> <li>Instantiate <code>genie_tooling.config.models.MiddlewareConfig</code>.</li> <li>Prioritize using <code>genie_tooling.config.features.FeatureSettings</code>.</li> <li>Crucially, populate <code>app_config.tool_configurations</code> with entries for every tool that needs to be active. For tools without specific settings, an empty dictionary is sufficient (e.g., <code>app_config.tool_configurations = {\"calculator_tool\": {}}</code>).<ul> <li>Example:     <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_features = FeatureSettings(\n    llm=\"ollama\", \n    # ... other features like task_queue=\"celery\" ...\n)\napp_config = MiddlewareConfig(\n    features=app_features,\n    tool_configurations={\n        \"calculator_tool\": {}, # Enable calculator\n        \"my_existing_api_tool_id\": {\"api_base_url\": \"https://service.com/api\"}, # Enable &amp; configure\n        \"another_simple_tool_id\": {} \n    }\n)\n</code></pre></li> </ul> </li> <li>For functionalities not covered by <code>FeatureSettings</code> or requiring specific overrides, populate the relevant <code>*_configurations</code> dictionaries (e.g., <code>llm_provider_configurations</code>, <code>distributed_task_queue_configurations</code>).</li> <li>If custom plugins reside in project-specific directories, add these paths to <code>app_config.plugin_dev_dirs</code>.</li> </ul> </li> <li>Implement/Configure <code>KeyProvider</code>: (Same as before)</li> <li>Instantiate <code>Genie</code> Facade: (Same as before)</li> <li>Integrate Teardown: (Same as before)</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-3-iterative-refactoring-of-mapped-components","title":"Phase 3: Iterative Refactoring of Mapped Components","text":"<p>LLM Instructions: Systematically replace existing functionalities with their Genie Tooling equivalents.</p> <ol> <li>LLM Interactions (<code>genie.llm</code>): (Same as before. Consider <code>llama_cpp_internal</code> for local execution.)</li> <li>Tool Definition &amp; Execution (<code>@tool</code>, <code>genie.execute_tool</code>, <code>genie.run_command</code>):<ul> <li>Refactor Functions to Tools: Apply <code>@genie_tooling.tool</code>.</li> <li>Register Tools: Call <code>await genie.register_tool_functions([...])</code>.</li> <li>Enable Registered Tools: Ensure the identifiers of these registered tools are added as keys to <code>MiddlewareConfig.tool_configurations</code> (e.g., <code>{\"my_decorated_function_name\": {}}</code>).</li> <li>Replace Direct Calls: Change to <code>await genie.execute_tool(\"tool_name_as_string\", ...)</code>.</li> <li>Refactor Command Parsing: Replace with <code>await genie.run_command(user_query_string)</code>. Ensure any tools the command processor might select are enabled in <code>tool_configurations</code>.</li> </ul> </li> <li>Command Processing (<code>genie.run_command</code>):<ul> <li>(Same as before, but reiterate that any tools the processor might select must be enabled in <code>tool_configurations</code>).</li> </ul> </li> <li>RAG Pipeline (<code>genie.rag</code>): (Same as before)</li> <li>Prompt Management (<code>genie.prompts</code>): (Same as before)</li> <li>Conversation State (<code>genie.conversation</code>): (Same as before)</li> <li>Observability (<code>genie.observability</code>): (Same as before, noting OpenTelemetry support)</li> <li>Human-in-the-Loop (<code>genie.human_in_loop</code>): (Same as before)</li> <li>Token Usage Tracking (<code>genie.usage</code>): (Same as before, noting OpenTelemetry metrics support)</li> <li>Guardrails: (Same as before)</li> <li>LLM Output Parsing (<code>genie.llm.parse_output</code>): (Same as before)</li> <li>Agentic Loops (<code>genie_tooling.agents</code>):<ul> <li>(Same as before, ensuring any tools used by the agent are enabled in <code>tool_configurations</code>).</li> </ul> </li> <li>Distributed Task Offloading (<code>genie.task_queue</code>):<ul> <li>Replace direct calls to Celery/RQ client libraries with <code>await genie.task_queue.submit_task(...)</code>, <code>await genie.task_queue.get_task_status(...)</code>, etc.</li> <li>If the remote task involves executing a Genie tool, that tool must be enabled in the worker's Genie configuration.</li> <li>Consider if the <code>DistributedTaskInvocationStrategy</code> can simplify tool offloading, if applicable and configured.</li> </ul> </li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-4-testing-validation-and-refinement","title":"Phase 4: Testing, Validation, and Refinement","text":"<p>LLM Instructions:</p> <ol> <li>Unit Tests: (Same as before)</li> <li>Integration Tests:<ul> <li>Ensure <code>MiddlewareConfig</code> in tests correctly enables all necessary tools via <code>tool_configurations</code>.</li> <li>Test new features like task queue interactions and internal Llama.cpp provider usage.</li> </ul> </li> <li>Behavioral Verification: (Same as before)</li> <li>Configuration Robustness: (Same as before)</li> <li>Observability Review: Review traces and metrics generated by the framework to ensure they are correct and provide meaningful insight.</li> </ol>"},{"location":"LLM_code_assist_instructions/LLM_refactor_guide/#phase-5-finalization-documentation-code-cleanup","title":"Phase 5: Finalization - Documentation &amp; Code Cleanup","text":"<p>LLM Instructions:</p> <ol> <li>Update Project Documentation:<ul> <li>Clearly document that tools must be enabled via <code>MiddlewareConfig.tool_configurations</code>.</li> <li>Update guides for new features: Llama.cpp internal provider, distributed task queues (Celery &amp; RQ), OpenTelemetry tracing and metrics.</li> </ul> </li> <li>Code Cleanup: (Same as before)</li> <li>Configuration Review:<ul> <li>Verify that <code>tool_configurations</code> correctly lists all active tools.</li> </ul> </li> <li>Update <code>LLM_api_guide.md</code> (If Genie Tooling Itself Was Modified):<ul> <li>CRITICAL LLM SELF-REFLECTION: (Same as before. Ensure the API guide reflects the explicit tool enablement via <code>tool_configurations</code> and all new features like <code>llama_cpp_internal</code> and <code>task_queue</code> interface.)</li> </ul> </li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>This section contains the auto-generated API documentation for Genie Tooling.</p> <p>Explore the modules and classes below for detailed information.</p>"},{"location":"api/#core","title":"Core","text":"<ul> <li>Genie Facade &amp; Interfaces</li> <li>Decorators (@tool)</li> <li>Core Types </li> <li>Plugin Manager</li> </ul>"},{"location":"api/#configuration","title":"Configuration","text":"<ul> <li>MiddlewareConfig</li> <li>FeatureSettings</li> </ul>"},{"location":"api/#plugins-protocols-implementations","title":"Plugins (Protocols &amp; Implementations)","text":"<p>This section can be expanded by <code>mkdocstrings</code> or by manually linking to specific plugin protocol documentation pages. Key plugin categories include: *   Tools *   LLM Providers *   Command Processors *   RAG Components (Loaders, Splitters, Embedders, Vector Stores, Retrievers) *   Tool Lookup Providers *   Definition Formatters *   Caching Providers *   Key Providers *   Invocation Strategies *   Error Handling *   Logging &amp; Redaction *   Code Executors *   Prompt System Plugins (Registries, Templates) *   Conversation State Providers *   Observability Tracers *   HITL Approvers *   Token Usage Recorders *   Guardrail Plugins *   LLM Output Parsers</p>"},{"location":"api/#genie_tooling","title":"genie_tooling","text":""},{"location":"api/#genie_tooling--genie-tooling","title":"genie-tooling","text":"<p>A hyper-pluggable Python middleware for Agentic AI and LLM applications. Async-first for performance.</p>"},{"location":"api/#genie_tooling-classes","title":"Classes","text":""},{"location":"api/#genie_tooling.BaseAgent","title":"BaseAgent","text":"<pre><code>BaseAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for agents that implement specific execution patterns (e.g., ReAct, Plan-and-Execute) using the Genie facade.</p> <p>Initializes the BaseAgent.</p> <p>Parameters:</p> Name Type Description Default <code>genie</code> <code>Genie</code> <p>An initialized instance of the Genie facade.</p> required <code>agent_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary for agent-specific configurations.</p> <code>None</code> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initializes the BaseAgent.\n\n    Args:\n        genie: An initialized instance of the Genie facade.\n        agent_config: Optional dictionary for agent-specific configurations.\n    \"\"\"\n    if not genie:\n        raise ValueError(\"A Genie instance is required to initialize an agent.\")\n    self.genie = genie\n    self.agent_config = agent_config or {}\n    logger.info(f\"Initialized {self.__class__.__name__} with Genie instance and config: {self.agent_config}\")\n</code></pre>"},{"location":"api/#genie_tooling.BaseAgent-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.BaseAgent.run","title":"run  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>run(goal: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>The main entry point to execute the agent's logic for a given goal.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The high-level goal or task for the agent to accomplish.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional runtime parameters specific to the agent's execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The final result or outcome of the agent's execution.</p> <code>Any</code> <p>The structure of the result is specific to the agent implementation.</p> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>@abc.abstractmethod\nasync def run(self, goal: str, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    The main entry point to execute the agent's logic for a given goal.\n\n    Args:\n        goal: The high-level goal or task for the agent to accomplish.\n        **kwargs: Additional runtime parameters specific to the agent's execution.\n\n    Returns:\n        The final result or outcome of the agent's execution.\n        The structure of the result is specific to the agent implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.BaseAgent.teardown","title":"teardown  <code>async</code>","text":"<pre><code>teardown() -&gt; None\n</code></pre> <p>Optional method for any cleanup specific to the agent. The Genie instance itself should be torn down separately by the application.</p> Source code in <code>src/genie_tooling/agents/base_agent.py</code> <pre><code>async def teardown(self) -&gt; None:\n    \"\"\"\n    Optional method for any cleanup specific to the agent.\n    The Genie instance itself should be torn down separately by the application.\n    \"\"\"\n    logger.info(f\"{self.__class__.__name__} teardown initiated.\")\n    # Add any agent-specific cleanup here if needed in the future.\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.PlanAndExecuteAgent","title":"PlanAndExecuteAgent","text":"<pre><code>PlanAndExecuteAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseAgent</code></p> <p>Implements the Plan-and-Execute agentic loop. 1. Planner: LLM generates a sequence of steps (tool calls) to achieve the goal.             Steps can name their outputs for use in subsequent steps. 2. Executor: Executes these steps sequentially, resolving placeholders.</p> Source code in <code>src/genie_tooling/agents/plan_and_execute_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    super().__init__(genie, agent_config)\n    self.planner_system_prompt_id = self.agent_config.get(\"planner_system_prompt_id\", DEFAULT_PLANNER_SYSTEM_PROMPT_ID)\n    self.planner_llm_provider_id = self.agent_config.get(\"planner_llm_provider_id\")\n    self.tool_formatter_id = self.agent_config.get(\"tool_formatter_id\", \"compact_text_formatter_plugin_v1\")\n    self.max_plan_retries = self.agent_config.get(\"max_plan_retries\", 1)\n    self.max_step_retries = self.agent_config.get(\"max_step_retries\", 0)\n    self.replan_on_step_failure = self.agent_config.get(\"replan_on_step_failure\", False)\n\n    logger.info(\n        f\"PlanAndExecuteAgent initialized. Planner Prompt ID: {self.planner_system_prompt_id}, \"\n        f\"Planner LLM: {self.planner_llm_provider_id or 'Genie Default'}\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.ReActAgent","title":"ReActAgent","text":"<pre><code>ReActAgent(genie: Genie, agent_config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>               Bases: <code>BaseAgent</code></p> <p>Implements the ReAct (Reason-Act) agentic loop.</p> Source code in <code>src/genie_tooling/agents/react_agent.py</code> <pre><code>def __init__(self, genie: \"Genie\", agent_config: Optional[Dict[str, Any]] = None):\n    super().__init__(genie, agent_config)\n    self.max_iterations = self.agent_config.get(\"max_iterations\", DEFAULT_REACT_MAX_ITERATIONS)\n    self.system_prompt_id = self.agent_config.get(\"system_prompt_id\", DEFAULT_REACT_SYSTEM_PROMPT_ID)\n    self.llm_provider_id = self.agent_config.get(\"llm_provider_id\")\n    self.tool_formatter_id = self.agent_config.get(\"tool_formatter_id\", \"compact_text_formatter_plugin_v1\")\n    self.stop_sequences = self.agent_config.get(\"stop_sequences\", [\"Observation:\"])\n    self.llm_retry_attempts = self.agent_config.get(\"llm_retry_attempts\", 1)\n    self.llm_retry_delay = self.agent_config.get(\"llm_retry_delay_seconds\", 2.0)\n\n    logger.info(\n        f\"ReActAgent initialized. Max iterations: {self.max_iterations}, \"\n        f\"System Prompt ID: {self.system_prompt_id}, LLM Provider: {self.llm_provider_id or 'Genie Default'}\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.AgentOutput","title":"AgentOutput","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized output structure from an agent's run method.</p>"},{"location":"api/#genie_tooling.PlannedStep","title":"PlannedStep","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single step in a generated plan.</p>"},{"location":"api/#genie_tooling.ReActObservation","title":"ReActObservation","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents one cycle of Thought-Action-Observation in ReAct.</p>"},{"location":"api/#genie_tooling.CacheProviderPlugin","title":"CacheProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a cache provider, designed for async operations.</p>"},{"location":"api/#genie_tooling.CacheProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CacheProviderPlugin.get","title":"get  <code>async</code>","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Retrieves an item from the cache. Args:     key: The key of the item to retrieve. Returns:     The cached item, or None if the key is not found or item is expired.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def get(self, key: str) -&gt; Optional[Any]:\n    \"\"\"\n    Retrieves an item from the cache.\n    Args:\n        key: The key of the item to retrieve.\n    Returns:\n        The cached item, or None if the key is not found or item is expired.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' get method not fully implemented.\")\n    return None\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.set","title":"set  <code>async</code>","text":"<pre><code>set(key: str, value: Any, ttl_seconds: Optional[int] = None) -&gt; None\n</code></pre> <p>Stores an item in the cache. Args:     key: The key under which to store the item.     value: The item to store. Should be serializable if cache is external.     ttl_seconds: Optional time-to-live in seconds. If None, item may persist indefinitely                  or use a default TTL defined by the cache provider.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -&gt; None:\n    \"\"\"\n    Stores an item in the cache.\n    Args:\n        key: The key under which to store the item.\n        value: The item to store. Should be serializable if cache is external.\n        ttl_seconds: Optional time-to-live in seconds. If None, item may persist indefinitely\n                     or use a default TTL defined by the cache provider.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' set method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(key: str) -&gt; bool\n</code></pre> <p>Deletes an item from the cache. Args:     key: The key of the item to delete. Returns:     True if the key existed and was deleted, False otherwise.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def delete(self, key: str) -&gt; bool:\n    \"\"\"\n    Deletes an item from the cache.\n    Args:\n        key: The key of the item to delete.\n    Returns:\n        True if the key existed and was deleted, False otherwise.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' delete method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.exists","title":"exists  <code>async</code>","text":"<pre><code>exists(key: str) -&gt; bool\n</code></pre> <p>Checks if a key exists in the cache (and is not expired). Args:     key: The key to check. Returns:     True if the key exists and is valid, False otherwise.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def exists(self, key: str) -&gt; bool:\n    \"\"\"\n    Checks if a key exists in the cache (and is not expired).\n    Args:\n        key: The key to check.\n    Returns:\n        True if the key exists and is valid, False otherwise.\n    \"\"\"\n    # Default implementation using get()\n    logger.debug(f\"CacheProvider '{self.plugin_id}' exists method using default get() check.\")\n    return await self.get(key) is not None\n</code></pre>"},{"location":"api/#genie_tooling.CacheProviderPlugin.clear_all","title":"clear_all  <code>async</code>","text":"<pre><code>clear_all() -&gt; bool\n</code></pre> <p>Clears all items from the cache managed by this provider. Use with caution, especially for shared caches. Returns:     True if the clear operation was successful or attempted, False on failure.</p> Source code in <code>src/genie_tooling/cache_providers/abc.py</code> <pre><code>async def clear_all(self) -&gt; bool:\n    \"\"\"\n    Clears all items from the cache managed by this provider.\n    Use with caution, especially for shared caches.\n    Returns:\n        True if the clear operation was successful or attempted, False on failure.\n    \"\"\"\n    logger.warning(f\"CacheProvider '{self.plugin_id}' clear_all method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.CodeExecutionResult","title":"CodeExecutionResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Standardized result structure for code execution.</p>"},{"location":"api/#genie_tooling.CodeExecutorPlugin","title":"CodeExecutorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that executes code, ideally in a sandboxed environment.</p>"},{"location":"api/#genie_tooling.CodeExecutorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CodeExecutorPlugin.execute_code","title":"execute_code  <code>async</code>","text":"<pre><code>execute_code(\n    language: str,\n    code: str,\n    timeout_seconds: int,\n    input_data: Optional[Dict[str, Any]] = None,\n) -&gt; CodeExecutionResult\n</code></pre> <p>Asynchronously executes the provided code string in the specified language. Implementations should strive for secure execution (sandboxing). Args:     language: The programming language of the code.     code: The code script to execute.     timeout_seconds: Maximum allowed execution time in seconds.     input_data: Optional dictionary to make available as variables within the code's scope                 (e.g., as a global dict named <code>_input</code> or similar, executor-defined). Returns:     A CodeExecutionResult NamedTuple.</p> Source code in <code>src/genie_tooling/code_executors/abc.py</code> <pre><code>async def execute_code(\n    self,\n    language: str,\n    code: str,\n    timeout_seconds: int,\n    input_data: Optional[Dict[str, Any]] = None # For passing structured input to the code's execution scope\n) -&gt; CodeExecutionResult:\n    \"\"\"\n    Asynchronously executes the provided code string in the specified language.\n    Implementations should strive for secure execution (sandboxing).\n    Args:\n        language: The programming language of the code.\n        code: The code script to execute.\n        timeout_seconds: Maximum allowed execution time in seconds.\n        input_data: Optional dictionary to make available as variables within the code's scope\n                    (e.g., as a global dict named `_input` or similar, executor-defined).\n    Returns:\n        A CodeExecutionResult NamedTuple.\n    \"\"\"\n    logger.warning(f\"CodeExecutor '{self.plugin_id}' execute_code method not fully implemented.\")\n    return CodeExecutionResult(stdout=\"\", stderr=\"Not implemented\", result=None, error=\"Executor not implemented\", execution_time_ms=0.0)\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorPlugin","title":"CommandProcessorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that processes a natural language command to determine which tool to use and what parameters to pass to it.</p>"},{"location":"api/#genie_tooling.CommandProcessorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.CommandProcessorPlugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]]) -&gt; None\n</code></pre> <p>Initializes the command processor. Args:     config: Processor-specific configuration dictionary. Expected to contain             'genie_facade: Genie' for accessing other middleware components.</p> Source code in <code>src/genie_tooling/command_processors/abc.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Initializes the command processor.\n    Args:\n        config: Processor-specific configuration dictionary. Expected to contain\n                'genie_facade: Genie' for accessing other middleware components.\n    \"\"\"\n    await super().setup(config)\n    logger.debug(f\"CommandProcessorPlugin '{self.plugin_id}': Base setup logic (if any) completed.\")\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorPlugin.process_command","title":"process_command  <code>async</code>","text":"<pre><code>process_command(\n    command: str,\n    conversation_history: Optional[List[ChatMessage]] = None,\n    correlation_id: Optional[str] = None,\n) -&gt; CommandProcessorResponse\n</code></pre> <p>Processes the given command, potentially using conversation history and other Genie components (via the facade provided in setup), to decide on a tool and its parameters.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The natural language command string from the user.</p> required <code>conversation_history</code> <code>Optional[List[ChatMessage]]</code> <p>Optional list of previous ChatMessages in the conversation.</p> <code>None</code> <code>correlation_id</code> <code>Optional[str]</code> <p>Optional ID to link related trace events.</p> <code>None</code> <p>Returns:</p> Type Description <code>CommandProcessorResponse</code> <p>A CommandProcessorResponse dictionary.</p> Source code in <code>src/genie_tooling/command_processors/abc.py</code> <pre><code>async def process_command(\n    self,\n    command: str,\n    conversation_history: Optional[List[ChatMessage]] = None,\n    correlation_id: Optional[str] = None\n) -&gt; CommandProcessorResponse:\n    \"\"\"\n    Processes the given command, potentially using conversation history and\n    other Genie components (via the facade provided in setup), to decide\n    on a tool and its parameters.\n\n    Args:\n        command: The natural language command string from the user.\n        conversation_history: Optional list of previous ChatMessages in the conversation.\n        correlation_id: Optional ID to link related trace events.\n\n    Returns:\n        A CommandProcessorResponse dictionary.\n    \"\"\"\n    logger.error(f\"CommandProcessorPlugin '{self.plugin_id}' process_command method not implemented.\")\n    raise NotImplementedError(f\"CommandProcessorPlugin '{self.plugin_id}' does not implement 'process_command'.\")\n</code></pre>"},{"location":"api/#genie_tooling.CommandProcessorResponse","title":"CommandProcessorResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response from a CommandProcessorPlugin.</p>"},{"location":"api/#genie_tooling.PluginManager","title":"PluginManager","text":"<pre><code>PluginManager(plugin_dev_dirs: Optional[List[str]] = None)\n</code></pre> <p>Manages discovery, loading, and access to plugins. Implements Hybrid: Entry Points + Configured Dev Directory discovery.</p> Source code in <code>src/genie_tooling/core/plugin_manager.py</code> <pre><code>def __init__(self, plugin_dev_dirs: Optional[List[str]] = None):\n    self.plugin_dev_dirs = [Path(p).resolve() for p in plugin_dev_dirs] if plugin_dev_dirs else []\n    self._plugin_instances: Dict[str, Plugin] = {}\n    self._discovered_plugin_classes: Dict[str, Type[Plugin]] = {}\n    self._plugin_source_map: Dict[str, str] = {}\n    logger.debug(f\"PluginManager initialized. Dev dirs: {self.plugin_dev_dirs}\")\n</code></pre>"},{"location":"api/#genie_tooling.Chunk","title":"Chunk","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a chunk of a document after splitting.</p>"},{"location":"api/#genie_tooling.Document","title":"Document","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a loaded document before splitting.</p>"},{"location":"api/#genie_tooling.Plugin","title":"Plugin","text":"<p>               Bases: <code>Protocol</code></p> <p>Base protocol for all plugins.</p>"},{"location":"api/#genie_tooling.Plugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.Plugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Optional asynchronous setup method for plugins. Called after instantiation.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Optional asynchronous setup method for plugins. Called after instantiation.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.Plugin.teardown","title":"teardown  <code>async</code>","text":"<pre><code>teardown() -&gt; None\n</code></pre> <p>Optional asynchronous teardown method for plugins. Called before application shutdown.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def teardown(self) -&gt; None:\n    \"\"\"Optional asynchronous teardown method for plugins. Called before application shutdown.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.RetrievedChunk","title":"RetrievedChunk","text":"<p>               Bases: <code>Chunk</code>, <code>Protocol</code></p> <p>Represents a chunk retrieved from a vector store, with a relevance score.</p>"},{"location":"api/#genie_tooling.StructuredError","title":"StructuredError","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized structure for reporting errors, especially to LLMs.</p>"},{"location":"api/#genie_tooling.DefinitionFormatterPlugin","title":"DefinitionFormatterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that formats a tool's metadata into a specific structure (e.g., for LLMs, for human readability).</p>"},{"location":"api/#genie_tooling.DefinitionFormatterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DefinitionFormatterPlugin.format","title":"format","text":"<pre><code>format(tool_metadata: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Takes the comprehensive metadata from Tool.get_metadata() and transforms it into the specific output format.</p> <p>Parameters:</p> Name Type Description Default <code>tool_metadata</code> <code>Dict[str, Any]</code> <p>The raw metadata dictionary from a Tool instance.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The formatted definition (e.g., a dict for JSON, a string for text).</p> <code>Any</code> <p>The type <code>Any</code> allows flexibility for various output formats.</p> Source code in <code>src/genie_tooling/definition_formatters/abc.py</code> <pre><code>def format(self, tool_metadata: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Takes the comprehensive metadata from Tool.get_metadata()\n    and transforms it into the specific output format.\n\n    Args:\n        tool_metadata: The raw metadata dictionary from a Tool instance.\n\n    Returns:\n        The formatted definition (e.g., a dict for JSON, a string for text).\n        The type `Any` allows flexibility for various output formats.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.DocumentLoaderPlugin","title":"DocumentLoaderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Loads documents from a source into an async stream of Document objects.</p>"},{"location":"api/#genie_tooling.DocumentLoaderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DocumentLoaderPlugin.load","title":"load  <code>async</code>","text":"<pre><code>load(\n    source_uri: str, config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Document]\n</code></pre> <p>Loads documents from the given source URI. Args:     source_uri: The URI of the data source (e.g., file path, URL, database connection string).     config: Loader-specific configuration dictionary. Yields:     Document objects.</p> Source code in <code>src/genie_tooling/document_loaders/abc.py</code> <pre><code>async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]:\n    \"\"\"\n    Loads documents from the given source URI.\n    Args:\n        source_uri: The URI of the data source (e.g., file path, URL, database connection string).\n        config: Loader-specific configuration dictionary.\n    Yields:\n        Document objects.\n    \"\"\"\n    # Example of how to make an async generator that does nothing if not implemented:\n    logger.warning(f\"DocumentLoaderPlugin '{self.plugin_id}' load method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin","title":"EmbeddingGeneratorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Generates embeddings for an async stream of Chunks.</p>"},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.EmbeddingGeneratorPlugin.embed","title":"embed  <code>async</code>","text":"<pre><code>embed(\n    chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]\n</code></pre> <p>Generates embeddings for each chunk. Args:     chunks: An async iterable of Chunk objects.     config: Embedder-specific configuration (e.g., model_name, batch_size, key_provider). Yields:     Tuples of (Chunk, EmbeddingVector).</p> Source code in <code>src/genie_tooling/embedding_generators/abc.py</code> <pre><code>async def embed(self, chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]:\n    \"\"\"\n    Generates embeddings for each chunk.\n    Args:\n        chunks: An async iterable of Chunk objects.\n        config: Embedder-specific configuration (e.g., model_name, batch_size, key_provider).\n    Yields:\n        Tuples of (Chunk, EmbeddingVector).\n    \"\"\"\n    logger.warning(f\"EmbeddingGeneratorPlugin '{self.plugin_id}' embed method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.ErrorFormatter","title":"ErrorFormatter","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for formatting StructuredError for different consumers (e.g., LLM, logs).</p>"},{"location":"api/#genie_tooling.ErrorFormatter-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ErrorFormatter.format","title":"format","text":"<pre><code>format(structured_error: StructuredError, target_format: str = 'llm') -&gt; Any\n</code></pre> <p>Formats the structured_error. Args:     structured_error: The error dictionary produced by an ErrorHandler.     target_format: A hint for the desired output format (e.g., \"llm\", \"json\", \"human_log\").                    Default is \"llm\". Returns:     The formatted error (e.g., a string for LLM, a dict for JSON). This method is synchronous.</p> Source code in <code>src/genie_tooling/error_formatters/abc.py</code> <pre><code>def format(self, structured_error: StructuredError, target_format: str = \"llm\") -&gt; Any:\n    \"\"\"\n    Formats the structured_error.\n    Args:\n        structured_error: The error dictionary produced by an ErrorHandler.\n        target_format: A hint for the desired output format (e.g., \"llm\", \"json\", \"human_log\").\n                       Default is \"llm\".\n    Returns:\n        The formatted error (e.g., a string for LLM, a dict for JSON).\n    This method is synchronous.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ErrorHandler","title":"ErrorHandler","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for handling exceptions during tool execution and converting them to StructuredError.</p>"},{"location":"api/#genie_tooling.ErrorHandler-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ErrorHandler.handle","title":"handle","text":"<pre><code>handle(\n    exception: Exception, tool: Any, context: Optional[Dict[str, Any]]\n) -&gt; StructuredError\n</code></pre> <p>Handles an exception that occurred during tool.execute() or related steps. Args:     exception: The exception instance caught.     tool: The Tool instance that was being executed (typed as Any to avoid circular import).           Implementations can cast or use getattr to access tool.identifier.     context: Optional context dictionary active during the call. Returns:     A StructuredError dictionary. This method is synchronous as error classification is typically CPU-bound.</p> Source code in <code>src/genie_tooling/error_handlers/abc.py</code> <pre><code>def handle(self, exception: Exception, tool: Any, context: Optional[Dict[str, Any]]) -&gt; StructuredError:\n    \"\"\"\n    Handles an exception that occurred during tool.execute() or related steps.\n    Args:\n        exception: The exception instance caught.\n        tool: The Tool instance that was being executed (typed as Any to avoid circular import).\n              Implementations can cast or use getattr to access tool.identifier.\n        context: Optional context dictionary active during the call.\n    Returns:\n        A StructuredError dictionary.\n    This method is synchronous as error classification is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.GuardrailPlugin","title":"GuardrailPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Base protocol for all guardrail plugins.</p>"},{"location":"api/#genie_tooling.InputGuardrailPlugin","title":"InputGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check input data (e.g., prompts, user messages).</p>"},{"location":"api/#genie_tooling.InputGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InputGuardrailPlugin.check_input","title":"check_input  <code>async</code>","text":"<pre><code>check_input(data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation\n</code></pre> <p>Checks input data. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_input(self, data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks input data.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"InputGuardrailPlugin '{self.plugin_id}' check_input method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.OutputGuardrailPlugin","title":"OutputGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check output data (e.g., LLM responses, tool results).</p>"},{"location":"api/#genie_tooling.OutputGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.OutputGuardrailPlugin.check_output","title":"check_output  <code>async</code>","text":"<pre><code>check_output(data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation\n</code></pre> <p>Checks output data. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_output(self, data: Any, context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks output data.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"OutputGuardrailPlugin '{self.plugin_id}' check_output method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin","title":"ToolUsageGuardrailPlugin","text":"<p>               Bases: <code>GuardrailPlugin</code>, <code>Protocol</code></p> <p>Protocol for guardrails that check tool usage attempts.</p>"},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolUsageGuardrailPlugin.check_tool_usage","title":"check_tool_usage  <code>async</code>","text":"<pre><code>check_tool_usage(\n    tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None\n) -&gt; GuardrailViolation\n</code></pre> <p>Checks if a tool usage attempt is permissible. Returns:     GuardrailViolation: Contains action (allow, block, warn) and reason.</p> Source code in <code>src/genie_tooling/guardrails/abc.py</code> <pre><code>async def check_tool_usage(self, tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -&gt; GuardrailViolation:\n    \"\"\"\n    Checks if a tool usage attempt is permissible.\n    Returns:\n        GuardrailViolation: Contains action (allow, block, warn) and reason.\n    \"\"\"\n    logger.warning(f\"ToolUsageGuardrailPlugin '{self.plugin_id}' check_tool_usage method not fully implemented.\")\n    return GuardrailViolation(action=self.default_action, reason=\"Not implemented\")\n</code></pre>"},{"location":"api/#genie_tooling.GuardrailViolation","title":"GuardrailViolation","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the outcome of a guardrail check.</p>"},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin","title":"HumanApprovalRequestPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that requests human approval for an action.</p>"},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.HumanApprovalRequestPlugin.request_approval","title":"request_approval  <code>async</code>","text":"<pre><code>request_approval(request: ApprovalRequest) -&gt; ApprovalResponse\n</code></pre> <p>Requests human approval for a given action or data. Implementations will vary (e.g., CLI prompt, web UI notification, API call).</p> Source code in <code>src/genie_tooling/hitl/abc.py</code> <pre><code>async def request_approval(self, request: ApprovalRequest) -&gt; ApprovalResponse:\n    \"\"\"\n    Requests human approval for a given action or data.\n    Implementations will vary (e.g., CLI prompt, web UI notification, API call).\n    \"\"\"\n    logger.warning(f\"HumanApprovalRequestPlugin '{self.plugin_id}' request_approval method not fully implemented.\")\n    # Default to denied if not implemented\n    return ApprovalResponse(\n        request_id=request.request_id,\n        status=\"denied\",\n        approver_id=\"system_default\",\n        reason=\"Plugin not implemented.\"\n    )\n</code></pre>"},{"location":"api/#genie_tooling.HITLManager","title":"HITLManager","text":"<pre><code>HITLManager(\n    plugin_manager: PluginManager,\n    default_approver_id: Optional[str] = None,\n    approver_configurations: Optional[Dict[str, Dict[str, Any]]] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/hitl/manager.py</code> <pre><code>def __init__(self, plugin_manager: PluginManager, default_approver_id: Optional[str] = None, approver_configurations: Optional[Dict[str, Dict[str, Any]]] = None):\n    self._plugin_manager = plugin_manager\n    self._default_approver_id = default_approver_id\n    self._approver_configurations = approver_configurations or {}\n    self._default_approver_instance: Optional[HumanApprovalRequestPlugin] = None\n    self._initialized_default = False\n    logger.info(\"HITLManager initialized.\")\n</code></pre>"},{"location":"api/#genie_tooling.HITLManager-attributes","title":"Attributes","text":""},{"location":"api/#genie_tooling.HITLManager.is_active","title":"is_active  <code>property</code>","text":"<pre><code>is_active: bool\n</code></pre> <p>Returns True if a default approver is configured (and not 'none').</p>"},{"location":"api/#genie_tooling.ApprovalRequest","title":"ApprovalRequest","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a request for human approval.</p>"},{"location":"api/#genie_tooling.ApprovalResponse","title":"ApprovalResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the response from a human approval request.</p>"},{"location":"api/#genie_tooling.InputValidationException","title":"InputValidationException","text":"<pre><code>InputValidationException(\n    message: str, errors: Any = None, params: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>               Bases: <code>ValueError</code></p> <p>Custom exception for input validation errors, providing more context.</p> Source code in <code>src/genie_tooling/input_validators/abc.py</code> <pre><code>def __init__(self, message: str, errors: Any = None, params: Optional[Dict[str,Any]] = None):\n    super().__init__(message)\n    self.errors = errors\n    self.params = params\n</code></pre>"},{"location":"api/#genie_tooling.InputValidator","title":"InputValidator","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for input parameter validators.</p>"},{"location":"api/#genie_tooling.InputValidator-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InputValidator.validate","title":"validate","text":"<pre><code>validate(params: Dict[str, Any], schema: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Validates parameters against a schema. Should raise InputValidationException on failure. May return params (possibly coerced or with defaults applied by validator). This method is synchronous as validation is typically CPU-bound.</p> Source code in <code>src/genie_tooling/input_validators/abc.py</code> <pre><code>def validate(self, params: Dict[str, Any], schema: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validates parameters against a schema.\n    Should raise InputValidationException on failure.\n    May return params (possibly coerced or with defaults applied by validator).\n    This method is synchronous as validation is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.InvocationStrategy","title":"InvocationStrategy","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a strategy that defines the complete lifecycle of invoking a tool. This includes validation, execution, transformation, caching, and error handling. All strategies must be designed to be async.</p>"},{"location":"api/#genie_tooling.InvocationStrategy-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InvocationStrategy.invoke","title":"invoke  <code>async</code>","text":"<pre><code>invoke(\n    tool: Tool,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]],\n    invoker_config: Dict[str, Any],\n) -&gt; Any\n</code></pre> <p>Executes the full tool invocation lifecycle according to this strategy.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Tool</code> <p>The Tool instance to invoke.</p> required <code>params</code> <code>Dict[str, Any]</code> <p>The parameters for the tool.</p> required <code>key_provider</code> <code>KeyProvider</code> <p>The async key provider.</p> required <code>context</code> <code>Optional[Dict[str, Any]]</code> <p>Optional context dictionary.</p> required <code>invoker_config</code> <code>Dict[str, Any]</code> <p>Configuration from the ToolInvoker, including:             - \"plugin_manager\": PluginManager instance             - \"validator_id\": Optional[str]             - \"transformer_id\": Optional[str]             - \"error_handler_id\": Optional[str]             - \"error_formatter_id\": Optional[str]             - \"cache_provider_id\": Optional[str]             - \"cache_config\": Optional[Dict[str, Any]]             - \"tracing_manager\": Optional[InteractionTracingManager]             - \"correlation_id\": Optional[str]</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool execution (possibly transformed by an OutputTransformer)</p> <code>Any</code> <p>or a structured error (formatted by an ErrorFormatter). The exact type depends</p> <code>Any</code> <p>on the formatter's output.</p> Source code in <code>src/genie_tooling/invocation_strategies/abc.py</code> <pre><code>async def invoke(\n    self,\n    tool: Tool,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Optional[Dict[str, Any]],\n    invoker_config: Dict[str, Any] # Contains plugin_manager and override IDs for components\n) -&gt; Any:\n    \"\"\"\n    Executes the full tool invocation lifecycle according to this strategy.\n\n    Args:\n        tool: The Tool instance to invoke.\n        params: The parameters for the tool.\n        key_provider: The async key provider.\n        context: Optional context dictionary.\n        invoker_config: Configuration from the ToolInvoker, including:\n                        - \"plugin_manager\": PluginManager instance\n                        - \"validator_id\": Optional[str]\n                        - \"transformer_id\": Optional[str]\n                        - \"error_handler_id\": Optional[str]\n                        - \"error_formatter_id\": Optional[str]\n                        - \"cache_provider_id\": Optional[str]\n                        - \"cache_config\": Optional[Dict[str, Any]]\n                        - \"tracing_manager\": Optional[InteractionTracingManager]\n                        - \"correlation_id\": Optional[str]\n\n    Returns:\n        The result of the tool execution (possibly transformed by an OutputTransformer)\n        or a structured error (formatted by an ErrorFormatter). The exact type depends\n        on the formatter's output.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.LLMProviderPlugin","title":"LLMProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that interacts with a Large Language Model provider.</p>"},{"location":"api/#genie_tooling.LLMProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LLMProviderPlugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]]) -&gt; None\n</code></pre> <p>Initializes the LLM provider. The 'config' dictionary is expected to contain 'key_provider: KeyProvider' if the specific LLM provider implementation requires API keys.</p> Source code in <code>src/genie_tooling/llm_providers/abc.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Initializes the LLM provider.\n    The 'config' dictionary is expected to contain 'key_provider: KeyProvider'\n    if the specific LLM provider implementation requires API keys.\n    \"\"\"\n    await super().setup(config)\n    logger.debug(f\"LLMProviderPlugin '{getattr(self, 'plugin_id', 'UnknownPluginID')}': Base setup logic (if any) completed.\")\n</code></pre>"},{"location":"api/#genie_tooling.ChatMessage","title":"ChatMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single message in a chat conversation. Compatible with OpenAI's ChatCompletion message structure.</p>"},{"location":"api/#genie_tooling.LLMChatResponse","title":"LLMChatResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response for chat completion LLM calls.</p>"},{"location":"api/#genie_tooling.LLMCompletionResponse","title":"LLMCompletionResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized response for text completion LLM calls.</p>"},{"location":"api/#genie_tooling.LLMUsageInfo","title":"LLMUsageInfo","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents token usage information from an LLM response.</p>"},{"location":"api/#genie_tooling.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a tool call requested by the LLM. Compatible with OpenAI's tool_calls structure.</p>"},{"location":"api/#genie_tooling.ToolCallFunction","title":"ToolCallFunction","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents the function to be called within a ToolCall.</p>"},{"location":"api/#genie_tooling.LogAdapterPlugin","title":"LogAdapterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a logging/monitoring adapter.</p>"},{"location":"api/#genie_tooling.LogAdapterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LogAdapterPlugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Configures logging handlers or integrates with external monitoring systems. This method is called after the adapter is instantiated. Args:     config: Adapter-specific configuration dictionary. May include 'plugin_manager'             if this adapter needs to load other plugins (e.g., a RedactorPlugin).</p> Source code in <code>src/genie_tooling/log_adapters/abc.py</code> <pre><code>async def setup(self, config: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Configures logging handlers or integrates with external monitoring systems.\n    This method is called after the adapter is instantiated.\n    Args:\n        config: Adapter-specific configuration dictionary. May include 'plugin_manager'\n                if this adapter needs to load other plugins (e.g., a RedactorPlugin).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.LogAdapterPlugin.process_event","title":"process_event  <code>async</code>","text":"<pre><code>process_event(\n    event_type: str,\n    data: Dict[str, Any],\n    schema_for_data: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Processes a structured event (e.g., for monitoring or detailed logging). Data should be pre-sanitized by this method or by a configured RedactorPlugin. Args:     event_type: A string identifying the type of event (e.g., \"tool_invoked\", \"tool_error\").     data: A dictionary containing event-specific data.     schema_for_data: Optional JSON schema corresponding to 'data', to aid redaction.</p> Source code in <code>src/genie_tooling/log_adapters/abc.py</code> <pre><code>async def process_event(self, event_type: str, data: Dict[str, Any], schema_for_data: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Processes a structured event (e.g., for monitoring or detailed logging).\n    Data should be pre-sanitized by this method or by a configured RedactorPlugin.\n    Args:\n        event_type: A string identifying the type of event (e.g., \"tool_invoked\", \"tool_error\").\n        data: A dictionary containing event-specific data.\n        schema_for_data: Optional JSON schema corresponding to 'data', to aid redaction.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupService","title":"ToolLookupService","text":"<pre><code>ToolLookupService(\n    tool_manager: ToolManager,\n    plugin_manager: PluginManager,\n    default_provider_id: Optional[str] = DEFAULT_LOOKUP_PROVIDER_ID,\n    default_indexing_formatter_id: Optional[str] = DEFAULT_INDEXING_FORMATTER_ID,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/lookup/service.py</code> <pre><code>def __init__(\n    self,\n    tool_manager: ToolManager,\n    plugin_manager: PluginManager,\n    default_provider_id: Optional[str] = DEFAULT_LOOKUP_PROVIDER_ID,\n    default_indexing_formatter_id: Optional[str] = DEFAULT_INDEXING_FORMATTER_ID,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n):\n    self._tool_manager = tool_manager\n    self._plugin_manager = plugin_manager\n    self._default_provider_id = default_provider_id\n    self._default_indexing_formatter_id = default_indexing_formatter_id\n    self._tracing_manager = tracing_manager\n    self._tracing_manager = tracing_manager\n    self._is_indexed_map: Dict[str, bool] = {}\n    logger.info(f\"ToolLookupService initialized. Default provider: '{default_provider_id}', default formatter plugin for indexing: '{default_indexing_formatter_id}'. Tracing enabled: {self._tracing_manager is not None}\")\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupService-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolLookupService.add_or_update_tools","title":"add_or_update_tools  <code>async</code>","text":"<pre><code>add_or_update_tools(\n    tool_ids: List[str],\n    provider_id_override: Optional[str] = None,\n    indexing_formatter_id_override: Optional[str] = None,\n    provider_config_override: Optional[Dict[str, Any]] = None,\n    correlation_id: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Adds or updates a list of tools in the specified lookup provider's index. This is an incremental update.</p> Source code in <code>src/genie_tooling/lookup/service.py</code> <pre><code>async def add_or_update_tools(\n    self,\n    tool_ids: List[str],\n    provider_id_override: Optional[str] = None,\n    indexing_formatter_id_override: Optional[str] = None,\n    provider_config_override: Optional[Dict[str, Any]] = None,\n    correlation_id: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Adds or updates a list of tools in the specified lookup provider's index.\n    This is an incremental update.\n    \"\"\"\n    corr_id = correlation_id or str(uuid.uuid4())\n    await self._trace(\"tool_lookup.add_or_update.start\", {\"tool_ids\": tool_ids}, \"debug\", corr_id)\n\n    target_provider_id = provider_id_override or self._default_provider_id\n    if not target_provider_id:\n        await self._trace(\"log.warning\", {\"message\": \"No tool lookup provider specified for add/update. Skipping.\"}, \"warning\", corr_id)\n        return\n\n    formatter_id_to_use = indexing_formatter_id_override or self._default_indexing_formatter_id\n    if not formatter_id_to_use:\n        await self._trace(\"log.error\", {\"message\": f\"No indexing formatter ID available for provider '{target_provider_id}'. Cannot update index.\"}, \"error\", corr_id)\n        return\n\n    await self._ensure_provider_is_indexed(target_provider_id, formatter_id_to_use, provider_config_override, correlation_id=corr_id)\n\n    provider_instance = await self._plugin_manager.get_plugin_instance(target_provider_id, config=provider_config_override)\n    if not isinstance(provider_instance, ToolLookupProviderPlugin):\n        await self._trace(\"log.error\", {\"message\": f\"Tool lookup provider '{target_provider_id}' not found or invalid for update.\"}, \"error\", corr_id)\n        return\n\n    for tool_id in tool_ids:\n        tool_data = await self._get_formatted_tool_data(tool_id, formatter_id_to_use, correlation_id=corr_id)\n        if tool_data:\n            try:\n                await provider_instance.update_tool(tool_id, tool_data, config=provider_config_override)\n                await self._trace(\"tool_lookup.add_or_update.success\", {\"tool_id\": tool_id}, \"debug\", corr_id)\n            except Exception as e_update:\n                await self._trace(\"log.error\", {\"message\": f\"Error updating tool '{tool_id}' in provider '{target_provider_id}': {e_update}\", \"exc_info\": True}, \"error\", corr_id)\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupService.remove_tools","title":"remove_tools  <code>async</code>","text":"<pre><code>remove_tools(\n    tool_ids: List[str],\n    provider_id_override: Optional[str] = None,\n    provider_config_override: Optional[Dict[str, Any]] = None,\n    correlation_id: Optional[str] = None,\n) -&gt; None\n</code></pre> <p>Removes a list of tools from the specified lookup provider's index.</p> Source code in <code>src/genie_tooling/lookup/service.py</code> <pre><code>async def remove_tools(\n    self,\n    tool_ids: List[str],\n    provider_id_override: Optional[str] = None,\n    provider_config_override: Optional[Dict[str, Any]] = None,\n    correlation_id: Optional[str] = None\n) -&gt; None:\n    \"\"\"Removes a list of tools from the specified lookup provider's index.\"\"\"\n    corr_id = correlation_id or str(uuid.uuid4())\n    await self._trace(\"tool_lookup.remove.start\", {\"tool_ids\": tool_ids}, \"debug\", corr_id)\n\n    target_provider_id = provider_id_override or self._default_provider_id\n    if not target_provider_id:\n        await self._trace(\"log.warning\", {\"message\": \"No tool lookup provider specified for remove. Skipping.\"}, \"warning\", corr_id)\n        return\n\n    if not self._is_indexed_map.get(target_provider_id, False):\n        await self._trace(\"log.debug\", {\"message\": f\"Index for provider '{target_provider_id}' not built. Skipping tool removal as it's not indexed anyway.\"}, \"debug\", corr_id)\n        return\n\n    provider_instance = await self._plugin_manager.get_plugin_instance(target_provider_id, config=provider_config_override)\n    if not isinstance(provider_instance, ToolLookupProviderPlugin):\n        await self._trace(\"log.error\", {\"message\": f\"Tool lookup provider '{target_provider_id}' not found or invalid for removal.\"}, \"error\", corr_id)\n        return\n\n    for tool_id in tool_ids:\n        try:\n            await provider_instance.remove_tool(tool_id, config=provider_config_override)\n            await self._trace(\"tool_lookup.remove.success\", {\"tool_id\": tool_id}, \"debug\", corr_id)\n        except Exception as e_remove:\n            await self._trace(\"log.error\", {\"message\": f\"Error removing tool '{tool_id}' from provider '{target_provider_id}': {e_remove}\", \"exc_info\": True}, \"error\", corr_id)\n</code></pre>"},{"location":"api/#genie_tooling.RankedToolResult","title":"RankedToolResult","text":"<pre><code>RankedToolResult(\n    tool_identifier: str,\n    score: float,\n    matched_tool_data: Optional[Dict[str, Any]] = None,\n    description_snippet: Optional[str] = None,\n    matched_keywords: Optional[List[str]] = None,\n    similarity_score_details: Optional[Dict[str, float]] = None,\n)\n</code></pre> <p>Represents a tool found by a lookup provider, with diagnostic info.</p> Source code in <code>src/genie_tooling/lookup/types.py</code> <pre><code>def __init__(\n    self,\n    tool_identifier: str,\n    score: float,\n    matched_tool_data: Optional[Dict[str, Any]] = None,\n    description_snippet: Optional[str] = None,\n    matched_keywords: Optional[List[str]] = None,\n    similarity_score_details: Optional[Dict[str, float]] = None,\n):\n    self.tool_identifier: str = tool_identifier\n    self.score: float = score # Relevance score (higher is better)\n    self.matched_tool_data: Optional[Dict[str, Any]] = matched_tool_data or {}\n    self.description_snippet: Optional[str] = description_snippet\n    self.matched_keywords: Optional[List[str]] = matched_keywords\n    self.similarity_score_details: Optional[Dict[str, float]] = similarity_score_details\n</code></pre>"},{"location":"api/#genie_tooling.InteractionTracerPlugin","title":"InteractionTracerPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that records interaction traces.</p>"},{"location":"api/#genie_tooling.InteractionTracerPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.InteractionTracerPlugin.record_trace","title":"record_trace  <code>async</code>","text":"<pre><code>record_trace(event: TraceEvent) -&gt; None\n</code></pre> <p>Records a single trace event. Implementations should handle batching or asynchronous sending if needed.</p> Source code in <code>src/genie_tooling/observability/abc.py</code> <pre><code>async def record_trace(self, event: TraceEvent) -&gt; None:\n    \"\"\"\n    Records a single trace event.\n    Implementations should handle batching or asynchronous sending if needed.\n    \"\"\"\n    logger.warning(f\"InteractionTracerPlugin '{self.plugin_id}' record_trace method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.TraceEvent","title":"TraceEvent","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single event to be traced.</p>"},{"location":"api/#genie_tooling.OutputTransformer","title":"OutputTransformer","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for output transformers.</p>"},{"location":"api/#genie_tooling.OutputTransformer-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.OutputTransformer.transform","title":"transform","text":"<pre><code>transform(output: Any, schema: Dict[str, Any]) -&gt; Any\n</code></pre> <p>Transforms raw tool output according to an output_schema or desired format. Raises OutputTransformationException on failure. This method is synchronous as transformation is typically CPU-bound.</p> Source code in <code>src/genie_tooling/output_transformers/abc.py</code> <pre><code>def transform(self, output: Any, schema: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Transforms raw tool output according to an output_schema or desired format.\n    Raises OutputTransformationException on failure.\n    This method is synchronous as transformation is typically CPU-bound.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.LLMOutputParserPlugin","title":"LLMOutputParserPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that parses the text output of an LLM.</p>"},{"location":"api/#genie_tooling.LLMOutputParserPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.LLMOutputParserPlugin.parse","title":"parse","text":"<pre><code>parse(text_output: str, schema: Optional[Any] = None) -&gt; ParsedOutput\n</code></pre> <p>Parses the LLM's text output. Args:     text_output: The raw text string from the LLM.     schema: Optional schema (e.g., Pydantic model, JSON schema) to guide parsing. Returns:     The parsed data, which could be a dict, list, Pydantic model instance, etc. Raises:     ValueError or a custom parsing exception if parsing fails.</p> Source code in <code>src/genie_tooling/prompts/llm_output_parsers/abc.py</code> <pre><code>def parse(self, text_output: str, schema: Optional[Any] = None) -&gt; ParsedOutput:\n    \"\"\"\n    Parses the LLM's text output.\n    Args:\n        text_output: The raw text string from the LLM.\n        schema: Optional schema (e.g., Pydantic model, JSON schema) to guide parsing.\n    Returns:\n        The parsed data, which could be a dict, list, Pydantic model instance, etc.\n    Raises:\n        ValueError or a custom parsing exception if parsing fails.\n    \"\"\"\n    logger.warning(f\"LLMOutputParserPlugin '{self.plugin_id}' parse method not implemented.\")\n    # Basic fallback: return as is, or attempt JSON if it looks like it\n    if text_output.strip().startswith(\"{\") and text_output.strip().endswith(\"}\"):\n        try:\n            return json.loads(text_output) # type: ignore\n        except Exception:\n            pass\n    return text_output # type: ignore\n</code></pre>"},{"location":"api/#genie_tooling.RedactorPlugin","title":"RedactorPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a data redaction plugin.</p>"},{"location":"api/#genie_tooling.RedactorPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.RedactorPlugin.sanitize","title":"sanitize","text":"<pre><code>sanitize(data: Any, schema_hints: Optional[Dict[str, Any]] = None) -&gt; Any\n</code></pre> <p>Sanitizes data to remove or mask sensitive information. This method is synchronous as redaction is typically CPU-bound. Args:     data: The data to sanitize (can be any Python object, commonly dicts or lists).     schema_hints: Optional JSON schema corresponding to 'data'. If provided,                   the redactor can use schema annotations (e.g., 'x-sensitive', 'format')                   to guide redaction. Returns:     The sanitized data, with sensitive parts replaced or removed.</p> Source code in <code>src/genie_tooling/redactors/abc.py</code> <pre><code>def sanitize(self, data: Any, schema_hints: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"\n    Sanitizes data to remove or mask sensitive information.\n    This method is synchronous as redaction is typically CPU-bound.\n    Args:\n        data: The data to sanitize (can be any Python object, commonly dicts or lists).\n        schema_hints: Optional JSON schema corresponding to 'data'. If provided,\n                      the redactor can use schema annotations (e.g., 'x-sensitive', 'format')\n                      to guide redaction.\n    Returns:\n        The sanitized data, with sensitive parts replaced or removed.\n    \"\"\"\n    logger.warning(f\"Redactor '{self.plugin_id}' sanitize method not fully implemented. Returning data as is.\")\n    return data # Default: no redaction\n</code></pre>"},{"location":"api/#genie_tooling.RetrieverPlugin","title":"RetrieverPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Retrieves relevant chunks based on a query, typically by composing an embedder and a vector store.</p>"},{"location":"api/#genie_tooling.RetrieverPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.RetrieverPlugin.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(\n    query: str, top_k: int, config: Optional[Dict[str, Any]] = None\n) -&gt; List[RetrievedChunk]\n</code></pre> <p>Retrieves relevant chunks for the given query. Args:     query: The natural language query string.     top_k: The number of top results to return.     config: Retriever-specific configuration. This should include details for               the embedder (e.g., \"embedder_id\", \"embedder_config\") and               vector store (e.g., \"vector_store_id\", \"vector_store_config\")               that this retriever instance will use. It should also contain \"plugin_manager\". Returns:     A list of RetrievedChunk objects.</p> Source code in <code>src/genie_tooling/retrievers/abc.py</code> <pre><code>async def retrieve(self, query: str, top_k: int, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]:\n    \"\"\"\n    Retrieves relevant chunks for the given query.\n    Args:\n        query: The natural language query string.\n        top_k: The number of top results to return.\n        config: Retriever-specific configuration. This should include details for\n                  the embedder (e.g., \"embedder_id\", \"embedder_config\") and\n                  vector store (e.g., \"vector_store_id\", \"vector_store_config\")\n                  that this retriever instance will use. It should also contain \"plugin_manager\".\n    Returns:\n        A list of RetrievedChunk objects.\n    \"\"\"\n    logger.warning(f\"RetrieverPlugin '{self.plugin_id}' retrieve method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.KeyProvider","title":"KeyProvider","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a component that securely provides API keys. This protocol must be implemented by the consuming application. The middleware itself does not store or manage API keys directly. All methods must be async.</p>"},{"location":"api/#genie_tooling.KeyProvider-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.KeyProvider.get_key","title":"get_key  <code>async</code>","text":"<pre><code>get_key(key_name: str) -&gt; Optional[str]\n</code></pre> <p>Asynchronously retrieves the API key value for the given key name.</p> <p>Parameters:</p> Name Type Description Default <code>key_name</code> <code>str</code> <p>The logical name of the API key required by a tool       (e.g., \"OPENWEATHERMAP_API_KEY\", \"OPENAI_API_KEY\").</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The API key string if found and accessible, otherwise None.</p> <code>Optional[str]</code> <p>Implementations should fetch keys securely (e.g., from environment</p> <code>Optional[str]</code> <p>variables, a secrets manager/vault, or application configuration).</p> <code>Optional[str]</code> <p>It should log (at debug level) if a key is requested but not found,</p> <code>Optional[str]</code> <p>but avoid logging the key value itself.</p> Source code in <code>src/genie_tooling/security/key_provider.py</code> <pre><code>async def get_key(self, key_name: str) -&gt; Optional[str]:\n    \"\"\"\n    Asynchronously retrieves the API key value for the given key name.\n\n    Args:\n        key_name: The logical name of the API key required by a tool\n                  (e.g., \"OPENWEATHERMAP_API_KEY\", \"OPENAI_API_KEY\").\n\n    Returns:\n        The API key string if found and accessible, otherwise None.\n        Implementations should fetch keys securely (e.g., from environment\n        variables, a secrets manager/vault, or application configuration).\n        It should log (at debug level) if a key is requested but not found,\n        but avoid logging the key value itself.\n    \"\"\"\n    # Example of what an implementation might log (this is a protocol, so no actual logic here)\n    # logger.debug(f\"KeyProvider: Request received for key '{key_name}'.\")\n    # value = self._internal_secure_key_storage.get(key_name)\n    # if not value:\n    #     logger.warning(f\"KeyProvider: Key '{key_name}' not found in secure storage.\")\n    # return value\n    ... # Indicates abstract method in Protocol\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin","title":"DistributedTaskQueuePlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that interacts with a distributed task queue system.</p>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.submit_task","title":"submit_task  <code>async</code>","text":"<pre><code>submit_task(\n    task_name: str,\n    args: Tuple = (),\n    kwargs: Optional[Dict[str, Any]] = None,\n    queue_name: Optional[str] = None,\n    task_options: Optional[Dict[str, Any]] = None,\n) -&gt; str\n</code></pre> <p>Submits a task to the distributed queue. Returns:     str: The unique ID of the submitted task. Raises:     Exception: If submission fails.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def submit_task(\n    self,\n    task_name: str, # Name of the task registered with the queue system\n    args: Tuple = (),\n    kwargs: Optional[Dict[str, Any]] = None,\n    queue_name: Optional[str] = None, # Specific queue to send to\n    task_options: Optional[Dict[str, Any]] = None # Options like countdown, eta, priority\n) -&gt; str:\n    \"\"\"\n    Submits a task to the distributed queue.\n    Returns:\n        str: The unique ID of the submitted task.\n    Raises:\n        Exception: If submission fails.\n    \"\"\"\n    logger.error(f\"DistributedTaskQueuePlugin '{self.plugin_id}' submit_task not implemented.\")\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.get_task_status","title":"get_task_status  <code>async</code>","text":"<pre><code>get_task_status(task_id: str, queue_name: Optional[str] = None) -&gt; TaskStatus\n</code></pre> <p>Gets the status of a previously submitted task.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def get_task_status(self, task_id: str, queue_name: Optional[str] = None) -&gt; TaskStatus:\n    \"\"\"Gets the status of a previously submitted task.\"\"\"\n    logger.warning(f\"DistributedTaskQueuePlugin '{self.plugin_id}' get_task_status not implemented.\")\n    return \"unknown\"\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.get_task_result","title":"get_task_result  <code>async</code>","text":"<pre><code>get_task_result(\n    task_id: str,\n    queue_name: Optional[str] = None,\n    timeout_seconds: Optional[float] = None,\n) -&gt; Any\n</code></pre> <p>Retrieves the result of a completed task. May block until the task is complete or timeout occurs. Raises:     TimeoutError: If timeout is specified and exceeded.     Exception: If task failed or other error.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def get_task_result(\n    self, task_id: str, queue_name: Optional[str] = None, timeout_seconds: Optional[float] = None\n) -&gt; Any:\n    \"\"\"\n    Retrieves the result of a completed task.\n    May block until the task is complete or timeout occurs.\n    Raises:\n        TimeoutError: If timeout is specified and exceeded.\n        Exception: If task failed or other error.\n    \"\"\"\n    logger.error(f\"DistributedTaskQueuePlugin '{self.plugin_id}' get_task_result not implemented.\")\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#genie_tooling.DistributedTaskQueuePlugin.revoke_task","title":"revoke_task  <code>async</code>","text":"<pre><code>revoke_task(\n    task_id: str, queue_name: Optional[str] = None, terminate: bool = False\n) -&gt; bool\n</code></pre> <p>Revokes (cancels) a pending or running task. Args:     terminate: If True, attempt to terminate a running task. Returns:     True if revocation was successful or task was already completed/revoked.</p> Source code in <code>src/genie_tooling/task_queues/abc.py</code> <pre><code>async def revoke_task(self, task_id: str, queue_name: Optional[str] = None, terminate: bool = False) -&gt; bool:\n    \"\"\"\n    Revokes (cancels) a pending or running task.\n    Args:\n        terminate: If True, attempt to terminate a running task.\n    Returns:\n        True if revocation was successful or task was already completed/revoked.\n    \"\"\"\n    logger.warning(f\"DistributedTaskQueuePlugin '{self.plugin_id}' revoke_task not implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.TextSplitterPlugin","title":"TextSplitterPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Splits an async stream of Documents into an async stream of Chunks.</p>"},{"location":"api/#genie_tooling.TextSplitterPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.TextSplitterPlugin.split","title":"split  <code>async</code>","text":"<pre><code>split(\n    documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None\n) -&gt; AsyncIterable[Chunk]\n</code></pre> <p>Splits documents into smaller chunks. Args:     documents: An async iterable of Document objects.     config: Splitter-specific configuration (e.g., chunk_size, chunk_overlap). Yields:     Chunk objects.</p> Source code in <code>src/genie_tooling/text_splitters/abc.py</code> <pre><code>async def split(self, documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Chunk]:\n    \"\"\"\n    Splits documents into smaller chunks.\n    Args:\n        documents: An async iterable of Document objects.\n        config: Splitter-specific configuration (e.g., chunk_size, chunk_overlap).\n    Yields:\n        Chunk objects.\n    \"\"\"\n    logger.warning(f\"TextSplitterPlugin '{self.plugin_id}' split method not fully implemented.\")\n    if False: # pylint: disable=false-condition\n        yield # type: ignore\n    return\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin","title":"TokenUsageRecorderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that records LLM token usage.</p>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.record_usage","title":"record_usage  <code>async</code>","text":"<pre><code>record_usage(record: TokenUsageRecord) -&gt; None\n</code></pre> <p>Records a single token usage event.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def record_usage(self, record: TokenUsageRecord) -&gt; None:\n    \"\"\"Records a single token usage event.\"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' record_usage method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.get_summary","title":"get_summary  <code>async</code>","text":"<pre><code>get_summary(filter_criteria: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieves a summary of token usage. The structure of the summary is implementation-dependent.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def get_summary(self, filter_criteria: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieves a summary of token usage.\n    The structure of the summary is implementation-dependent.\n    \"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' get_summary method not fully implemented.\")\n    return {\"error\": \"Not implemented\"}\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecorderPlugin.clear_records","title":"clear_records  <code>async</code>","text":"<pre><code>clear_records(filter_criteria: Optional[Dict[str, Any]] = None) -&gt; bool\n</code></pre> <p>Clears recorded usage data, optionally based on criteria.</p> Source code in <code>src/genie_tooling/token_usage/abc.py</code> <pre><code>async def clear_records(self, filter_criteria: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"Clears recorded usage data, optionally based on criteria.\"\"\"\n    logger.warning(f\"TokenUsageRecorderPlugin '{self.plugin_id}' clear_records method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.TokenUsageRecord","title":"TokenUsageRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single LLM token usage event.</p>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin","title":"ToolLookupProviderPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a plugin that finds relevant tools based on a query.</p>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.index_tools","title":"index_tools  <code>async</code>","text":"<pre><code>index_tools(\n    tools_data: List[Dict[str, Any]], config: Optional[Dict[str, Any]] = None\n) -&gt; None\n</code></pre> <p>Builds or completely replaces an internal index using formatted tool data. This is for full, batch re-indexing. For dynamic updates, use add/update/remove_tool.</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def index_tools(self, tools_data: List[Dict[str, Any]], config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Builds or completely replaces an internal index using formatted tool data.\n    This is for full, batch re-indexing. For dynamic updates, use add/update/remove_tool.\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' index_tools method not fully implemented.\")\n    pass\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.add_tool","title":"add_tool  <code>async</code>","text":"<pre><code>add_tool(tool_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -&gt; bool\n</code></pre> <p>Adds a single tool to the index. Returns:     True if the tool was added successfully, False otherwise.</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def add_tool(self, tool_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"\n    Adds a single tool to the index.\n    Returns:\n        True if the tool was added successfully, False otherwise.\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' add_tool method not implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.update_tool","title":"update_tool  <code>async</code>","text":"<pre><code>update_tool(\n    tool_id: str, tool_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None\n) -&gt; bool\n</code></pre> <p>Updates an existing tool in the index. This may be an add/overwrite operation. Returns:     True if the tool was updated successfully, False otherwise.</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def update_tool(self, tool_id: str, tool_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"\n    Updates an existing tool in the index. This may be an add/overwrite operation.\n    Returns:\n        True if the tool was updated successfully, False otherwise.\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' update_tool method not implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.remove_tool","title":"remove_tool  <code>async</code>","text":"<pre><code>remove_tool(tool_id: str, config: Optional[Dict[str, Any]] = None) -&gt; bool\n</code></pre> <p>Removes a single tool from the index by its ID. Returns:     True if the tool was removed or did not exist, False on failure.</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def remove_tool(self, tool_id: str, config: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"\n    Removes a single tool from the index by its ID.\n    Returns:\n        True if the tool was removed or did not exist, False on failure.\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' remove_tool method not implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling.ToolLookupProviderPlugin.find_tools","title":"find_tools  <code>async</code>","text":"<pre><code>find_tools(\n    natural_language_query: str, top_k: int = 5, config: Optional[Dict[str, Any]] = None\n) -&gt; List[RankedToolResult]\n</code></pre> <p>Searches the indexed tools based on the natural_language_query. Returns:     A list of RankedToolResult objects, sorted by relevance (highest score first).</p> Source code in <code>src/genie_tooling/tool_lookup_providers/abc.py</code> <pre><code>async def find_tools(\n    self,\n    natural_language_query: str,\n    top_k: int = 5,\n    config: Optional[Dict[str, Any]] = None\n) -&gt; List[RankedToolResult]:\n    \"\"\"\n    Searches the indexed tools based on the natural_language_query.\n    Returns:\n        A list of RankedToolResult objects, sorted by relevance (highest score first).\n    \"\"\"\n    logger.warning(f\"ToolLookupProvider '{self.plugin_id}' find_tools method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.ToolPlugin","title":"ToolPlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Protocol for a tool that can be executed by the middleware. All tools must be designed to be async.</p>"},{"location":"api/#genie_tooling.ToolPlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolPlugin.get_metadata","title":"get_metadata  <code>async</code>","text":"<pre><code>get_metadata() -&gt; Dict[str, Any]\n</code></pre> <p>Returns comprehensive metadata about the tool. This metadata is crucial for tool discovery, LLM function calling, and UI display.</p> <p>Expected structure: {     \"identifier\": str, (matches self.identifier)     \"name\": str, (human-friendly name)     \"description_human\": str, (detailed for developers/UI)     \"description_llm\": str, (concise, token-efficient for LLM prompts/function descriptions)     \"input_schema\": Dict[str, Any], (JSON Schema for tool parameters)     \"output_schema\": Dict[str, Any], (JSON Schema for tool's expected output structure)     \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", \"description\": \"Purpose of key\"}])     \"tags\": List[str], (for categorization, e.g., [\"weather\", \"api\", \"location\"])     \"version\": str, (e.g., \"1.0.0\")     \"cacheable\": bool, (optional, hints if tool output can be cached, default False)     \"cache_ttl_seconds\": Optional[int] (optional, default TTL if cacheable) }</p> Source code in <code>src/genie_tooling/tools/abc.py</code> <pre><code>async def get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Returns comprehensive metadata about the tool.\n    This metadata is crucial for tool discovery, LLM function calling, and UI display.\n\n    Expected structure:\n    {\n        \"identifier\": str, (matches self.identifier)\n        \"name\": str, (human-friendly name)\n        \"description_human\": str, (detailed for developers/UI)\n        \"description_llm\": str, (concise, token-efficient for LLM prompts/function descriptions)\n        \"input_schema\": Dict[str, Any], (JSON Schema for tool parameters)\n        \"output_schema\": Dict[str, Any], (JSON Schema for tool's expected output structure)\n        \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", \"description\": \"Purpose of key\"}])\n        \"tags\": List[str], (for categorization, e.g., [\"weather\", \"api\", \"location\"])\n        \"version\": str, (e.g., \"1.0.0\")\n        \"cacheable\": bool, (optional, hints if tool output can be cached, default False)\n        \"cache_ttl_seconds\": Optional[int] (optional, default TTL if cacheable)\n    }\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ToolPlugin.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(\n    params: Dict[str, Any], key_provider: KeyProvider, context: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>Executes the tool with the given parameters. Must be an async method.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>Validated parameters for the tool, conforming to its input_schema.</p> required <code>key_provider</code> <code>KeyProvider</code> <p>An async key provider instance for fetching necessary API keys.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Context dictionary carrying session/request-specific data, including      observability trace context.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The result of the tool execution. The structure should align with output_schema.</p> <code>Any</code> <p>If an error occurs during execution that the tool handles, it should still</p> <code>Any</code> <p>return a structured response, possibly including an 'error' field, conforming</p> <code>Any</code> <p>to its output_schema. Unhandled exceptions will be caught by the InvocationStrategy.</p> Source code in <code>src/genie_tooling/tools/abc.py</code> <pre><code>async def execute(\n    self,\n    params: Dict[str, Any],\n    key_provider: KeyProvider,\n    context: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    Executes the tool with the given parameters.\n    Must be an async method.\n\n    Args:\n        params: Validated parameters for the tool, conforming to its input_schema.\n        key_provider: An async key provider instance for fetching necessary API keys.\n        context: Context dictionary carrying session/request-specific data, including\n                 observability trace context.\n\n    Returns:\n        The result of the tool execution. The structure should align with output_schema.\n        If an error occurs during execution that the tool handles, it should still\n        return a structured response, possibly including an 'error' field, conforming\n        to its output_schema. Unhandled exceptions will be caught by the InvocationStrategy.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#genie_tooling.ToolManager","title":"ToolManager","text":"<pre><code>ToolManager(\n    plugin_manager: PluginManager,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/tools/manager.py</code> <pre><code>def __init__(self, plugin_manager: PluginManager, tracing_manager: Optional[InteractionTracingManager] = None):\n    self._plugin_manager = plugin_manager\n    self._tracing_manager = tracing_manager\n    self._tools: Dict[str, Tool] = {}\n    self._tool_initial_configs: Dict[str, Dict[str, Any]] = {} # Stores the initial tool_configurations\n    logger.debug(\"ToolManager initialized.\")\n</code></pre>"},{"location":"api/#genie_tooling.ToolManager-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.ToolManager.register_decorated_tools","title":"register_decorated_tools  <code>async</code>","text":"<pre><code>register_decorated_tools(functions: List[Callable], auto_enable: bool)\n</code></pre> <p>Processes a list of @tool decorated functions, enabling them based on the auto_enable flag and whether they are listed in the initial tool_configurations.</p> Source code in <code>src/genie_tooling/tools/manager.py</code> <pre><code>async def register_decorated_tools(self, functions: List[Callable], auto_enable: bool):\n    \"\"\"\n    Processes a list of @tool decorated functions, enabling them based on the auto_enable flag\n    and whether they are listed in the initial tool_configurations.\n    \"\"\"\n    from genie_tooling.genie import (\n        FunctionToolWrapper,\n    )\n    registered_count = 0\n    for func_item in functions:\n        metadata = getattr(func_item, \"_tool_metadata_\", None)\n        original_func_to_call = getattr(func_item, \"_original_function_\", func_item)\n\n        if not (metadata and isinstance(metadata, dict) and callable(original_func_to_call)):\n            await self._trace(\"log.warning\", {\"message\": f\"Function '{getattr(func_item, '__name__', str(func_item))}' not @tool decorated. Skipping.\"})\n            continue\n\n        tool_wrapper = FunctionToolWrapper(original_func_to_call, metadata)\n        tool_id = tool_wrapper.identifier\n\n        if tool_id in self._tools:\n            # This case means a class-based tool with the same identifier was already loaded\n            # via tool_configurations. We honor the explicitly configured class-based tool.\n            await self._trace(\"log.debug\", {\"message\": f\"Tool '{tool_id}' from decorated function was already loaded (e.g., as a class-based plugin via tool_configurations). Explicit/prior loading takes precedence.\"})\n            continue\n\n        # Determine if the tool should be enabled:\n        # 1. If auto_enable is True.\n        # 2. OR if auto_enable is False, BUT the tool_id is present in self._tool_initial_configs\n        #    (meaning it was listed in the `tool_configurations` dict passed to Genie.create).\n        should_enable_this_tool = auto_enable or (tool_id in self._tool_initial_configs)\n\n        if should_enable_this_tool:\n            self._tools[tool_id] = tool_wrapper\n            registered_count += 1\n            if auto_enable: # Logged as auto-enabled\n                await self._trace(\"log.info\", {\"message\": f\"Auto-enabled tool '{tool_id}' from decorated function '{func_item.__name__}'.\"})\n            else: # Logged as explicitly enabled via tool_configurations\n                await self._trace(\"log.info\", {\"message\": f\"Explicitly enabled tool '{tool_id}' from decorated function '{func_item.__name__}' via tool_configurations.\"})\n        else: # auto_enable is False AND tool_id is NOT in tool_configurations\n            await self._trace(\"log.warning\", {\"message\": f\"Tool '{tool_id}' from decorated function '{func_item.__name__}' was registered but is NOT active. To enable it, add '{tool_id}' to the `tool_configurations` dictionary in MiddlewareConfig.\"})\n\n    if registered_count &gt; 0 and not auto_enable: # Log summary if any tools were enabled explicitly this way\n        await self._trace(\"log.info\", {\"message\": f\"Explicitly enabled {registered_count} decorated tools listed in tool_configurations.\"})\n    elif registered_count &gt; 0 and auto_enable:\n        await self._trace(\"log.info\", {\"message\": f\"Auto-enabled {registered_count} decorated tools.\"})\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin","title":"VectorStorePlugin","text":"<p>               Bases: <code>Plugin</code>, <code>Protocol</code></p> <p>Interface for interacting with a vector database.</p>"},{"location":"api/#genie_tooling.VectorStorePlugin-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.VectorStorePlugin.add","title":"add  <code>async</code>","text":"<pre><code>add(\n    embeddings: AsyncIterable[Tuple[Chunk, EmbeddingVector]],\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Adds chunks and their embeddings to the vector store. Should handle batching internally if the input stream is large. Args:     embeddings: An async iterable of (Chunk, EmbeddingVector) tuples.     config: Vector store-specific configuration (e.g., collection name, batch_size). Returns:     A dictionary with status, e.g., {\"added_count\": int, \"errors\": List[str]}</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def add(self, embeddings: AsyncIterable[Tuple[Chunk, EmbeddingVector]], config: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]: # Returns status/count\n    \"\"\"\n    Adds chunks and their embeddings to the vector store.\n    Should handle batching internally if the input stream is large.\n    Args:\n        embeddings: An async iterable of (Chunk, EmbeddingVector) tuples.\n        config: Vector store-specific configuration (e.g., collection name, batch_size).\n    Returns:\n        A dictionary with status, e.g., {\"added_count\": int, \"errors\": List[str]}\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' add method not fully implemented.\")\n    return {\"added_count\": 0, \"errors\": [\"Not implemented\"]}\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin.search","title":"search  <code>async</code>","text":"<pre><code>search(\n    query_embedding: EmbeddingVector,\n    top_k: int,\n    filter_metadata: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; List[RetrievedChunk]\n</code></pre> <p>Searches the vector store for chunks similar to the query_embedding. Args:     query_embedding: The embedding vector of the query.     top_k: The number of top results to return.     filter_metadata: Optional metadata filter to apply during search.     config: Search-specific configuration. Returns:     A list of RetrievedChunk objects.</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def search(self, query_embedding: EmbeddingVector, top_k: int, filter_metadata: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]:\n    \"\"\"\n    Searches the vector store for chunks similar to the query_embedding.\n    Args:\n        query_embedding: The embedding vector of the query.\n        top_k: The number of top results to return.\n        filter_metadata: Optional metadata filter to apply during search.\n        config: Search-specific configuration.\n    Returns:\n        A list of RetrievedChunk objects.\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' search method not fully implemented.\")\n    return []\n</code></pre>"},{"location":"api/#genie_tooling.VectorStorePlugin.delete","title":"delete  <code>async</code>","text":"<pre><code>delete(\n    ids: Optional[List[str]] = None,\n    filter_metadata: Optional[Dict[str, Any]] = None,\n    delete_all: bool = False,\n    config: Optional[Dict[str, Any]] = None,\n) -&gt; bool\n</code></pre> <p>Deletes items from the vector store. Args:     ids: Optional list of chunk IDs to delete.     filter_metadata: Optional metadata filter to select items for deletion.     delete_all: If True, delete all items in the collection/store.     config: Deletion-specific configuration. Returns:     True if deletion was successful (or partially successful), False otherwise.</p> Source code in <code>src/genie_tooling/vector_stores/abc.py</code> <pre><code>async def delete(self, ids: Optional[List[str]] = None, filter_metadata: Optional[Dict[str, Any]] = None, delete_all: bool = False, config: Optional[Dict[str, Any]] = None) -&gt; bool:\n    \"\"\"\n    Deletes items from the vector store.\n    Args:\n        ids: Optional list of chunk IDs to delete.\n        filter_metadata: Optional metadata filter to select items for deletion.\n        delete_all: If True, delete all items in the collection/store.\n        config: Deletion-specific configuration.\n    Returns:\n        True if deletion was successful (or partially successful), False otherwise.\n    \"\"\"\n    logger.warning(f\"VectorStorePlugin '{self.plugin_id}' delete method not fully implemented.\")\n    return False\n</code></pre>"},{"location":"api/#genie_tooling-functions","title":"Functions","text":""},{"location":"api/#genie_tooling.tool","title":"tool","text":"<pre><code>tool(func: Callable) -&gt; Callable\n</code></pre> <p>Decorator to transform a Python function into a Genie-compatible Tool.</p> Source code in <code>src/genie_tooling/decorators.py</code> <pre><code>def tool(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to transform a Python function into a Genie-compatible Tool.\"\"\"\n    globalns = getattr(func, \"__globals__\", {})\n    try:\n        type_hints = get_type_hints(func, globalns=globalns)\n    except NameError as e:\n        try:\n            type_hints = get_type_hints(func)\n        except Exception:\n            type_hints = {}\n            print(\n                f\"Warning: Could not fully resolve type hints for {func.__name__} due to {e}. Schemas might be incomplete.\"\n            )\n\n    sig = inspect.signature(func)\n    docstring = inspect.getdoc(func) or \"\"\n    main_description = docstring.split(\"\\n\\n\")[0].strip() or f\"Executes the '{func.__name__}' tool.\"\n    param_descriptions_from_doc = _parse_docstring_for_params(docstring)\n    properties: Dict[str, Any] = {}\n    required_params: List[str] = []\n\n    for name, param in sig.parameters.items():\n        if name in (\"self\", \"cls\") or param.kind in (\n            inspect.Parameter.VAR_POSITIONAL,\n            inspect.Parameter.VAR_KEYWORD,\n        ):\n            continue\n\n        param_py_type_hint = type_hints.get(name, Any)\n        if isinstance(param_py_type_hint, str):\n            try:\n                param_py_type_hint = ForwardRef(param_py_type_hint)._evaluate(  # type: ignore\n                    globalns, {}, recursive_guard=frozenset()\n                )\n            except Exception:\n                param_py_type_hint = Any\n\n        is_optional_from_union_type = False\n        actual_param_type_for_schema = param_py_type_hint\n        origin = get_origin(param_py_type_hint)\n        args = get_args(param_py_type_hint)\n\n        if origin is Union and type(None) in (args or []):\n            is_optional_from_union_type = True\n            if args:\n                non_none_args = [t for t in args if t is not type(None)]\n                if len(non_none_args) == 1:\n                    actual_param_type_for_schema = non_none_args[0]\n                else:\n                    actual_param_type_for_schema = Union[tuple(non_none_args)]  # Keep as Union of non-None\n\n        param_schema_def = _map_type_to_json_schema(actual_param_type_for_schema)\n        # --- FIX: Default to 'string' if schema is empty (from Any type) ---\n        if not param_schema_def:\n            param_schema_def[\"type\"] = \"string\"\n\n        param_schema_def[\"description\"] = param_descriptions_from_doc.get(name, f\"Parameter '{name}'.\")\n\n        # Handle optionality by adding \"null\" to the type list\n        if is_optional_from_union_type:\n            if \"type\" in param_schema_def and isinstance(param_schema_def[\"type\"], str):\n                param_schema_def[\"type\"] = [param_schema_def[\"type\"], \"null\"]\n            elif \"type\" in param_schema_def and isinstance(param_schema_def[\"type\"], list):\n                if \"null\" not in param_schema_def[\"type\"]:\n                    param_schema_def[\"type\"].append(\"null\")\n\n        if param.default is inspect.Parameter.empty:\n            if not is_optional_from_union_type and name not in FRAMEWORK_INJECTED_PARAMS:\n                required_params.append(name)\n        else:\n            param_schema_def[\"default\"] = param.default\n\n        properties[name] = param_schema_def\n\n    input_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": properties}\n    if required_params:\n        input_schema[\"required\"] = required_params\n\n    return_py_type_hint = type_hints.get(\"return\", Any)\n    if isinstance(return_py_type_hint, str):\n        try:\n            return_py_type_hint = ForwardRef(return_py_type_hint)._evaluate(  # type: ignore\n                globalns, {}, recursive_guard=frozenset()\n            )\n        except Exception:\n            return_py_type_hint = Any\n\n    actual_return_type_for_schema = return_py_type_hint\n    ret_origin = get_origin(return_py_type_hint)\n    ret_args = get_args(return_py_type_hint)\n    if ret_origin is Union and type(None) in (ret_args or []):\n        if ret_args:\n            actual_return_type_for_schema = next((t for t in ret_args if t is not type(None)), Any)\n\n    output_schema_prop_def = _map_type_to_json_schema(actual_return_type_for_schema)\n    if not output_schema_prop_def:\n        output_schema_prop_def = {\"type\": \"object\"}\n\n    output_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": {\"result\": output_schema_prop_def}}\n    if (\n        output_schema_prop_def.get(\"type\") != \"null\"\n        and not (\n            isinstance(output_schema_prop_def.get(\"type\"), list)\n            and \"null\" in output_schema_prop_def[\"type\"]\n            and len(output_schema_prop_def[\"type\"]) == 1\n        )\n        and output_schema_prop_def != {}\n    ):\n        output_schema[\"required\"] = [\"result\"]\n\n    tool_metadata = {\n        \"identifier\": func.__name__,\n        \"name\": func.__name__.replace(\"_\", \" \").title(),\n        \"description_human\": main_description,\n        \"description_llm\": main_description,\n        \"input_schema\": input_schema,\n        \"output_schema\": output_schema,\n        \"key_requirements\": [],\n        \"tags\": [\"decorated_tool\"],\n        \"version\": \"1.0.0\",\n        \"cacheable\": False,\n    }\n\n    if inspect.iscoroutinefunction(func):\n\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await func(*args, **kwargs)\n\n    else:\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n\n    wrapper._tool_metadata_ = tool_metadata  # type: ignore\n    wrapper._original_function_ = func  # type: ignore\n    return wrapper\n</code></pre>"},{"location":"api/config_features/","title":"FeatureSettings","text":""},{"location":"api/config_features/#docsapiconfig_featuresmd","title":"docs/api/config_features.md","text":""},{"location":"api/config_features/#featuresettings","title":"FeatureSettings","text":""},{"location":"api/config_features/#genie_tooling.config.features","title":"genie_tooling.config.features","text":""},{"location":"api/config_features/#genie_tooling.config.features-classes","title":"Classes","text":""},{"location":"api/config_features/#genie_tooling.config.features.FeatureSettings","title":"FeatureSettings","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines high-level, user-friendly feature toggles and default choices for initializing the Genie middleware. This model is processed by ConfigResolver to populate the more detailed MiddlewareConfig.</p>"},{"location":"api/config_models/","title":"MiddlewareConfig","text":""},{"location":"api/config_models/#docsapiconfig_modelsmd","title":"docs/api/config_models.md","text":""},{"location":"api/config_models/#middlewareconfig","title":"MiddlewareConfig","text":""},{"location":"api/config_models/#genie_tooling.config.models","title":"genie_tooling.config.models","text":""},{"location":"api/config_models/#genie_tooling.config.models-classes","title":"Classes","text":""},{"location":"api/core_types/","title":"Core Types","text":""},{"location":"api/core_types/#docsapicore_typesmd","title":"docs/api/core_types.md","text":""},{"location":"api/core_types/#core-types","title":"Core Types","text":""},{"location":"api/core_types/#genie_tooling.core.types","title":"genie_tooling.core.types","text":"<p>Core shared types and protocols for the middleware.</p>"},{"location":"api/core_types/#genie_tooling.core.types-classes","title":"Classes","text":""},{"location":"api/core_types/#genie_tooling.core.types.Plugin","title":"Plugin","text":"<p>               Bases: <code>Protocol</code></p> <p>Base protocol for all plugins.</p>"},{"location":"api/core_types/#genie_tooling.core.types.Plugin-functions","title":"Functions","text":""},{"location":"api/core_types/#genie_tooling.core.types.Plugin.setup","title":"setup  <code>async</code>","text":"<pre><code>setup(config: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Optional asynchronous setup method for plugins. Called after instantiation.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Optional asynchronous setup method for plugins. Called after instantiation.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core_types/#genie_tooling.core.types.Plugin.teardown","title":"teardown  <code>async</code>","text":"<pre><code>teardown() -&gt; None\n</code></pre> <p>Optional asynchronous teardown method for plugins. Called before application shutdown.</p> Source code in <code>src/genie_tooling/core/types.py</code> <pre><code>async def teardown(self) -&gt; None:\n    \"\"\"Optional asynchronous teardown method for plugins. Called before application shutdown.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core_types/#genie_tooling.core.types.Document","title":"Document","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a loaded document before splitting.</p>"},{"location":"api/core_types/#genie_tooling.core.types.Chunk","title":"Chunk","text":"<p>               Bases: <code>Protocol</code></p> <p>Represents a chunk of a document after splitting.</p>"},{"location":"api/core_types/#genie_tooling.core.types.RetrievedChunk","title":"RetrievedChunk","text":"<p>               Bases: <code>Chunk</code>, <code>Protocol</code></p> <p>Represents a chunk retrieved from a vector store, with a relevance score.</p>"},{"location":"api/core_types/#genie_tooling.core.types.StructuredError","title":"StructuredError","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standardized structure for reporting errors, especially to LLMs.</p>"},{"location":"api/decorators/","title":"Decorators","text":""},{"location":"api/decorators/#genie_tooling.decorators","title":"genie_tooling.decorators","text":""},{"location":"api/decorators/#genie_tooling.decorators-functions","title":"Functions","text":""},{"location":"api/decorators/#genie_tooling.decorators.tool","title":"tool","text":"<pre><code>tool(func: Callable) -&gt; Callable\n</code></pre> <p>Decorator to transform a Python function into a Genie-compatible Tool.</p> Source code in <code>src/genie_tooling/decorators.py</code> <pre><code>def tool(func: Callable) -&gt; Callable:\n    \"\"\"Decorator to transform a Python function into a Genie-compatible Tool.\"\"\"\n    globalns = getattr(func, \"__globals__\", {})\n    try:\n        type_hints = get_type_hints(func, globalns=globalns)\n    except NameError as e:\n        try:\n            type_hints = get_type_hints(func)\n        except Exception:\n            type_hints = {}\n            print(\n                f\"Warning: Could not fully resolve type hints for {func.__name__} due to {e}. Schemas might be incomplete.\"\n            )\n\n    sig = inspect.signature(func)\n    docstring = inspect.getdoc(func) or \"\"\n    main_description = docstring.split(\"\\n\\n\")[0].strip() or f\"Executes the '{func.__name__}' tool.\"\n    param_descriptions_from_doc = _parse_docstring_for_params(docstring)\n    properties: Dict[str, Any] = {}\n    required_params: List[str] = []\n\n    for name, param in sig.parameters.items():\n        if name in (\"self\", \"cls\") or param.kind in (\n            inspect.Parameter.VAR_POSITIONAL,\n            inspect.Parameter.VAR_KEYWORD,\n        ):\n            continue\n\n        param_py_type_hint = type_hints.get(name, Any)\n        if isinstance(param_py_type_hint, str):\n            try:\n                param_py_type_hint = ForwardRef(param_py_type_hint)._evaluate(  # type: ignore\n                    globalns, {}, recursive_guard=frozenset()\n                )\n            except Exception:\n                param_py_type_hint = Any\n\n        is_optional_from_union_type = False\n        actual_param_type_for_schema = param_py_type_hint\n        origin = get_origin(param_py_type_hint)\n        args = get_args(param_py_type_hint)\n\n        if origin is Union and type(None) in (args or []):\n            is_optional_from_union_type = True\n            if args:\n                non_none_args = [t for t in args if t is not type(None)]\n                if len(non_none_args) == 1:\n                    actual_param_type_for_schema = non_none_args[0]\n                else:\n                    actual_param_type_for_schema = Union[tuple(non_none_args)]  # Keep as Union of non-None\n\n        param_schema_def = _map_type_to_json_schema(actual_param_type_for_schema)\n        # --- FIX: Default to 'string' if schema is empty (from Any type) ---\n        if not param_schema_def:\n            param_schema_def[\"type\"] = \"string\"\n\n        param_schema_def[\"description\"] = param_descriptions_from_doc.get(name, f\"Parameter '{name}'.\")\n\n        # Handle optionality by adding \"null\" to the type list\n        if is_optional_from_union_type:\n            if \"type\" in param_schema_def and isinstance(param_schema_def[\"type\"], str):\n                param_schema_def[\"type\"] = [param_schema_def[\"type\"], \"null\"]\n            elif \"type\" in param_schema_def and isinstance(param_schema_def[\"type\"], list):\n                if \"null\" not in param_schema_def[\"type\"]:\n                    param_schema_def[\"type\"].append(\"null\")\n\n        if param.default is inspect.Parameter.empty:\n            if not is_optional_from_union_type and name not in FRAMEWORK_INJECTED_PARAMS:\n                required_params.append(name)\n        else:\n            param_schema_def[\"default\"] = param.default\n\n        properties[name] = param_schema_def\n\n    input_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": properties}\n    if required_params:\n        input_schema[\"required\"] = required_params\n\n    return_py_type_hint = type_hints.get(\"return\", Any)\n    if isinstance(return_py_type_hint, str):\n        try:\n            return_py_type_hint = ForwardRef(return_py_type_hint)._evaluate(  # type: ignore\n                globalns, {}, recursive_guard=frozenset()\n            )\n        except Exception:\n            return_py_type_hint = Any\n\n    actual_return_type_for_schema = return_py_type_hint\n    ret_origin = get_origin(return_py_type_hint)\n    ret_args = get_args(return_py_type_hint)\n    if ret_origin is Union and type(None) in (ret_args or []):\n        if ret_args:\n            actual_return_type_for_schema = next((t for t in ret_args if t is not type(None)), Any)\n\n    output_schema_prop_def = _map_type_to_json_schema(actual_return_type_for_schema)\n    if not output_schema_prop_def:\n        output_schema_prop_def = {\"type\": \"object\"}\n\n    output_schema: Dict[str, Any] = {\"type\": \"object\", \"properties\": {\"result\": output_schema_prop_def}}\n    if (\n        output_schema_prop_def.get(\"type\") != \"null\"\n        and not (\n            isinstance(output_schema_prop_def.get(\"type\"), list)\n            and \"null\" in output_schema_prop_def[\"type\"]\n            and len(output_schema_prop_def[\"type\"]) == 1\n        )\n        and output_schema_prop_def != {}\n    ):\n        output_schema[\"required\"] = [\"result\"]\n\n    tool_metadata = {\n        \"identifier\": func.__name__,\n        \"name\": func.__name__.replace(\"_\", \" \").title(),\n        \"description_human\": main_description,\n        \"description_llm\": main_description,\n        \"input_schema\": input_schema,\n        \"output_schema\": output_schema,\n        \"key_requirements\": [],\n        \"tags\": [\"decorated_tool\"],\n        \"version\": \"1.0.0\",\n        \"cacheable\": False,\n    }\n\n    if inspect.iscoroutinefunction(func):\n\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            return await func(*args, **kwargs)\n\n    else:\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n\n    wrapper._tool_metadata_ = tool_metadata  # type: ignore\n    wrapper._original_function_ = func  # type: ignore\n    return wrapper\n</code></pre>"},{"location":"api/genie_facade/","title":"Genie Facade and Interfaces","text":""},{"location":"api/genie_facade/#genie-facade","title":"Genie Facade","text":""},{"location":"api/genie_facade/#genie_tooling.genie.Genie","title":"genie_tooling.genie.Genie","text":"<pre><code>Genie(\n    plugin_manager: PluginManager,\n    key_provider: KeyProvider,\n    config: MiddlewareConfig,\n    tool_manager: ToolManager,\n    tool_invoker: ToolInvoker,\n    rag_manager: RAGManager,\n    tool_lookup_service: ToolLookupService,\n    llm_provider_manager: LLMProviderManager,\n    command_processor_manager: CommandProcessorManager,\n    log_adapter: LogAdapter,\n    tracing_manager: InteractionTracingManager,\n    hitl_manager: HITLManager,\n    token_usage_manager: TokenUsageManager,\n    guardrail_manager: GuardrailManager,\n    prompt_manager: PromptManager,\n    conversation_manager: ConversationStateManager,\n    llm_output_parser_manager: LLMOutputParserManager,\n    task_queue_manager: DistributedTaskQueueManager,\n    llm_interface: LLMInterface,\n    rag_interface: RAGInterface,\n    observability_interface: ObservabilityInterface,\n    hitl_interface: HITLInterface,\n    usage_tracking_interface: UsageTrackingInterface,\n    prompt_interface: PromptInterface,\n    conversation_interface: ConversationInterface,\n    task_queue_interface: TaskQueueInterface,\n)\n</code></pre> Source code in <code>src/genie_tooling/genie.py</code> <pre><code>def __init__(\n    self, plugin_manager: PluginManager, key_provider: KeyProvider, config: MiddlewareConfig, tool_manager: ToolManager,\n    tool_invoker: ToolInvoker, rag_manager: RAGManager, tool_lookup_service: ToolLookupService, llm_provider_manager: LLMProviderManager,\n    command_processor_manager: CommandProcessorManager, log_adapter: LogAdapterPlugin, tracing_manager: InteractionTracingManager,\n    hitl_manager: HITLManager, token_usage_manager: TokenUsageManager, guardrail_manager: GuardrailManager, prompt_manager: PromptManager,\n    conversation_manager: ConversationStateManager, llm_output_parser_manager: LLMOutputParserManager, task_queue_manager: DistributedTaskQueueManager,\n    llm_interface: LLMInterface, rag_interface: RAGInterface, observability_interface: ObservabilityInterface, hitl_interface: HITLInterface,\n    usage_tracking_interface: UsageTrackingInterface, prompt_interface: PromptInterface, conversation_interface: ConversationInterface,\n    task_queue_interface: TaskQueueInterface\n):\n    self._plugin_manager = plugin_manager\n    self._key_provider = key_provider\n    self._config = config\n    self._tool_manager = tool_manager\n    self._tool_invoker = tool_invoker\n    self._rag_manager = rag_manager\n    self._tool_lookup_service = tool_lookup_service\n    self._llm_provider_manager = llm_provider_manager\n    self._command_processor_manager = command_processor_manager\n    self._log_adapter = log_adapter\n    self._tracing_manager = tracing_manager\n    self._hitl_manager = hitl_manager\n    self._token_usage_manager = token_usage_manager\n    self._guardrail_manager = guardrail_manager\n    self._prompt_manager = prompt_manager\n    self._conversation_manager = conversation_manager\n    self._llm_output_parser_manager = llm_output_parser_manager\n    self._task_queue_manager = task_queue_manager\n    self.llm = llm_interface\n    self.rag = rag_interface\n    self.observability = observability_interface\n    self.human_in_loop = hitl_interface\n    self.usage = usage_tracking_interface\n    self.prompts = prompt_interface\n    self.conversation = conversation_interface\n    self.task_queue = task_queue_interface\n    self._config._genie_instance = self # type: ignore\n    task = asyncio.create_task(self.observability.trace_event(\"log.info\", {\"message\": \"Genie facade initialized with resolved configuration.\"}, \"Genie\"))\n    background_tasks.add(task)\n    task.add_done_callback(background_tasks.discard)\n</code></pre>"},{"location":"api/genie_facade/#core-interfaces","title":"Core Interfaces","text":"<p>These interfaces are accessed via attributes on the <code>Genie</code> facade instance (e.g., <code>genie.llm</code>, <code>genie.rag</code>).</p>"},{"location":"api/genie_facade/#genie_tooling.interfaces.LLMInterface","title":"genie_tooling.interfaces.LLMInterface","text":"<pre><code>LLMInterface(\n    llm_provider_manager: LLMProviderManager,\n    default_provider_id: Optional[str],\n    output_parser_manager: LLMOutputParserManager,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n    guardrail_manager: Optional[GuardrailManager] = None,\n    token_usage_manager: Optional[TokenUsageManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(\n    self,\n    llm_provider_manager: \"LLMProviderManager\",\n    default_provider_id: Optional[str],\n    output_parser_manager: \"LLMOutputParserManager\",\n    tracing_manager: Optional[\"InteractionTracingManager\"] = None,\n    guardrail_manager: Optional[\"GuardrailManager\"] = None,\n    token_usage_manager: Optional[\"TokenUsageManager\"] = None\n):\n    self._llm_provider_manager = llm_provider_manager\n    self._default_provider_id = default_provider_id\n    self._output_parser_manager = output_parser_manager\n    self._tracing_manager = tracing_manager\n    self._guardrail_manager = guardrail_manager\n    self._token_usage_manager = token_usage_manager\n    logger.debug(f\"LLMInterface initialized with default provider ID: {self._default_provider_id}\")\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.RAGInterface","title":"genie_tooling.interfaces.RAGInterface","text":"<pre><code>RAGInterface(\n    rag_manager: RAGManager,\n    config: MiddlewareConfig,\n    key_provider: KeyProvider,\n    tracing_manager: Optional[InteractionTracingManager] = None,\n)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, rag_manager: \"RAGManager\", config: \"MiddlewareConfig\", key_provider: \"KeyProvider\", tracing_manager: Optional[\"InteractionTracingManager\"] = None):\n    self._rag_manager = rag_manager\n    self._config = config\n    self._key_provider = key_provider\n    self._tracing_manager = tracing_manager\n    logger.debug(\"RAGInterface initialized.\")\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.ObservabilityInterface","title":"genie_tooling.interfaces.ObservabilityInterface","text":"<pre><code>ObservabilityInterface(tracing_manager: InteractionTracingManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, tracing_manager: \"InteractionTracingManager\"): self._tracing_manager = tracing_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.HITLInterface","title":"genie_tooling.interfaces.HITLInterface","text":"<pre><code>HITLInterface(hitl_manager: HITLManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, hitl_manager: \"HITLManager\"): self._hitl_manager = hitl_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.UsageTrackingInterface","title":"genie_tooling.interfaces.UsageTrackingInterface","text":"<pre><code>UsageTrackingInterface(token_usage_manager: TokenUsageManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, token_usage_manager: \"TokenUsageManager\"): self._token_usage_manager = token_usage_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.PromptInterface","title":"genie_tooling.interfaces.PromptInterface","text":"<pre><code>PromptInterface(prompt_manager: PromptManager)\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, prompt_manager: \"PromptManager\"): self._prompt_manager = prompt_manager\n</code></pre>"},{"location":"api/genie_facade/#genie_tooling.interfaces.ConversationInterface","title":"genie_tooling.interfaces.ConversationInterface","text":"<pre><code>ConversationInterface(conversation_manager: Optional[ConversationStateManager])\n</code></pre> Source code in <code>src/genie_tooling/interfaces.py</code> <pre><code>def __init__(self, conversation_manager: Optional[\"ConversationStateManager\"]):\n    self._conversation_manager = conversation_manager\n    if not self._conversation_manager:\n        logger.warning(\"ConversationInterface initialized without a ConversationStateManager. Operations will be no-ops or return defaults.\")\n</code></pre>"},{"location":"api/plugin_manager/","title":"Plugin Manager","text":""},{"location":"api/plugin_manager/#docsapiplugin_managermd","title":"docs/api/plugin_manager.md","text":""},{"location":"api/plugin_manager/#plugin-manager","title":"Plugin Manager","text":""},{"location":"api/plugin_manager/#genie_tooling.core.plugin_manager","title":"genie_tooling.core.plugin_manager","text":"<p>PluginManager for discovering, loading, and managing plugins.</p>"},{"location":"api/plugin_manager/#genie_tooling.core.plugin_manager-classes","title":"Classes","text":""},{"location":"api/plugin_manager/#genie_tooling.core.plugin_manager.PluginManager","title":"PluginManager","text":"<pre><code>PluginManager(plugin_dev_dirs: Optional[List[str]] = None)\n</code></pre> <p>Manages discovery, loading, and access to plugins. Implements Hybrid: Entry Points + Configured Dev Directory discovery.</p> Source code in <code>src/genie_tooling/core/plugin_manager.py</code> <pre><code>def __init__(self, plugin_dev_dirs: Optional[List[str]] = None):\n    self.plugin_dev_dirs = [Path(p).resolve() for p in plugin_dev_dirs] if plugin_dev_dirs else []\n    self._plugin_instances: Dict[str, Plugin] = {}\n    self._discovered_plugin_classes: Dict[str, Type[Plugin]] = {}\n    self._plugin_source_map: Dict[str, str] = {}\n    logger.debug(f\"PluginManager initialized. Dev dirs: {self.plugin_dev_dirs}\")\n</code></pre>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>Genie Tooling is configured at runtime using a <code>MiddlewareConfig</code> object. For ease of use, especially for common setups, <code>MiddlewareConfig</code> integrates a <code>FeatureSettings</code> model.</p>"},{"location":"guides/configuration/#simplified-configuration-with-featuresettings","title":"Simplified Configuration with <code>FeatureSettings</code>","text":"<p>The recommended way to start configuring Genie is by using the <code>features</code> attribute of <code>MiddlewareConfig</code>. <code>FeatureSettings</code> provides high-level toggles and default choices for major components like LLM providers, RAG components, caching, tool lookup, logging adapter, observability, HITL, token usage, guardrails, prompt system, conversation state, and distributed task queues.</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        command_processor=\"llm_assisted\",\n        # ... other features ...\n    ),\n    # Even in automatic mode, class-based tools often need to be enabled.\n    # Tools requiring specific config must always be listed.\n    tool_configurations={\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": \"./my_agent_workspace\"}\n    }\n)\n</code></pre>"},{"location":"guides/configuration/#how-featuresettings-works","title":"How <code>FeatureSettings</code> Works","text":"<p>When you initialize <code>Genie</code> with a <code>MiddlewareConfig</code> containing <code>FeatureSettings</code>, an internal <code>ConfigResolver</code> processes these settings. It translates your high-level choices into specific plugin IDs and default configurations for those plugins.</p> <p>For a full list of available aliases and more details on simplified configuration, please see the Simplified Configuration Guide.</p>"},{"location":"guides/configuration/#explicit-overrides-and-detailed-configuration","title":"Explicit Overrides and Detailed Configuration","text":"<p>While <code>FeatureSettings</code> provides a convenient starting point, you can always provide more detailed, explicit configurations that will override or augment the settings derived from features.</p>"},{"location":"guides/configuration/#the-auto_enable_registered_tools-flag","title":"The <code>auto_enable_registered_tools</code> Flag","text":"<p><code>MiddlewareConfig</code> includes a crucial flag for managing tool enablement:</p> <ul> <li><code>auto_enable_registered_tools: bool</code><ul> <li>Default: <code>True</code></li> <li>Behavior:<ul> <li>When <code>True</code>, any tool registered via <code>@tool</code> and <code>genie.register_tool_functions()</code> is automatically enabled and ready for use. This is convenient for development and rapid prototyping.</li> <li>When <code>False</code>, a tool is only active if its identifier is explicitly listed as a key in the <code>tool_configurations</code> dictionary.</li> </ul> </li> <li>Recommendation: For production environments, it is strongly recommended to set this to <code>False</code> to maintain a clear, secure manifest of the agent's capabilities.</li> </ul> </li> </ul>"},{"location":"guides/configuration/#configuring-specific-plugins","title":"Configuring Specific Plugins","text":"<p>You can provide specific configurations for individual plugins using the various <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code>. These dictionaries are keyed by the canonical plugin ID or a recognized alias.</p> <pre><code>app_config = MiddlewareConfig(\n    auto_enable_registered_tools=False, # Production-safe setting\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\", \n    ),\n    # Override configuration for the OpenAI LLM provider\n    llm_provider_configurations={\n        \"openai_llm_provider_v1\": { # Canonical ID\n            \"model_name\": \"gpt-4-turbo-preview\", \n            \"request_timeout_seconds\": 120\n        }\n    },\n    # Explicitly enable and configure tools\n    tool_configurations={\n        \"calculator_tool\": {}, # Enable calculator\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": \"./agent_workspace\"}\n    },\n    # ... other specific configurations ...\n)\n</code></pre> <p>Key Points for Explicit Configuration:</p> <ul> <li>Precedence: Explicit configurations in <code>default_*_id</code> fields or within the <code>*_configurations</code> dictionaries take precedence over settings derived from <code>FeatureSettings</code>.</li> <li>Tool Enablement: A tool plugin's availability is controlled by <code>auto_enable_registered_tools</code> and the <code>tool_configurations</code> dictionary.</li> <li>Canonical IDs vs. Aliases: You can use either the canonical plugin ID (e.g., <code>\"ollama_llm_provider_v1\"</code>) or a recognized alias (e.g., <code>\"ollama\"</code>) as keys in configuration dictionaries.</li> </ul>"},{"location":"guides/configuration/#plugin-development-directories","title":"Plugin Development Directories","text":"<p>If you have custom plugins located outside your main Python path or not installed as entry points, you can specify their location:</p> <p><pre><code>app_config = MiddlewareConfig(\n    plugin_dev_dirs=[\"/path/to/my/custom_plugins\", \"./project_plugins\"]\n)\n</code></pre> The <code>PluginManager</code> will scan these directories for valid plugin classes.</p>"},{"location":"guides/creating_other_plugins/","title":"Creating Other Plugins","text":"<p>Beyond tools and RAG components, Genie Tooling's pluggable architecture allows for customization of many other functionalities. This guide provides an overview of how to create various other types of plugins.</p>"},{"location":"guides/creating_other_plugins/#general-plugin-principles","title":"General Plugin Principles","text":"<p>All plugins in Genie Tooling typically adhere to the <code>genie_tooling.core.types.Plugin</code> protocol:</p> <pre><code>from typing import Protocol, Optional, Dict, Any\n\nclass Plugin(Protocol):\n    @property\n    def plugin_id(self) -&gt; str:\n        ... # Unique identifier for the plugin\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n        pass # Optional async setup\n\n    async def teardown(self) -&gt; None:\n        pass # Optional async teardown\n</code></pre> <p>Your custom plugin class should: 1.  Define a unique <code>plugin_id</code> as a class attribute. 2.  Implement the specific protocol for the type of plugin you are creating (e.g., <code>LLMProviderPlugin</code>, <code>CacheProviderPlugin</code>). 3.  Implement <code>async def setup(self, config: Optional[Dict[str, Any]] = None)</code> if your plugin requires initialization with configuration. 4.  Implement <code>async def teardown(self)</code> if your plugin needs to release resources.</p>"},{"location":"guides/creating_other_plugins/#registering-your-plugin","title":"Registering Your Plugin","text":"<p>Make your plugin discoverable by Genie: *   Entry Points: Define an entry point in your <code>pyproject.toml</code> under the <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> group.     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_custom_cache_v1\" = \"my_package.my_module:MyCustomCacheProvider\"\n</code></pre> *   Plugin Development Directories: Place your plugin's Python file in a directory specified in <code>MiddlewareConfig.plugin_dev_dirs</code>.</p>"},{"location":"guides/creating_other_plugins/#common-plugin-categories-and-their-protocols","title":"Common Plugin Categories and Their Protocols","text":"<p>Refer to the <code>src/genie_tooling/</code> subdirectories for the specific abstract base classes (ABCs) or protocols for each plugin type. Key examples include:</p> <ul> <li>Key Providers: <code>genie_tooling.security.key_provider.KeyProvider</code><ul> <li>Implement <code>async def get_key(self, key_name: str) -&gt; Optional[str]</code></li> </ul> </li> <li>LLM Providers: <code>genie_tooling.llm_providers.abc.LLMProviderPlugin</code><ul> <li>Implement <code>async def generate(...)</code> and/or <code>async def chat(...)</code>.</li> </ul> </li> <li>Command Processors: <code>genie_tooling.command_processors.abc.CommandProcessorPlugin</code><ul> <li>Implement <code>async def process_command(...)</code>.</li> </ul> </li> <li>Definition Formatters: <code>genie_tooling.definition_formatters.abc.DefinitionFormatter</code><ul> <li>Implement <code>def format(...)</code>.</li> </ul> </li> <li>Caching Providers: <code>genie_tooling.cache_providers.abc.CacheProvider</code><ul> <li>Implement <code>async def get(...)</code>, <code>async def set(...)</code>, <code>async def delete(...)</code>, etc.</li> </ul> </li> <li>Invocation Strategies: <code>genie_tooling.invocation_strategies.abc.InvocationStrategy</code><ul> <li>Implement <code>async def invoke(...)</code>.</li> </ul> </li> <li>Input Validators: <code>genie_tooling.input_validators.abc.InputValidator</code><ul> <li>Implement <code>def validate(...)</code>.</li> </ul> </li> <li>Output Transformers: <code>genie_tooling.output_transformers.abc.OutputTransformer</code><ul> <li>Implement <code>def transform(...)</code>.</li> </ul> </li> <li>Error Handlers &amp; Formatters: <code>genie_tooling.error_handlers.abc.ErrorHandler</code>, <code>genie_tooling.error_formatters.abc.ErrorFormatter</code></li> <li>Log Adapters &amp; Redactors: <code>genie_tooling.log_adapters.abc.LogAdapter</code>, <code>genie_tooling.redactors.abc.Redactor</code></li> <li>Code Executors: <code>genie_tooling.code_executors.abc.CodeExecutor</code></li> <li>Observability Tracers: <code>genie_tooling.observability.abc.InteractionTracerPlugin</code></li> <li>HITL Approvers: <code>genie_tooling.hitl.abc.HumanApprovalRequestPlugin</code></li> <li>Token Usage Recorders: <code>genie_tooling.token_usage.abc.TokenUsageRecorderPlugin</code></li> <li>Guardrail Plugins: <code>genie_tooling.guardrails.abc.InputGuardrailPlugin</code>, <code>OutputGuardrailPlugin</code>, <code>ToolUsageGuardrailPlugin</code></li> <li>Prompt System Plugins: <code>genie_tooling.prompts.abc.PromptRegistryPlugin</code>, <code>PromptTemplatePlugin</code></li> <li>Conversation State Providers: <code>genie_tooling.prompts.conversation.impl.abc.ConversationStateProviderPlugin</code></li> <li>LLM Output Parsers: <code>genie_tooling.prompts.llm_output_parsers.abc.LLMOutputParserPlugin</code></li> <li>Distributed Task Queues: <code>genie_tooling.task_queues.abc.DistributedTaskQueuePlugin</code></li> </ul>"},{"location":"guides/creating_other_plugins/#configuration","title":"Configuration","text":"<p>Once your plugin is created and registered, you can configure it in <code>MiddlewareConfig</code> using the appropriate <code>*_configurations</code> dictionary, keyed by your plugin's <code>plugin_id</code>.</p> <pre><code># In your MiddlewareConfig\napp_config = MiddlewareConfig(\n    # ...\n    cache_provider_configurations={\n        \"my_custom_cache_v1\": {\n            \"connection_string\": \"...\",\n            \"default_ttl\": 300\n        }\n    },\n    default_cache_provider_id=\"my_custom_cache_v1\" # If it should be the default\n    # ...\n)\n</code></pre> <p>By following the relevant protocol and registering your plugin, you can extend Genie Tooling to meet your specific application needs.</p>"},{"location":"guides/creating_plugins/","title":"Creating Plugins (Overview)","text":"<p>Genie Tooling is designed to be highly extensible through its plugin architecture. Almost every functional component can be replaced or augmented with custom implementations.</p> <p>This page provides a general overview. For specific plugin types, refer to: *   Creating Tool Plugins *   Creating RAG Plugins *   Creating Other Plugins (for caches, LLM providers, etc.)</p>"},{"location":"guides/creating_plugins/#core-plugin-protocol","title":"Core Plugin Protocol","text":"<p>All plugins in Genie Tooling should ideally adhere to the <code>genie_tooling.core.types.Plugin</code> protocol:</p> <pre><code>from typing import Protocol, Optional, Dict, Any\n\nclass Plugin(Protocol):\n    @property\n    def plugin_id(self) -&gt; str:\n        \"\"\"A unique identifier for this plugin instance/type.\"\"\"\n        ...\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Optional asynchronous setup method.\n        Called after the plugin is instantiated, with its specific configuration.\n        \"\"\"\n        pass # Default implementation\n\n    async def teardown(self) -&gt; None:\n        \"\"\"\n        Optional asynchronous teardown method.\n        Called when the Genie facade is closing down, to release resources.\n        \"\"\"\n        pass # Default implementation\n</code></pre> <p>Key Requirements for a Plugin Class:</p> <ol> <li> <p><code>plugin_id</code> (Class Attribute):</p> <ul> <li>A unique string identifier for your plugin.</li> <li>Convention: <code>your_plugin_name_v1</code> (e.g., <code>my_custom_llm_provider_v1</code>, <code>advanced_calculator_tool_v1</code>).</li> <li>This ID is used in configuration files and for discovery.</li> </ul> </li> <li> <p>Implement the Specific Protocol:</p> <ul> <li>Your plugin class must implement the methods defined by the protocol for its type (e.g., <code>Tool</code>, <code>LLMProviderPlugin</code>, <code>CacheProviderPlugin</code>).</li> <li>These protocols are typically found in the <code>abc.py</code> file of the relevant submodule (e.g., <code>genie_tooling.tools.abc.Tool</code>).</li> </ul> </li> <li> <p><code>async def setup(self, config: Optional[Dict[str, Any]] = None)</code> (Optional):</p> <ul> <li>If your plugin needs initialization with configuration values, implement this asynchronous method.</li> <li>The <code>config</code> dictionary will contain the settings provided for your plugin's <code>plugin_id</code> in the <code>MiddlewareConfig</code> (e.g., in <code>llm_provider_configurations[\"my_custom_llm_v1\"]</code>).</li> </ul> </li> <li> <p><code>async def teardown(self)</code> (Optional):</p> <ul> <li>If your plugin acquires resources (e.g., network connections, file handles) that need to be released, implement this asynchronous method.</li> </ul> </li> </ol>"},{"location":"guides/creating_plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>Genie's <code>PluginManager</code> discovers plugins through two primary mechanisms:</p> <ol> <li> <p>Entry Points (Recommended for distributable plugins):</p> <ul> <li>Define an entry point in your package's <code>pyproject.toml</code> under the group <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code>.</li> <li>Example:     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_awesome_tool_v1\" = \"my_package.my_module:MyAwesomeToolClass\"\n\"custom_cache_provider_alpha\" = \"my_other_package.cache:CustomCache\"\n</code></pre></li> <li>The key is the <code>plugin_id</code> Genie will use to refer to your plugin.</li> <li>The value is the import path to your plugin class.</li> </ul> </li> <li> <p>Plugin Development Directories (For local/project-specific plugins):</p> <ul> <li>Specify a list of directories in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> <li>The <code>PluginManager</code> will scan these directories for Python files (<code>*.py</code>).</li> <li>It will attempt to import these files as modules and look for classes that implement the <code>Plugin</code> protocol and have a <code>plugin_id</code>.</li> <li>Files starting with <code>_</code> or <code>.</code> (e.g., <code>__init__.py</code>, <code>_internal_utils.py</code>) are ignored.</li> </ul> </li> </ol>"},{"location":"guides/creating_plugins/#configuration-and-usage","title":"Configuration and Usage","text":"<p>Once your plugin is discoverable, you can: *   Configure it: Provide settings in the appropriate <code>*_configurations</code> dictionary within <code>MiddlewareConfig</code>, keyed by your plugin's <code>plugin_id</code>. *   Set it as a default: If applicable, set its <code>plugin_id</code> in fields like <code>default_llm_provider_id</code>, <code>default_cache_provider_id</code>, etc., in <code>MiddlewareConfig</code> or via <code>FeatureSettings</code>. *   Use it explicitly: Some <code>Genie</code> facade methods allow specifying a <code>plugin_id</code> at runtime (e.g., <code>genie.llm.chat(..., provider_id=\"my_llm_v1\")</code>).</p> <p>By adhering to these principles, you can create robust and reusable extensions for Genie Tooling.</p>"},{"location":"guides/creating_rag_plugins/","title":"Creating RAG Plugins","text":"<p>Genie Tooling's Retrieval Augmented Generation (RAG) system is composed of several pluggable components. You can create custom implementations for each to tailor the RAG pipeline to your specific data sources and needs.</p> <p>The core RAG component protocols are:</p> <ul> <li><code>DocumentLoaderPlugin</code>: Loads raw data from a source (e.g., files, web pages, databases) into <code>Document</code> objects.<ul> <li>Located in: <code>genie_tooling.document_loaders.abc</code></li> <li>Key method: <code>async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]</code></li> </ul> </li> <li><code>TextSplitterPlugin</code>: Splits <code>Document</code> objects into smaller <code>Chunk</code> objects.<ul> <li>Located in: <code>genie_tooling.text_splitters.abc</code></li> <li>Key method: <code>async def split(self, documents: AsyncIterable[Document], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Chunk]</code></li> </ul> </li> <li><code>EmbeddingGeneratorPlugin</code>: Generates embedding vectors for <code>Chunk</code> objects.<ul> <li>Located in: <code>genie_tooling.embedding_generators.abc</code></li> <li>Key method: <code>async def embed(self, chunks: AsyncIterable[Chunk], config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Tuple[Chunk, EmbeddingVector]]</code></li> </ul> </li> <li><code>VectorStorePlugin</code>: Stores, manages, and searches <code>Chunk</code> embeddings.<ul> <li>Located in: <code>genie_tooling.vector_stores.abc</code></li> <li>Key methods: <code>async def add(...)</code>, <code>async def search(...)</code>, <code>async def delete(...)</code>.</li> </ul> </li> <li><code>RetrieverPlugin</code>: Orchestrates the process of taking a query, embedding it, and searching a vector store to retrieve relevant chunks. Often composes an <code>EmbeddingGeneratorPlugin</code> and a <code>VectorStorePlugin</code>.<ul> <li>Located in: <code>genie_tooling.retrievers.abc</code></li> <li>Key method: <code>async def retrieve(self, query: str, top_k: int, config: Optional[Dict[str, Any]] = None) -&gt; List[RetrievedChunk]</code></li> </ul> </li> </ul>"},{"location":"guides/creating_rag_plugins/#steps-to-create-a-rag-plugin","title":"Steps to Create a RAG Plugin","text":"<ol> <li>Identify the Component: Determine which part of the RAG pipeline you want to customize (e.g., a new document loader for a specific API, a different text splitting strategy).</li> <li>Implement the Protocol:<ul> <li>Create a Python class that inherits from the relevant plugin protocol (e.g., <code>DocumentLoaderPlugin</code>).</li> <li>Your class must also implicitly or explicitly adhere to the base <code>genie_tooling.core.types.Plugin</code> protocol (by having a <code>plugin_id</code> and optional <code>setup</code>/<code>teardown</code> methods).</li> <li>Implement all required methods from the chosen RAG component protocol. <pre><code># Example: Custom Document Loader\nfrom genie_tooling.core.types import Document\nfrom genie_tooling.document_loaders.abc import DocumentLoaderPlugin\nfrom typing import AsyncIterable, Dict, Any, Optional\n\nclass MyCustomAPILoader(DocumentLoaderPlugin):\n    plugin_id: str = \"my_custom_api_loader_v1\"\n    description: str = \"Loads documents from MyCustomAPI.\"\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None):\n        # Initialize API clients, etc.\n        self.api_endpoint = (config or {}).get(\"api_endpoint\", \"https://api.example.com/data\")\n        # self.key_provider = (config or {}).get(\"key_provider\") # If API key needed\n\n    async def load(self, source_uri: str, config: Optional[Dict[str, Any]] = None) -&gt; AsyncIterable[Document]:\n        # source_uri might be an entity ID or query for your API\n        # api_key = await self.key_provider.get_key(\"MY_CUSTOM_API_KEY\")\n        # ... fetch data from API ...\n        # for item in fetched_data:\n        #     yield Document(content=item['text'], metadata={\"source\": \"MyCustomAPI\", \"id\": item['id']})\n        if False: # Make it an async generator\n             yield\n        pass # Replace with actual implementation\n</code></pre></li> </ul> </li> <li>Register Your Plugin:<ul> <li>Use entry points in <code>pyproject.toml</code> or place it in a <code>plugin_dev_dirs</code> directory.</li> </ul> </li> <li>Configure Genie to Use Your Plugin:<ul> <li>In <code>MiddlewareConfig</code>, update the relevant <code>features</code> settings (e.g., <code>features.rag_loader = \"my_custom_api_loader_v1\"</code>) or explicitly set the default ID (e.g., <code>default_rag_loader_id = \"my_custom_api_loader_v1\"</code>).</li> <li>Provide any necessary configuration for your plugin in the corresponding <code>*_configurations</code> dictionary (e.g., <code>document_loader_configurations[\"my_custom_api_loader_v1\"] = {\"api_endpoint\": \"...\"}</code>).</li> </ul> </li> </ol>"},{"location":"guides/creating_rag_plugins/#example-using-a-custom-rag-component","title":"Example: Using a Custom RAG Component","text":"<p><pre><code># In your MiddlewareConfig\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        rag_loader=\"my_custom_api_loader_v1\", # Use your custom loader\n        rag_embedder=\"st_embedder\",\n        rag_vector_store=\"faiss_vs\"\n    ),\n    document_loader_configurations={\n        \"my_custom_api_loader_v1\": {\n            \"api_endpoint\": \"https://my.service.com/data_source\",\n            # \"key_provider\": my_key_provider_instance (if needed and passed to Genie.create)\n        }\n    }\n)\n\n# genie = await Genie.create(config=app_config, key_provider_instance=my_key_provider_instance)\n# await genie.rag.index_directory(source_uri=\"query_for_my_api\", collection_name=\"custom_data\")\n</code></pre> The <code>RAGManager</code> (used internally by <code>genie.rag</code>) will then pick up and use your custom plugin based on the configuration.</p>"},{"location":"guides/creating_tool_plugins/","title":"Creating Tool Plugins","text":"<p>Tools are fundamental to Genie Tooling, representing actions an agent can perform. You can create custom tools by implementing the <code>Tool</code> protocol.</p>"},{"location":"guides/creating_tool_plugins/#the-tool-protocol","title":"The <code>Tool</code> Protocol","text":"<p>Located in <code>genie_tooling.tools.abc.Tool</code>, the protocol requires:</p> <p><pre><code>from typing import Protocol, Any, Dict\nfrom genie_tooling.core.types import Plugin # For base Plugin behavior\nfrom genie_tooling.security.key_provider import KeyProvider\n\nclass Tool(Plugin, Protocol):\n    @property\n    def identifier(self) -&gt; str:\n        \"\"\"A unique string identifier for this tool.\"\"\"\n        ...\n\n    async def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Returns comprehensive metadata about the tool.\n        Expected structure:\n        {\n            \"identifier\": str,\n            \"name\": str, (Human-friendly)\n            \"description_human\": str, (Detailed for UI/developers)\n            \"description_llm\": str, (Concise for LLM prompts)\n            \"input_schema\": Dict[str, Any], (JSON Schema for parameters)\n            \"output_schema\": Dict[str, Any], (JSON Schema for result)\n            \"key_requirements\": List[Dict[str, str]], (e.g., [{\"name\": \"API_KEY_NAME\", ...}])\n            \"tags\": List[str],\n            \"version\": str,\n            \"cacheable\": bool, (Optional, default False)\n            \"cache_ttl_seconds\": Optional[int] (Optional)\n        }\n        \"\"\"\n        ...\n\n    async def execute(\n        self, \n        params: Dict[str, Any], \n        key_provider: KeyProvider, \n        context: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        Executes the tool with validated parameters.\n        The `context` dictionary is now required and is used to pass system-level\n        information, such as observability trace context.\n        \"\"\"\n        ...\n</code></pre> Your tool class must also have a <code>plugin_id</code> attribute (usually the same as <code>identifier</code>).</p>"},{"location":"guides/creating_tool_plugins/#steps-to-create-a-tool-plugin","title":"Steps to Create a Tool Plugin","text":"<ol> <li> <p>Define Your Class:     <pre><code>from genie_tooling.tools.abc import Tool\nfrom genie_tooling.security.key_provider import KeyProvider\nfrom typing import Dict, Any\n\nclass MyCustomSearchTool(Tool):\n    plugin_id: str = \"my_custom_search_tool_v1\" # Unique plugin ID\n\n    @property\n    def identifier(self) -&gt; str:\n        return self.plugin_id # Often same as plugin_id\n\n    async def setup(self, config: Optional[Dict[str, Any]] = None):\n        self.api_base_url = (config or {}).get(\"api_base_url\", \"https://api.customsearch.com\")\n        # Initialize HTTP client, etc.\n\n    async def get_metadata(self) -&gt; Dict[str, Any]:\n        # ... (metadata definition as before) ...\n        return {\n            \"identifier\": self.identifier,\n            \"name\": \"My Custom Search\",\n            \"description_human\": \"Searches my custom data source.\",\n            \"description_llm\": \"CustomSearch: Finds items in my data. Args: query (str, req), limit (int, opt, default 10).\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query.\"},\n                    \"limit\": {\"type\": \"integer\", \"default\": 10, \"description\": \"Max results.\"}\n                },\n                \"required\": [\"query\"]\n            },\n            \"output_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"results\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}},\n                    \"error\": {\"type\": [\"string\", \"null\"]}\n                },\n                \"required\": [\"results\"]\n            },\n            \"key_requirements\": [{\"name\": \"MY_CUSTOM_API_KEY\", \"description\": \"API key for custom search.\"}],\n            \"tags\": [\"search\", \"custom\"],\n            \"version\": \"1.0.0\"\n        }\n\n    async def execute(\n        self, \n        params: Dict[str, Any], \n        key_provider: KeyProvider, \n        context: Dict[str, Any] # Note: context is now required\n    ) -&gt; Any:\n        query = params[\"query\"]\n        limit = params.get(\"limit\", 10)\n        api_key = await key_provider.get_key(\"MY_CUSTOM_API_KEY\")\n        if not api_key:\n            return {\"results\": [], \"error\": \"API key not found.\"}\n\n        # ... actual search logic using self.api_base_url, query, limit, api_key ...\n        # For example:\n        # response = await self._http_client.get(f\"{self.api_base_url}/search?q={query}&amp;limit={limit}&amp;key={api_key}\")\n        # search_results = response.json() \n        search_results = [{\"title\": f\"Mock result for {query}\"}] # Placeholder\n        return {\"results\": search_results, \"error\": None}\n\n    async def teardown(self):\n        # Close HTTP client, etc.\n        pass\n</code></pre></p> </li> <li> <p>Register Your Plugin:</p> <ul> <li>Add an entry point in <code>pyproject.toml</code>:     <pre><code>[tool.poetry.plugins.\"genie_tooling.plugins\"]\n\"my_custom_search_tool_v1\" = \"my_package.tools:MyCustomSearchTool\"\n</code></pre></li> <li>Or, place the Python file in a directory listed in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> </ul> </li> <li> <p>Enable and Configure in <code>MiddlewareConfig</code>:     <pre><code>app_config = MiddlewareConfig(\n    # ...\n    tool_configurations={\n        \"my_custom_search_tool_v1\": { # This is the plugin_id\n            \"api_base_url\": \"https://prod.customsearch.com/v2\" # Configuration for its setup()\n        }\n    }\n    # ...\n)\n</code></pre> Important: Your tool will only be loaded and available if its <code>plugin_id</code> is a key in the <code>tool_configurations</code> dictionary. An empty dictionary <code>{}</code> as the value is sufficient if no specific configuration is needed for <code>setup()</code>.</p> </li> </ol> <p>Now, your <code>MyCustomSearchTool</code> can be used by <code>genie.execute_tool(\"my_custom_search_tool_v1\", ...)</code> or selected by a command processor.</p>"},{"location":"guides/distributed_tasks/","title":"Using Distributed Task Queues","text":"<p>Genie Tooling supports offloading tasks, such as long-running tool executions, to distributed task queues like Celery or RQ. This allows your main agent loop to remain responsive.</p>"},{"location":"guides/distributed_tasks/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>TaskQueueInterface</code> (<code>genie.task_queue</code>): Facade for submitting and managing distributed tasks.</li> <li><code>DistributedTaskQueuePlugin</code>: Plugin for a specific task queue system.<ul> <li>Built-in: <code>CeleryTaskQueuePlugin</code> (alias: <code>celery_task_queue</code>), <code>RedisQueueTaskPlugin</code> (alias: <code>rq_task_queue</code>).</li> </ul> </li> <li><code>DistributedTaskInvocationStrategy</code>: An <code>InvocationStrategy</code> that uses a task queue plugin to execute tools. (Note: This strategy is currently more conceptual and may require further application-side setup for robust use.)</li> </ul>"},{"location":"guides/distributed_tasks/#configuration","title":"Configuration","text":"<p>Enable and configure your chosen task queue system via <code>FeatureSettings</code>:</p> <p><pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\n# Example for Celery\napp_config_celery = MiddlewareConfig(\n    features=FeatureSettings(\n        task_queue=\"celery\", \n        task_queue_celery_broker_url=\"redis://localhost:6379/1\", # Your Celery broker\n        task_queue_celery_backend_url=\"redis://localhost:6379/2\", # Your Celery result backend\n    ),\n    # Optional: Further Celery-specific configurations\n    distributed_task_queue_configurations={\n        \"celery_task_queue_v1\": { # Canonical ID\n            \"celery_app_name\": \"my_genie_worker_app\",\n            # \"celery_include_task_paths\": [\"my_project.worker_tasks\"] # Paths for Celery to find tasks\n        }\n    }\n)\n\n# Example for RQ (Redis Queue)\napp_config_rq = MiddlewareConfig(\n    features=FeatureSettings(\n        task_queue=\"rq\" \n        # RQ typically uses a single Redis connection, configured in its plugin settings\n    ),\n    distributed_task_queue_configurations={\n        \"redis_queue_task_plugin_v1\": { # Canonical ID\n            \"redis_url\": \"redis://localhost:6379/3\", # Redis for RQ\n            \"default_queue_name\": \"genie-rq-default\"\n        }\n    }\n)\n</code></pre> Ensure your chosen task queue broker (e.g., Redis, RabbitMQ) is running and accessible.</p>"},{"location":"guides/distributed_tasks/#using-genietask_queue-direct-task-submission","title":"Using <code>genie.task_queue</code> (Direct Task Submission)","text":"<p>This interface allows you to directly submit tasks to the configured queue. You need to have tasks defined and registered with your Celery/RQ worker environment.</p> <pre><code># Assuming 'genie' is initialized with a task queue configured (e.g., Celery)\n# and you have a Celery task defined as 'my_project.tasks.long_computation'\n\ntask_id = await genie.task_queue.submit_task(\n    task_name=\"my_project.tasks.long_computation\", \n    args=(10, 20),\n    kwargs={\"operation\": \"multiply\"},\n    task_options={\"countdown\": 5} # Example: Celery-specific option\n)\n\nif task_id:\n    print(f\"Task submitted with ID: {task_id}\")\n\n    # Poll for status and result (example polling loop)\n    for _ in range(30): # Try for 30 seconds\n        status = await genie.task_queue.get_task_status(task_id)\n        print(f\"Task {task_id} status: {status}\")\n        if status == \"success\":\n            result = await genie.task_queue.get_task_result(task_id)\n            print(f\"Task result: {result}\")\n            break\n        elif status in [\"failure\", \"revoked\"]:\n            print(f\"Task failed or was revoked.\")\n            # Optionally try to get error details if it failed\n            # result = await genie.task_queue.get_task_result(task_id) \n            break\n        await asyncio.sleep(1)\nelse:\n    print(\"Failed to submit task.\")\n</code></pre>"},{"location":"guides/distributed_tasks/#using-distributedtaskinvocationstrategy-for-tools-conceptual","title":"Using <code>DistributedTaskInvocationStrategy</code> for Tools (Conceptual)","text":"<p>The <code>DistributedTaskInvocationStrategy</code> aims to automatically offload tool executions to a task queue. This is a more advanced setup.</p> <ol> <li>Configure Task Queue: Ensure a task queue (e.g., Celery) is configured in <code>FeatureSettings</code>.</li> <li>Worker Task: Define a generic task in your Celery/RQ worker environment that can execute Genie tools. This task would typically:<ul> <li>Receive <code>tool_id</code>, <code>tool_params</code>, and potentially serialized key information or context.</li> <li>Instantiate a minimal Genie environment (or have one pre-configured).</li> <li>Execute the specified tool with the given parameters.</li> <li>Return the tool's result.</li> <li>Example task name: <code>genie_tooling.worker_tasks.execute_genie_tool_task</code> (this is a placeholder; you'd implement this).</li> </ul> </li> <li>Configure the Strategy: In <code>MiddlewareConfig</code>, you would configure the <code>DistributedTaskInvocationStrategy</code> to use your chosen task queue plugin and specify the name of your generic worker task.     <pre><code># In MiddlewareConfig:\n# invocation_strategy_configurations={\n#     \"distributed_task_invocation_strategy_v1\": {\n#         \"task_queue_plugin_id\": \"celery_task_queue_v1\", # Or \"redis_queue_task_plugin_v1\"\n#         \"worker_tool_execution_task_name\": \"my_project.worker_tasks.execute_genie_tool_task\"\n#     }\n# }\n</code></pre></li> <li>Execute Tool with Strategy:     <pre><code># result_or_task_ref = await genie.execute_tool(\n#     \"my_long_running_tool\",\n#     param1=\"value\",\n#     strategy_id=\"distributed_task_invocation_strategy_v1\" \n#     # The strategy might return a task ID or future immediately,\n#     # or poll internally and return the final result.\n# )\n</code></pre></li> </ol> <p>Note on Security and Context: Securely passing API keys and sensitive context to distributed workers requires careful design. Options include: *   Workers fetching credentials themselves from a secure vault. *   Using short-lived, task-specific credentials. *   Carefully serializing only necessary and safe context information.</p> <p>The direct <code>genie.task_queue.submit_task</code> method gives you more control over what gets sent to the worker.</p>"},{"location":"guides/installation/","title":"Installation","text":"<p>This guide explains how to install Genie Tooling and its dependencies.</p>"},{"location":"guides/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher.</li> <li>Poetry for dependency management and packaging.</li> </ul>"},{"location":"guides/installation/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the Repository (Optional, if contributing or running examples locally):     If you want to work with the source code or run the examples directly from the repository:     <pre><code>git clone https://github.com/genie-tooling/genie-tooling.git\ncd genie-tooling\n</code></pre></p> </li> <li> <p>Install using Poetry:</p> <ul> <li> <p>To install the core library and all optional dependencies (recommended for full functionality and running all examples):     Navigate to the cloned repository directory (if applicable) or where your project that depends on Genie Tooling is located.     <pre><code>poetry install --all-extras\n</code></pre></p> </li> <li> <p>To install only the core library dependencies: <pre><code>poetry install\n</code></pre>     This will install the essential components of Genie Tooling. Some plugins requiring external libraries (e.g., specific LLM providers, vector stores, or tools) will not be functional unless their respective optional dependencies are installed.</p> </li> <li> <p>To install specific optional dependencies (extras):     You can install only the extras you need. Extras are defined in the <code>pyproject.toml</code> file.     For example, to install support for Ollama, OpenAI, Qdrant, Celery, and the internal Llama.cpp provider:     <pre><code>poetry install --extras \"ollama openai qdrant celery llama_cpp_internal\"\n</code></pre>     Common extras include:</p> <ul> <li><code>web_tools</code>: For tools that interact with web pages (e.g., <code>WebPageLoader</code>, <code>GoogleSearchTool</code>).</li> <li><code>openai_services</code>: For the OpenAI LLM provider and embedding generator.</li> <li><code>local_rag</code>: For local RAG components like Sentence Transformers and FAISS.</li> <li><code>distributed_rag</code>: For distributed RAG components like ChromaDB and Qdrant clients.</li> <li><code>ollama</code>: For the Ollama LLM provider.</li> <li><code>gemini</code>: For the Google Gemini LLM provider.</li> <li><code>secure_exec</code>: For the <code>SecureDockerExecutor</code>.</li> <li><code>observability</code>: For OpenTelemetry tracing and metrics.</li> <li><code>prompts</code>: For advanced prompt templating engines like Jinja2.</li> <li><code>task_queues</code>: For distributed task queue support (Celery, RQ).</li> <li><code>llama_cpp_server</code>: For the Llama.cpp server-based LLM provider.</li> <li><code>llama_cpp_internal</code>: For the Llama.cpp internal (direct library use) LLM provider.</li> </ul> </li> <li> <p>Adding Genie Tooling as a dependency to your existing project:     If you are integrating Genie Tooling into your own Poetry-managed project, you can add it:     <pre><code>poetry add genie-tooling # For the core library\n\n# To add with specific extras:\npoetry add genie-tooling -E ollama -E local_rag \n# or\npoetry add genie-tooling[ollama,local_rag]\n</code></pre></p> </li> </ul> </li> </ol>"},{"location":"guides/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can try running one of the examples, such as the simple Ollama chat example (ensure Ollama is running):</p> <pre><code># From the root of the genie-tooling repository if cloned:\npoetry run python examples/E02_ollama_chat_example.py \n</code></pre> <p>If the example runs without import errors and interacts with Ollama successfully, your basic installation is working.</p>"},{"location":"guides/logging/","title":"Logging","text":"<p>Genie Tooling uses a pluggable <code>LogAdapterPlugin</code> system for its internal structured event logging. This allows application developers to integrate Genie's detailed operational events into their existing logging infrastructure or use specialized logging libraries. The <code>InteractionTracingManager</code> (and by extension, tracers like <code>ConsoleTracerPlugin</code>) relies on the configured <code>LogAdapterPlugin</code> to process and output trace events.</p>"},{"location":"guides/logging/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>LogAdapterPlugin</code> (<code>genie_tooling.log_adapters.abc.LogAdapter</code>): The protocol for plugins that handle structured log events. They are responsible for final formatting, redaction, and output.</li> <li><code>DefaultLogAdapter</code> (alias: <code>default_log_adapter</code>): The standard built-in adapter. It uses Python's <code>logging</code> module and can integrate a <code>RedactorPlugin</code> for sanitizing data.</li> <li><code>PyviderTelemetryLogAdapter</code> (alias: <code>pyvider_log_adapter</code>): An adapter that integrates with the <code>pyvider-telemetry</code> library for advanced structured logging, including emoji-based DAS (Domain-Action-Status) logging.</li> <li>Library Logger Name: The root logger name for messages originating directly from Genie Tooling library components (when not processed through a <code>LogAdapterPlugin</code>) is <code>genie_tooling</code>.</li> </ul>"},{"location":"guides/logging/#configuring-the-log-adapter","title":"Configuring the Log Adapter","text":"<p>You select and configure the primary <code>LogAdapterPlugin</code> using <code>FeatureSettings</code> or explicit configurations in <code>MiddlewareConfig</code>.</p> <p>1. Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        logging_adapter=\"default_log_adapter\", # Default\n        # OR\n        # logging_adapter=\"pyvider_log_adapter\",\n        # logging_pyvider_service_name=\"my-genie-app-with-pyvider\" # Optional for Pyvider\n    ),\n    # Configuration for the chosen adapter goes into log_adapter_configurations\n    log_adapter_configurations={\n        \"default_log_adapter_v1\": { # If default_log_adapter is chosen\n            \"log_level\": \"DEBUG\", # For the 'genie_tooling' logger\n            \"library_logger_name\": \"genie_tooling_app_logs\", # Custom name for library logs\n            \"redactor_plugin_id\": \"schema_aware_redactor_v1\", # Example custom redactor\n            \"enable_schema_redaction\": True,\n            \"enable_key_name_redaction\": True\n        },\n        \"pyvider_telemetry_log_adapter_v1\": { # If pyvider_log_adapter is chosen\n            \"service_name\": \"MyGenieService\", # Overrides feature setting if both present\n            \"default_level\": \"INFO\", # For Pyvider's default logger\n            \"module_levels\": {\"genie_tooling.llm_providers\": \"DEBUG\"},\n            \"console_formatter\": \"json\",\n            \"redactor_plugin_id\": \"noop_redactor_v1\" # Pyvider might have its own redaction\n        }\n    }\n)\n</code></pre> <p>2. Explicit Configuration:</p> <pre><code>app_config = MiddlewareConfig(\n    default_log_adapter_id=\"pyvider_telemetry_log_adapter_v1\", # Explicitly set\n    log_adapter_configurations={\n        \"pyvider_telemetry_log_adapter_v1\": {\n            \"service_name\": \"MyExplicitPyviderServiceName\",\n            \"default_level\": \"TRACE\",\n            # ... other Pyvider specific settings ...\n            # \"redactor_plugin_id\": \"my_custom_redactor_for_pyvider_v1\"\n        }\n    }\n)\n</code></pre> <p>The <code>Genie.create()</code> method will instantiate the configured <code>LogAdapterPlugin</code> (passing the main <code>PluginManager</code> in its configuration, allowing it to load its own redactor) and then pass this <code>LogAdapterPlugin</code> instance to the <code>InteractionTracingManager</code>. Tracers like <code>ConsoleTracerPlugin</code> will then use this central <code>LogAdapterPlugin</code> to process their trace events.</p>"},{"location":"guides/logging/#redaction","title":"Redaction","text":"<p>Both <code>DefaultLogAdapter</code> and the new <code>PyviderTelemetryLogAdapter</code> (as per the plan) will integrate with a <code>RedactorPlugin</code> to sanitize sensitive data before logging. *   The <code>DefaultLogAdapter</code> has built-in schema-aware redaction which can be toggled, and it also uses the configured <code>RedactorPlugin</code>. *   The <code>PyviderTelemetryLogAdapter</code> will also use a configured <code>RedactorPlugin</code> for a first pass of redaction before handing data to Pyvider's logging methods.</p>"},{"location":"guides/logging/#standard-python-logging","title":"Standard Python Logging","text":"<p>If you need to configure logging for Genie Tooling library modules that log directly (not through the <code>LogAdapterPlugin</code> event system), you can still use the standard Python <code>logging</code> module:</p> <p><pre><code>import logging\n\n# Get the Genie Tooling library logger\nlibrary_logger = logging.getLogger(\"genie_tooling\")\nlibrary_logger.setLevel(logging.DEBUG) \nconsole_handler = logging.StreamHandler()\nformatter = logging.Formatter('%(asctime)s - %(name)s - [%(levelname)s] - %(message)s (%(module)s:%(lineno)d)')\nconsole_handler.setFormatter(formatter)\nif not any(isinstance(h, logging.StreamHandler) for h in library_logger.handlers):\n    library_logger.addHandler(console_handler)\n</code></pre> This standard setup is separate from the <code>LogAdapterPlugin</code> system, which is for structured event processing. The <code>DefaultLogAdapter</code> bridges these by configuring the <code>genie_tooling</code> logger as part of its setup.</p>"},{"location":"guides/observability_tracing/","title":"Observability and Interaction Tracing (<code>genie.observability</code>)","text":"<p>Genie Tooling is designed for production use, and a key requirement for any production system is deep visibility into its internal workings. The framework is heavily instrumented to provide \"zero-effort\" observability. By simply enabling a tracer plugin, you can get detailed, correlated traces for the entire lifecycle of an agentic operation, from command processing and tool lookup to LLM calls and tool execution.</p>"},{"location":"guides/observability_tracing/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>ObservabilityInterface</code> (<code>genie.observability</code>): The facade interface for tracing events.</li> <li><code>InteractionTracerPlugin</code>: A plugin responsible for handling trace events.<ul> <li>Built-in:<ul> <li><code>ConsoleTracerPlugin</code> (alias: <code>console_tracer</code>): Prints trace events to the console.</li> <li><code>OpenTelemetryTracerPlugin</code> (alias: <code>otel_tracer</code>): Exports traces using the OpenTelemetry SDK to backends like Jaeger, SigNoz, etc.</li> </ul> </li> </ul> </li> <li><code>LogAdapterPlugin</code>: A plugin that formats trace events for output. The <code>ConsoleTracerPlugin</code> delegates its formatting to the configured <code>LogAdapterPlugin</code> (e.g., <code>DefaultLogAdapter</code> or <code>PyviderTelemetryLogAdapter</code>). This decouples tracing from formatting.</li> <li><code>TraceEvent</code> (TypedDict): The raw, structured data for a trace event.</li> <li>Correlation ID: A unique ID that links all events within a single top-level operation (e.g., one <code>genie.run_command()</code> call), allowing observability backends to construct a complete trace hierarchy.</li> </ul>"},{"location":"guides/observability_tracing/#automatic-tracing","title":"Automatic Tracing","text":"<p>When a tracer is enabled, the framework automatically emits detailed events for: *   Facade Calls: Start and end of <code>genie.run_command</code>, <code>genie.execute_tool</code>, <code>genie.rag.index_*</code>, <code>genie.rag.search</code>, etc. *   Command Processing: The full lifecycle, including which tools were considered (<code>tool_lookup.end</code>), the prompt sent to the LLM (<code>command_processor.llm_assisted.prompt_context_ready</code>), and the LLM's parsed decision (<code>command_processor.llm_assisted.result</code>). *   Tool Invocation: Caching checks (<code>invocation.cache.hit/miss</code>), input validation, the actual tool execution (<code>tool.execute.start/end</code>), and output transformation. *   LLM Calls: All <code>genie.llm.chat/generate</code> calls, including the provider used, all parameters (<code>temperature</code>, <code>model</code>, etc.), and token usage from the response.</p>"},{"location":"guides/observability_tracing/#configuration","title":"Configuration","text":"<p>Configure the default interaction tracer via <code>FeatureSettings</code>.</p>"},{"location":"guides/observability_tracing/#example-1-simple-console-tracing","title":"Example 1: Simple Console Tracing","text":"<p>This is the easiest way to see what's happening during development.</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        observability_tracer=\"console_tracer\",\n    ),\n    # Optional: Configure the log adapter used by the console tracer\n    log_adapter_configurations={\n        \"default_log_adapter_v1\": {\n            \"log_level\": \"DEBUG\" # Set the level for the 'genie_tooling' logger\n        }\n    }\n)\n</code></pre>"},{"location":"guides/observability_tracing/#example-2-opentelemetry-tracing-for-jaeger-signoz-etc","title":"Example 2: OpenTelemetry Tracing (for Jaeger, SigNoz, etc.)","text":"<p>This configuration exports traces to an OpenTelemetry collector.</p> <pre><code># Prerequisite: Start an OTel collector (e.g., Jaeger all-in-one)\n# docker run -d --name jaeger -p 16686:16686 -p 4318:4318 jaegertracing/all-in-one:latest\n# With this setup, running your Genie application will send detailed traces to Jaeger, which you can view at `http://localhost:16686`.\n\n## Application-Level Tracing\n\nBeyond the automatic framework traces, Genie provides tools to seamlessly integrate your application's logic into the same trace.\n=======\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        observability_tracer=\"otel_tracer\",\n        # The endpoint for the OTLP/HTTP exporter\n        observability_otel_endpoint=\"http://localhost:4318/v1/traces\"\n    ),\n    observability_tracer_configurations={\n        \"otel_tracer_plugin_v1\": { # Canonical ID\n            \"otel_service_name\": \"my-genie-agent-app\",\n            \"otel_service_version\": \"1.2.3\",\n            \"resource_attributes\": {\"deployment.environment\": \"staging\"}\n        }\n    }\n)\n</code></pre>"},{"location":"guides/observability_tracing/#simplified-tracing-with-the-traceable-decorator","title":"Simplified Tracing with the <code>@traceable</code> Decorator","text":"<p>For the common case of tracing an entire function call, Genie provides the <code>@traceable</code> decorator. This is the recommended approach for instrumenting functions within your tools.</p> <p>How it works: *   It automatically creates a new OpenTelemetry span when the decorated function is called. *   It links this new span to the parent span found in the <code>context</code> argument. *   It records function arguments as span attributes. *   It automatically records exceptions and sets the span status to <code>ERROR</code>.</p> <p>Example:</p> <pre><code>from genie_tooling import tool\nfrom genie_tooling.observability import traceable\nfrom typing import Dict, Any\n\n@traceable\nasync def _perform_database_query(query: str, context: Dict[str, Any]):\n    # A span for '_perform_database_query' is automatically created\n    # and linked to the parent 'get_user_data.execute' span.\n    # The 'query' argument will be added as an attribute to the span.\n    # ... database logic ...\n    return {\"id\": 123, \"name\": \"John Doe\"}\n\n@tool\nclass UserDataTool:\n    # ... (plugin_id, get_metadata, etc.) ...\n    async def execute(self, params: Dict[str, Any], key_provider: Any, context: Dict[str, Any]) -&gt; Any:\n        user_id = params.get(\"user_id\")\n\n        # The context dictionary received here contains the OTel context,\n        # which is automatically passed to the @traceable function.\n        db_result = await _perform_database_query(\n            query=f\"SELECT * FROM users WHERE id={user_id}\",\n            context=context\n        )\n        return db_result\n</code></pre>"},{"location":"guides/observability_tracing/#context-propagation-and-auto-instrumentation","title":"Context Propagation and Auto-Instrumentation","text":"<p>The <code>@traceable</code> decorator works because Genie automatically propagates the OpenTelemetry <code>Context</code> object. When Genie's <code>ToolInvoker</code> calls your tool's <code>execute</code> method, the <code>context</code> dictionary it passes now contains a special key, <code>otel_context</code>.</p> <p>This seamless context propagation means that standard OpenTelemetry auto-instrumentation libraries (e.g., <code>opentelemetry-instrumentation-httpx</code>, <code>opentelemetry-instrumentation-psycopg2</code>) will work out-of-the-box. If your traceable function makes a call using an instrumented library, that library will automatically create a child span, giving you an incredibly detailed, end-to-end trace with zero extra effort. While most tracing is automatic, you can add custom events from your own application logic using <code>await genie.observability.trace_event(...)</code>.</p> <p><pre><code>import uuid\n# correlation_id = str(uuid.uuid4()) # Start a new logical operation\n# await genie.observability.trace_event(\n#     event_name=\"my_app.custom_process.start\",\n#     data={\"input_id\": \"123\", \"user_category\": \"premium\"},\n#     component=\"MyCustomModule\",\n#     correlation_id=correlation_id\n# )\n</code></pre> This allows your application-specific events to appear within the same trace as the framework's internal events, providing a complete picture.</p>"},{"location":"guides/plugin_architecture/","title":"Plugin Architecture","text":"<p>Genie Tooling is built upon a highly modular and extensible plugin architecture. This design principle allows developers to easily swap, customize, or add new functionalities without altering the core framework.</p>"},{"location":"guides/plugin_architecture/#core-idea","title":"Core Idea","text":"<p>The central idea is that most significant functionalities within Genie are implemented as plugins. Each plugin adheres to a specific protocol (an interface defined using Python's <code>typing.Protocol</code> or an Abstract Base Class). This ensures that different implementations of a particular functionality can be used interchangeably as long as they conform to the defined contract.</p>"},{"location":"guides/plugin_architecture/#key-components","title":"Key Components","text":"<ol> <li> <p><code>Plugin</code> Protocol (<code>genie_tooling.core.types.Plugin</code>):     This is the base protocol that all plugins should ideally implement. It defines:</p> <ul> <li><code>plugin_id</code> (property): A unique string identifier for the plugin.</li> <li><code>async def setup(config)</code> (optional): For initialization with configuration.</li> <li><code>async def teardown()</code> (optional): For resource cleanup.</li> </ul> </li> <li> <p>Specific Plugin Protocols:     For each type of extensible functionality, there's a more specific protocol that inherits from or includes the base <code>Plugin</code> requirements. Examples:</p> <ul> <li><code>Tool</code> (<code>genie_tooling.tools.abc.Tool</code>)</li> <li><code>LLMProviderPlugin</code> (<code>genie_tooling.llm_providers.abc.LLMProviderPlugin</code>)</li> <li><code>CacheProvider</code> (<code>genie_tooling.cache_providers.abc.CacheProvider</code>)</li> <li><code>DocumentLoaderPlugin</code> (<code>genie_tooling.document_loaders.abc.DocumentLoaderPlugin</code>)</li> <li>And many more (see <code>src/genie_tooling/.../abc.py</code> files).</li> </ul> </li> <li> <p><code>PluginManager</code> (<code>genie_tooling.core.plugin_manager.PluginManager</code>):</p> <ul> <li>Discovery: Responsible for finding available plugin classes. It searches:<ul> <li>Entry Points: Defined in <code>pyproject.toml</code> under the <code>[tool.poetry.plugins.\"genie_tooling.plugins\"]</code> group. This is the standard way for third-party libraries or separate modules to provide plugins.</li> <li>Plugin Development Directories: Specified in <code>MiddlewareConfig.plugin_dev_dirs</code>. Useful for project-specific plugins or during development.</li> </ul> </li> <li>Instantiation &amp; Setup: When a plugin is requested (e.g., by a manager or the <code>Genie</code> facade), the <code>PluginManager</code> instantiates the plugin class and calls its <code>async setup(config)</code> method, passing any relevant configuration.</li> <li>Caching: It typically caches instantiated plugins to avoid re-creating and re-setting them up on every request.</li> <li>Teardown: Manages the <code>async teardown()</code> lifecycle of loaded plugins.</li> </ul> </li> <li> <p>Managers (e.g., <code>ToolManager</code>, <code>LLMProviderManager</code>, <code>RAGManager</code>):</p> <ul> <li>These components orchestrate plugins of a specific type.</li> <li>They use the <code>PluginManager</code> to get instances of the plugins they need based on configuration.</li> <li>For example, <code>LLMProviderManager</code> uses <code>PluginManager</code> to load the configured <code>LLMProviderPlugin</code> (like <code>OllamaLLMProviderPlugin</code> or <code>OpenAILLMProviderPlugin</code>).</li> </ul> </li> <li> <p><code>Genie</code> Facade:</p> <ul> <li>The <code>Genie</code> facade simplifies interaction with the underlying managers and, by extension, the plugins.</li> <li>When you configure <code>Genie</code> (e.g., <code>features.llm = \"ollama\"</code>), it internally directs the <code>LLMProviderManager</code> to load and use the \"ollama\" LLM provider plugin.</li> </ul> </li> </ol>"},{"location":"guides/plugin_architecture/#benefits-of-the-plugin-architecture","title":"Benefits of the Plugin Architecture","text":"<ul> <li>Extensibility: Easily add new LLM providers, tools, data sources, caching backends, etc., without modifying Genie's core code.</li> <li>Customization: Replace default implementations with your own specialized versions.</li> <li>Modularity: Keeps different functionalities decoupled, making the system easier to understand, maintain, and test.</li> <li>Community Contributions: Facilitates sharing and using plugins developed by the community.</li> <li>Flexibility: Allows applications to select and configure only the components they need.</li> </ul>"},{"location":"guides/plugin_architecture/#how-to-create-a-plugin","title":"How to Create a Plugin","text":"<ol> <li>Identify the Protocol: Find the relevant plugin protocol in <code>genie_tooling</code> (e.g., <code>Tool</code> for a new tool, <code>CacheProvider</code> for a new cache).</li> <li>Implement the Class: Create a Python class that:<ul> <li>Defines a unique <code>plugin_id</code> class attribute.</li> <li>Implements all methods and properties required by the chosen protocol.</li> <li>Optionally implements <code>async setup(config)</code> and <code>async teardown()</code>.</li> </ul> </li> <li>Register the Plugin:<ul> <li>For distributable plugins: Add an entry point in your <code>pyproject.toml</code>.</li> <li>For local plugins: Place the file in a directory specified in <code>MiddlewareConfig.plugin_dev_dirs</code>.</li> </ul> </li> <li>Configure Genie: Update your <code>MiddlewareConfig</code> to use your new plugin, either by setting it as a default or by providing its configuration in the relevant <code>*_configurations</code> dictionary. For tools, ensure they are enabled in <code>tool_configurations</code>.</li> </ol> <p>Refer to the specific \"Creating ... Plugins\" guides for more detailed instructions on particular plugin types.</p>"},{"location":"guides/simplified_configuration/","title":"Simplified Configuration with FeatureSettings and Aliases","text":"<p>Genie Tooling aims to make common configurations straightforward through <code>FeatureSettings</code> and a system of plugin ID aliases. This guide explains these concepts in detail.</p>"},{"location":"guides/simplified_configuration/#featuresettings","title":"<code>FeatureSettings</code>","text":"<p>The <code>FeatureSettings</code> model, part of <code>MiddlewareConfig</code>, provides high-level toggles for major functionalities. When you use <code>Genie.create(config=MiddlewareConfig(features=...))</code>, the internal <code>ConfigResolver</code> uses these settings to:</p> <ol> <li>Set default plugin IDs for various components (e.g., <code>default_llm_provider_id</code>, <code>default_rag_embedder_id</code>, <code>default_log_adapter_id</code>).</li> <li>Populate basic configurations for these default plugins in the respective <code>*_configurations</code> dictionaries (e.g., setting the model name for the chosen LLM provider, or service name for Pyvider log adapter).</li> </ol> <p>Example of <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.features import FeatureSettings\n\nfeatures = FeatureSettings(\n    # LLM Provider\n    llm=\"ollama\",                             # Chooses OllamaLLMProviderPlugin\n    llm_ollama_model_name=\"mistral:7b-instruct-q4_K_M\", # Sets model for Ollama\n    # Or for Llama.cpp Internal:\n    # llm=\"llama_cpp_internal\",\n    # llm_llama_cpp_internal_model_path=\"/path/to/your/model.gguf\",\n    # llm_llama_cpp_internal_n_gpu_layers=-1,\n    # llm_llama_cpp_internal_chat_format=\"mistral\",\n\n    # Command Processing\n    command_processor=\"llm_assisted\",         # Chooses LLMAssistedToolSelectionProcessorPlugin\n    command_processor_formatter_id_alias=\"compact_text_formatter\", # Formatter for LLM prompt\n\n    # Tool Lookup\n    tool_lookup=\"hybrid\",                  # Uses HybridSearchLookupProvider (Recommended)\n    tool_lookup_embedder_id_alias=\"st_embedder\", # Embedder for tool lookup\n    tool_lookup_formatter_id_alias=\"compact_text_formatter\", # Formatter for tool indexing\n    # tool_lookup_chroma_path=\"./my_tool_lookup_db\", # Optional: Use ChromaDB for tool lookup embeddings\n\n    # RAG\n    rag_embedder=\"openai\",                    # Uses OpenAIEmbeddingGenerator for RAG\n    rag_vector_store=\"chroma\",                # Uses ChromaDBVectorStore for RAG\n    rag_vector_store_chroma_path=\"./my_rag_vector_db\",\n    rag_vector_store_chroma_collection_name=\"main_rag_docs\",\n\n    # Caching\n    cache=\"redis\",                            # Uses RedisCacheProvider\n    cache_redis_url=\"redis://localhost:6379/1\",\n\n    # Logging Adapter (NEW)\n    logging_adapter=\"pyvider_log_adapter\",      # Use Pyvider Telemetry\n    logging_pyvider_service_name=\"my-genie-service\", # Service name for Pyvider\n\n    # Observability &amp; Monitoring\n    observability_tracer=\"otel_tracer\",       # Use OpenTelemetry for traces\n    observability_otel_endpoint=\"http://localhost:4318/v1/traces\", # OTLP/HTTP endpoint\n    token_usage_recorder=\"otel_metrics_recorder\", # Use OpenTelemetry for token metrics\n\n    # Human-in-the-Loop\n    hitl_approver=\"cli_hitl_approver\",        # Use CLI for approvals\n\n    # Guardrails\n    input_guardrails=[\"keyword_blocklist_guardrail\"], # List of guardrail aliases/IDs\n\n    # Prompt System &amp; Conversation\n    prompt_registry=\"file_system_prompt_registry\",\n    prompt_template_engine=\"jinja2_chat_formatter\",\n    conversation_state_provider=\"in_memory_convo_provider\",\n    default_llm_output_parser=\"pydantic_output_parser\",\n\n    # Distributed Tasks\n    task_queue=\"celery\",                      # Use Celery\n    task_queue_celery_broker_url=\"redis://localhost:6379/3\",\n    task_queue_celery_backend_url=\"redis://localhost:6379/4\",\n    # Or for RQ:\n    # task_queue=\"rq\",\n    # (RQ typically uses the same Redis for broker/backend, configured via redis_url in its plugin config)\n)\n\n# This 'features' object would be passed to MiddlewareConfig:\n# from genie_tooling.config.models import MiddlewareConfig\n# app_config = MiddlewareConfig(features=features)\n</code></pre>"},{"location":"guides/simplified_configuration/#plugin-id-aliases","title":"Plugin ID Aliases","text":"<p>To make <code>FeatureSettings</code> and explicit configurations more readable, Genie uses a system of aliases. The <code>ConfigResolver</code> maps these short aliases to their full, canonical plugin IDs.</p> <p>Commonly Used Aliases (Examples):</p> <ul> <li>LLM Providers:<ul> <li><code>\"ollama\"</code>: <code>\"ollama_llm_provider_v1\"</code></li> <li><code>\"openai\"</code>: <code>\"openai_llm_provider_v1\"</code></li> <li><code>\"gemini\"</code>: <code>\"gemini_llm_provider_v1\"</code></li> <li><code>\"llama_cpp\"</code>: <code>\"llama_cpp_llm_provider_v1\"</code> (for server)</li> <li><code>\"llama_cpp_internal\"</code>: <code>\"llama_cpp_internal_llm_provider_v1\"</code></li> </ul> </li> <li>Key Provider:<ul> <li><code>\"env_keys\"</code>: <code>\"environment_key_provider_v1\"</code> (Default if no KeyProvider instance/ID is given)</li> </ul> </li> <li>Caching Providers:<ul> <li><code>\"in_memory_cache\"</code>: <code>\"in_memory_cache_provider_v1\"</code></li> <li><code>\"redis_cache\"</code>: <code>\"redis_cache_provider_v1\"</code></li> </ul> </li> <li>Embedding Generators (Embedders):<ul> <li><code>\"st_embedder\"</code>: <code>\"sentence_transformer_embedder_v1\"</code></li> <li><code>\"openai_embedder\"</code>: <code>\"openai_embedding_generator_v1\"</code></li> </ul> </li> <li>Vector Stores:<ul> <li><code>\"faiss_vs\"</code>: <code>\"faiss_vector_store_v1\"</code></li> <li><code>\"chroma_vs\"</code>: <code>\"chromadb_vector_store_v1\"</code></li> <li><code>\"qdrant_vs\"</code>: <code>\"qdrant_vector_store_v1\"</code></li> </ul> </li> <li>Tool Lookup Providers:<ul> <li><code>\"embedding_lookup\"</code>: <code>\"embedding_similarity_lookup_v1\"</code></li> <li><code>\"keyword_lookup\"</code>: <code>\"keyword_match_lookup_v1\"</code></li> <li><code>\"hybrid_lookup\"</code>: <code>\"hybrid_search_lookup_v1\"</code></li> </ul> </li> <li>Definition Formatters:<ul> <li><code>\"compact_text_formatter\"</code>: <code>\"compact_text_formatter_plugin_v1\"</code></li> <li><code>\"openai_func_formatter\"</code>: <code>\"openai_function_formatter_plugin_v1\"</code></li> <li><code>\"hr_json_formatter\"</code>: <code>\"human_readable_json_formatter_plugin_v1\"</code></li> </ul> </li> <li>Command Processors:<ul> <li><code>\"llm_assisted_cmd_proc\"</code>: <code>\"llm_assisted_tool_selection_processor_v1\"</code></li> <li><code>\"simple_keyword_cmd_proc\"</code>: <code>\"simple_keyword_processor_v1\"</code></li> </ul> </li> <li>Log Adapters (NEW):<ul> <li><code>\"default_log_adapter\"</code>: <code>\"default_log_adapter_v1\"</code></li> <li><code>\"pyvider_log_adapter\"</code>: <code>\"pyvider_telemetry_log_adapter_v1\"</code></li> </ul> </li> <li>Observability &amp; Monitoring:<ul> <li><code>\"console_tracer\"</code>: <code>\"console_tracer_plugin_v1\"</code></li> <li><code>\"otel_tracer\"</code>: <code>\"otel_tracer_plugin_v1\"</code></li> <li><code>\"in_memory_token_recorder\"</code>: <code>\"in_memory_token_usage_recorder_v1\"</code></li> <li><code>\"otel_metrics_recorder\"</code>: <code>\"otel_metrics_token_recorder_v1\"</code></li> </ul> </li> <li>HITL &amp; Guardrails:<ul> <li><code>\"cli_hitl_approver\"</code>: <code>\"cli_approval_plugin_v1\"</code></li> <li><code>\"keyword_blocklist_guardrail\"</code>: <code>\"keyword_blocklist_guardrail_v1\"</code></li> </ul> </li> <li>Prompt System &amp; Conversation:<ul> <li><code>\"file_system_prompt_registry\"</code>: <code>\"file_system_prompt_registry_v1\"</code></li> <li><code>\"basic_string_formatter\"</code>: <code>\"basic_string_format_template_v1\"</code></li> <li><code>\"jinja2_chat_formatter\"</code>: <code>\"jinja2_chat_template_v1\"</code></li> <li><code>\"in_memory_convo_provider\"</code>: <code>\"in_memory_conversation_state_v1\"</code></li> <li><code>\"redis_convo_provider\"</code>: <code>\"redis_conversation_state_v1\"</code></li> </ul> </li> <li>LLM Output Parsers:<ul> <li><code>\"json_output_parser\"</code>: <code>\"json_output_parser_v1\"</code></li> <li><code>\"pydantic_output_parser\"</code>: <code>\"pydantic_output_parser_v1\"</code></li> </ul> </li> <li>Task Queues:<ul> <li><code>\"celery_task_queue\"</code>: <code>\"celery_task_queue_v1\"</code></li> <li><code>\"rq_task_queue\"</code>: <code>\"redis_queue_task_plugin_v1\"</code></li> </ul> </li> </ul> <p>(This list is not exhaustive. Refer to <code>src/genie_tooling/config/resolver.py</code> for the complete <code>PLUGIN_ID_ALIASES</code> dictionary.)</p>"},{"location":"guides/simplified_configuration/#overriding-feature-derived-settings","title":"Overriding Feature-Derived Settings","text":"<p>You can always provide more specific configurations that will take precedence over what <code>FeatureSettings</code> implies.</p> <p>1. Overriding Default Plugin IDs:</p> <p>If a feature sets a default (e.g., <code>features.llm = \"openai\"</code> sets <code>default_llm_provider_id</code> to <code>\"openai_llm_provider_v1\"</code>), you can override this directly:</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(llm=\"openai\", logging_adapter=\"default_log_adapter\"),\n    default_llm_provider_id=\"my_special_openai_provider_v3\", # Explicit override\n    default_log_adapter_id=\"pyvider_telemetry_log_adapter_v1\" # Explicit override\n)\n</code></pre> <p>2. Overriding Plugin Configurations:</p> <p>Use the <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code> to provide settings for specific plugins. You can use either the canonical ID or an alias as the key.</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\", # Feature sets this model\n        logging_adapter=\"pyvider_log_adapter\",\n        logging_pyvider_service_name=\"FeatureServiceName\"\n    ),\n    llm_provider_configurations={\n        \"ollama\": { # Using alias 'ollama'\n            \"model_name\": \"llama3:8b-instruct-fp16\", # Override the model\n            \"request_timeout_seconds\": 300.0,\n            # Other Ollama-specific settings\n        }\n    },\n    log_adapter_configurations={\n        \"pyvider_telemetry_log_adapter_v1\": { # Using canonical ID\n            \"service_name\": \"ExplicitPyviderServiceName\", # Overrides feature setting\n            \"default_level\": \"DEBUG\"\n        }\n    }\n)\n</code></pre> <p>In this example: *   The default LLM is Ollama. *   The <code>mistral:latest</code> model set by <code>FeatureSettings</code> for Ollama is overridden by <code>llama3:8b-instruct-fp16</code>. *   The default Log Adapter is Pyvider. *   The <code>service_name</code> for Pyvider set by <code>FeatureSettings</code> is overridden by the explicit configuration.</p> <p>This layered approach (Features -&gt; Explicit Defaults -&gt; Explicit Plugin Configs) provides both ease of use for common cases and fine-grained control when needed.</p>"},{"location":"guides/token_usage_tracking/","title":"Token Usage Tracking (<code>genie.usage</code>)","text":"<p>Genie Tooling provides an interface for tracking token usage by LLM providers, accessible via <code>genie.usage</code>. This helps in monitoring costs and understanding LLM consumption patterns.</p>"},{"location":"guides/token_usage_tracking/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>UsageTrackingInterface</code> (<code>genie.usage</code>): The facade interface for recording and summarizing token usage.</li> <li><code>TokenUsageRecorderPlugin</code>: A plugin responsible for storing or exporting token usage records.<ul> <li>Built-in:<ul> <li><code>InMemoryTokenUsageRecorderPlugin</code> (alias: <code>in_memory_token_recorder</code>): Stores records in memory. Useful for simple summaries and testing.</li> <li><code>OpenTelemetryMetricsTokenRecorderPlugin</code> (alias: <code>otel_metrics_recorder</code>): Recommended for production. Emits token counts as standard OpenTelemetry metrics, which can be scraped by systems like Prometheus.</li> </ul> </li> </ul> </li> <li><code>TokenUsageRecord</code> (TypedDict): The structure for a single token usage event.</li> </ul>"},{"location":"guides/token_usage_tracking/#configuration","title":"Configuration","text":"<p>Configure the default token usage recorder via <code>FeatureSettings</code>.</p>"},{"location":"guides/token_usage_tracking/#example-1-in-memory-recorder-for-development","title":"Example 1: In-Memory Recorder (for Development)","text":"<pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        token_usage_recorder=\"in_memory_token_recorder\"\n    )\n)\n</code></pre>"},{"location":"guides/token_usage_tracking/#example-2-opentelemetry-metrics-recorder-for-production","title":"Example 2: OpenTelemetry Metrics Recorder (for Production)","text":"<p>This is the recommended approach for production monitoring. It requires an OpenTelemetry collector setup.</p> <pre><code># Prerequisite: An OTel collector that can scrape Prometheus metrics.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        token_usage_recorder=\"otel_metrics_recorder\",\n        # The OTel SDK must be initialized, which is done by the OTel tracer.\n        # So, enable the tracer, even if you only care about metrics.\n        observability_tracer=\"otel_tracer\",\n    ),\n    # Configure the OTel tracer to initialize the SDK\n    observability_tracer_configurations={\n        \"otel_tracer_plugin_v1\": {\n            \"otel_service_name\": \"my-app-with-token-metrics\",\n            \"exporter_type\": \"console\" # Or your preferred OTel trace exporter\n        }\n    }\n)\n</code></pre>"},{"location":"guides/token_usage_tracking/#how-it-works","title":"How It Works","text":"<p>Token usage is automatically recorded by <code>genie.llm.chat()</code> and <code>genie.llm.generate()</code> whenever the underlying LLM provider returns usage information.</p>"},{"location":"guides/token_usage_tracking/#using-the-in-memory-recorder","title":"Using the In-Memory Recorder","text":"<p>If you use <code>\"in_memory_token_recorder\"</code>, you can get a simple summary:</p> <pre><code># Assuming genie is initialized with the in-memory recorder\nsummary = await genie.usage.get_summary()\n\n# Example output:\n# {\n#   \"in_memory_token_usage_recorder_v1\": {\n#     \"total_records\": 5,\n#     \"total_prompt_tokens\": 1234,\n#     \"total_completion_tokens\": 567,\n#     \"total_tokens_overall\": 1801,\n#     \"by_model\": {\n#       \"mistral:latest\": { \"prompt\": 1234, \"completion\": 567, \"total\": 1801, \"count\": 5 }\n#     }\n#   }\n# }\n</code></pre>"},{"location":"guides/token_usage_tracking/#using-the-opentelemetry-metrics-recorder","title":"Using the OpenTelemetry Metrics Recorder","text":"<p>When <code>token_usage_recorder=\"otel_metrics_recorder\"</code> is configured, this plugin emits the following OTel metrics: *   <code>llm.request.tokens.prompt</code> (Counter, unit: <code>{token}</code>) *   <code>llm.request.tokens.completion</code> (Counter, unit: <code>{token}</code>) *   <code>llm.request.tokens.total</code> (Counter, unit: <code>{token}</code>)</p> <p>These metrics will have attributes (labels) like <code>llm.provider.id</code>, <code>llm.model.name</code>, and <code>llm.call_type</code>. Configure an OTel collector (e.g., with a Prometheus exporter) to scrape and visualize these metrics in a dashboarding tool like Grafana.</p> <p>Example PromQL Queries: *   Total prompt tokens per model (rate over 5m): <code>sum(rate(llm_request_tokens_prompt_total[5m])) by (llm_model_name)</code> *   Total completion tokens per provider (rate over 5m): <code>sum(rate(llm_request_tokens_completion_total[5m])) by (llm_provider_id)</code></p>"},{"location":"guides/tool_lookup/","title":"Tool Lookup","text":"<p>Genie Tooling refers to the process of finding relevant tools based on a natural language query. This is primarily used by the <code>LLMAssistedToolSelectionProcessorPlugin</code> to narrow down the list of tools presented to the LLM, making the LLM's selection task more efficient and accurate.</p>"},{"location":"guides/tool_lookup/#how-tool-lookup-is-used","title":"How Tool Lookup is Used","text":"<p>When you configure the <code>Genie</code> facade to use the <code>llm_assisted</code> command processor, you can also enable and configure tool lookup:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        command_processor=\"llm_assisted\",\n\n        # Tool Lookup Configuration\n        tool_lookup=\"hybrid\", # Enable hybrid (embedding + keyword) search\n        tool_lookup_top_k=5,  # How many tools to show the LLM\n    ),\n    tool_configurations={\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {} \n    }\n)\n</code></pre> <p>The <code>LLMAssistedToolSelectionProcessorPlugin</code> internally uses the <code>ToolLookupService</code> to: 1.  Index Tools: When first needed, it takes all enabled tools, formats their definitions, and indexes them using the configured <code>ToolLookupProvider</code>. This now happens incrementally when new tools are registered. 2.  Find Tools: When processing a user command, it queries the <code>ToolLookupService</code> with the command. The service returns a ranked list of potentially relevant tools. 3.  Filter Tools: The processor then takes the top N tools (defined by <code>tool_lookup_top_k</code>) from this list and presents only their definitions to the LLM for final selection.</p>"},{"location":"guides/tool_lookup/#available-tool-lookup-providers","title":"Available Tool Lookup Providers","text":"<p>Genie Tooling includes built-in providers:</p> <ul> <li><code>EmbeddingSimilarityLookupProvider</code> (alias: <code>\"embedding\"</code>):<ul> <li>Embeds tool definitions and queries for semantic search.</li> <li>Can use an in-memory NumPy index or a persistent <code>VectorStorePlugin</code>.</li> </ul> </li> <li><code>KeywordMatchLookupProvider</code> (alias: <code>\"keyword\"</code>):<ul> <li>Performs simple keyword matching. Stateless and fast.</li> </ul> </li> <li><code>HybridSearchLookupProvider</code> (alias: <code>\"hybrid\"</code>):<ul> <li>Recommended: Combines the results of both embedding and keyword search using Reciprocal Rank Fusion (RRF).</li> <li>This provides a powerful balance of semantic understanding (from embeddings) and lexical precision (from keywords), often yielding the most relevant results.</li> </ul> </li> </ul>"},{"location":"guides/tool_lookup/#incremental-indexing","title":"Incremental Indexing","text":"<p>The <code>ToolLookupService</code> now supports incremental indexing. When you register new tools at runtime with <code>await genie.register_tool_functions(...)</code>, the service will automatically add or update just those tools in the index, rather than rebuilding the entire index from scratch. This is crucial for performance in dynamic environments.</p>"},{"location":"guides/tool_lookup/#debugging-tool-discoverability","title":"Debugging Tool Discoverability","text":"<p>It can sometimes be challenging to understand why a specific tool was or was not selected for a query. To aid in this, Genie Tooling provides a developer utility.</p> <p><code>scripts/debug_tool_lookup.py</code></p> <p>This command-line script allows you to test queries directly against the <code>ToolLookupService</code> and see the ranked results.</p> <p>Usage:</p> <ol> <li> <p>(Optional) Create a simple config file, e.g., <code>debug_config.yaml</code>, especially if you use <code>plugin_dev_dirs</code>:     <pre><code>plugin_dev_dirs:\n  - ./src/my_project/custom_plugins\nfeatures:\n  tool_lookup: hybrid # Or 'embedding', 'keyword'\n</code></pre></p> </li> <li> <p>Run the script from your project's root directory:     <pre><code>poetry run python scripts/debug_tool_lookup.py --query \"what is the weather in Paris\" --config-path debug_config.yaml\n</code></pre></p> </li> </ol> <p>Output:</p> <p>The script will print a ranked list of tools, including their scores and any matched keywords or similarity details, helping you refine your tool descriptions for better discoverability.</p> <p><pre><code>--- Tool Lookup Debugger ---\nQuery: 'what is the weather in Paris'\nTop K: 5\n...\n--- Lookup Results ---\n\n1. Tool: open_weather_map_tool (Score: 1.6541)\n   Matched Keywords: weather\n   Similarity Details: {'cosine_similarity': 0.85}\n   Snippet/Reason: WeatherInfo: Get current weather for a city...\n\n2. Tool: some_other_tool (Score: 0.0164)\n   ...\n</code></pre> This allows you to iterate quickly on your tool's <code>description_llm</code> and <code>tags</code> to improve its ranking for relevant queries.</p>"},{"location":"guides/using_command_processors/","title":"Using Command Processors","text":"<p>Command Processors in Genie Tooling are responsible for interpreting a user's natural language command, selecting an appropriate tool, and extracting the necessary parameters for that tool. The primary way to use a command processor is via the <code>genie.run_command()</code> method.</p>"},{"location":"guides/using_command_processors/#genierun_command","title":"<code>genie.run_command()</code>","text":"<pre><code>async def run_command(\n    self, \n    command: str, \n    processor_id: Optional[str] = None,\n    conversation_history: Optional[List[ChatMessage]] = None\n) -&gt; Any:\n</code></pre> <ul> <li><code>command</code>: The natural language command string from the user.</li> <li><code>processor_id</code>: Optional. The ID of the <code>CommandProcessorPlugin</code> to use. If <code>None</code>, the default command processor configured in <code>MiddlewareConfig</code> (via <code>features.command_processor</code> or <code>default_command_processor_id</code>) is used.</li> <li><code>conversation_history</code>: Optional list of previous <code>ChatMessage</code> dictionaries to provide context.</li> </ul> <p>The method returns a dictionary, typically including: *   <code>tool_result</code>: The result from the executed tool, if a tool was chosen and run successfully. *   <code>thought_process</code>: An explanation from the processor (especially LLM-based ones). *   <code>error</code>: An error message if processing or tool execution failed. *   <code>message</code>: A message if, for example, no tool was selected. *   <code>hitl_decision</code>: If HITL was triggered, this contains the approval response.</p> <p>Important: Any tool that a command processor might select must be enabled in <code>MiddlewareConfig.tool_configurations</code>.</p>"},{"location":"guides/using_command_processors/#configuring-command-processors","title":"Configuring Command Processors","text":"<p>You select and configure command processors using <code>FeatureSettings</code> or explicit configurations in <code>MiddlewareConfig</code>.</p>"},{"location":"guides/using_command_processors/#1-simple-keyword-processor-simple_keyword","title":"1. Simple Keyword Processor (<code>simple_keyword</code>)","text":"<p>This processor matches keywords in the user's command against a predefined map to select a tool. It then prompts the user for parameters if the selected tool requires them.</p> <p>Configuration via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        command_processor=\"simple_keyword\"\n    ),\n    # Explicit configuration for the simple_keyword_processor_v1\n    command_processor_configurations={\n        \"simple_keyword_processor_v1\": { # Canonical ID\n            \"keyword_map\": {\n                \"calculate\": \"calculator_tool\",\n                \"math\": \"calculator_tool\",\n                \"weather\": \"open_weather_map_tool\"\n            },\n            \"keyword_priority\": [\"calculate\", \"math\", \"weather\"] # Order for matching\n        }\n    },\n    tool_configurations={ # Tools must be enabled\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {}\n    }\n)\n# genie = await Genie.create(config=app_config)\n# result = await genie.run_command(\"calculate 10 + 5\") \n# The processor will prompt for num1, num2, operation for calculator_tool.\n</code></pre>"},{"location":"guides/using_command_processors/#2-llm-assisted-processor-llm_assisted","title":"2. LLM-Assisted Processor (<code>llm_assisted</code>)","text":"<p>This processor uses an LLM to understand the command, select the most appropriate tool from a list of available tools, and extract its parameters.</p> <p>Configuration via <code>FeatureSettings</code>:</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\", # The LLM to be used by the processor\n        llm_ollama_model_name=\"mistral:latest\",\n\n        command_processor=\"llm_assisted\",\n\n        # Configure how tools are presented to the LLM\n        command_processor_formatter_id_alias=\"compact_text_formatter\", \n\n        # Configure tool lookup to help the LLM (optional but recommended)\n        tool_lookup=\"embedding\", # Use embedding-based lookup\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\", # Formatter for indexing tools\n        tool_lookup_embedder_id_alias=\"st_embedder\" # Embedder for tool descriptions\n    ),\n    # Optionally, provide specific settings for the LLM-assisted processor\n    command_processor_configurations={\n        \"llm_assisted_tool_selection_processor_v1\": { # Canonical ID\n            \"tool_lookup_top_k\": 3, # Show top 3 tools from lookup to the LLM\n            # \"system_prompt_template\": \"Your custom system prompt...\" # Override default prompt\n        }\n    },\n    tool_configurations={ # Any tool the LLM might pick must be enabled\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {} \n        # Add other tools the LLM might be expected to use\n    }\n)\n# genie = await Genie.create(config=app_config)\n# result = await genie.run_command(\"What's the weather like in Berlin tomorrow?\")\n</code></pre> <p>Key aspects for <code>llm_assisted</code> processor: *   LLM Dependency: It requires a configured LLM provider (<code>features.llm</code>). *   Tool Formatting: It uses a <code>DefinitionFormatterPlugin</code> (specified by <code>command_processor_formatter_id_alias</code> or an explicit <code>tool_formatter_id</code> in its configuration) to format tool definitions for the LLM prompt. *   Tool Lookup (Optional but Recommended): If <code>features.tool_lookup</code> is enabled (e.g., <code>\"embedding\"</code> or <code>\"keyword\"</code>), the processor first uses the <code>ToolLookupService</code> to find a smaller set of relevant tools. Only these candidate tools are then presented to the LLM. This improves efficiency and accuracy. The <code>tool_lookup_top_k</code> parameter in its configuration controls how many tools from the lookup are passed to the LLM.</p> <p>See the Tool Lookup Guide for more on configuring tool lookup.</p>"},{"location":"guides/using_conversation_state/","title":"Managing Conversation State (<code>genie.conversation</code>)","text":"<p>Genie Tooling provides an interface for managing conversation state, primarily the history of messages in a chat interaction. This is accessible via <code>genie.conversation</code>.</p>"},{"location":"guides/using_conversation_state/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>ConversationInterface</code> (<code>genie.conversation</code>): The facade interface for all conversation state operations.</li> <li><code>ConversationStateProviderPlugin</code>: Responsible for storing, retrieving, and deleting conversation states. Each state is typically identified by a unique <code>session_id</code>.<ul> <li>Built-in: <code>InMemoryStateProviderPlugin</code> (alias: <code>in_memory_convo_provider</code>), <code>RedisStateProviderPlugin</code> (alias: <code>redis_convo_provider</code>).</li> </ul> </li> <li><code>ConversationState</code> (TypedDict): The structure for conversation data:     <pre><code>from typing import List, Dict, Optional, Any\nfrom genie_tooling.llm_providers.types import ChatMessage # Assuming ChatMessage is defined\n\nclass ConversationState(TypedDict):\n    session_id: str\n    history: List[ChatMessage]\n    metadata: Optional[Dict[str, Any]] \n</code></pre></li> </ul>"},{"location":"guides/using_conversation_state/#configuration","title":"Configuration","text":"<p>You configure the default conversation state provider via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code> settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        conversation_state_provider=\"in_memory_convo_provider\" # Default\n        # OR for Redis:\n        # conversation_state_provider=\"redis_convo_provider\" \n    ),\n    # Example: Configure the RedisStateProviderPlugin if chosen\n    # conversation_state_provider_configurations={\n    #     \"redis_conversation_state_v1\": { # Canonical ID\n    #         \"redis_url\": \"redis://localhost:6379/2\",\n    #         \"key_prefix\": \"my_app_cs:\",\n    #         \"default_ttl_seconds\": 7200 # 2 hours\n    #     }\n    # }\n)\n</code></pre> <p>Explicit Configuration:</p> <pre><code>app_config = MiddlewareConfig(\n    default_conversation_state_provider_id=\"in_memory_conversation_state_v1\",\n    # conversation_state_provider_configurations={ ... }\n)\n</code></pre>"},{"location":"guides/using_conversation_state/#using-genieconversation","title":"Using <code>genie.conversation</code>","text":""},{"location":"guides/using_conversation_state/#1-loading-conversation-state","title":"1. Loading Conversation State","text":"<pre><code># Assuming 'genie' is an initialized Genie instance\nsession_id = \"user123_chat_abc\"\nstate = await genie.conversation.load_state(session_id)\n# state = await genie.conversation.load_state(session_id, provider_id=\"custom_store\") # Optional\n\nif state:\n    print(f\"Loaded history for {session_id}: {state['history']}\")\nelse:\n    print(f\"No existing state for {session_id}.\")\n</code></pre>"},{"location":"guides/using_conversation_state/#2-adding-a-message-to-a-session","title":"2. Adding a Message to a Session","text":"<p>This method handles loading the existing state (or creating a new one if it doesn't exist), appending the message, and saving the updated state.</p> <p><pre><code>from genie_tooling.llm_providers.types import ChatMessage\n\nsession_id = \"user123_chat_abc\"\nnew_user_message: ChatMessage = {\"role\": \"user\", \"content\": \"What's new?\"}\n\nawait genie.conversation.add_message(session_id, new_user_message)\n# await genie.conversation.add_message(session_id, new_user_message, provider_id=\"custom_store\")\n\n# If you then get an assistant response:\n# assistant_response: ChatMessage = {\"role\": \"assistant\", \"content\": \"Not much, how about you?\"}\n# await genie.conversation.add_message(session_id, assistant_response)\n</code></pre> The <code>add_message</code> method automatically updates a <code>last_updated</code> timestamp in the state's metadata. If it's a new session, it also adds a <code>created_at</code> timestamp.</p>"},{"location":"guides/using_conversation_state/#3-saving-conversation-state-explicitly","title":"3. Saving Conversation State (Explicitly)","text":"<p>While <code>add_message</code> handles saving, you can explicitly save a state if you've modified it directly (e.g., adding custom metadata).</p> <pre><code>from genie_tooling.prompts.conversation.types import ConversationState # Correct import\n\nsession_id = \"user123_chat_abc\"\ncurrent_state = await genie.conversation.load_state(session_id)\nif not current_state:\n    current_state = ConversationState(session_id=session_id, history=[], metadata={})\n\n# Modify state\ncurrent_state[\"history\"].append({\"role\": \"system\", \"content\": \"Context updated.\"})\nif current_state.get(\"metadata\") is None: current_state[\"metadata\"] = {}\ncurrent_state[\"metadata\"][\"custom_flag\"] = True \n\nawait genie.conversation.save_state(current_state)\n</code></pre>"},{"location":"guides/using_conversation_state/#4-deleting-conversation-state","title":"4. Deleting Conversation State","text":"<pre><code>session_id = \"user123_chat_to_delete\"\nwas_deleted = await genie.conversation.delete_state(session_id)\nif was_deleted:\n    print(f\"State for session {session_id} deleted.\")\nelse:\n    print(f\"State for session {session_id} not found or delete failed.\")\n</code></pre>"},{"location":"guides/using_conversation_state/#creating-custom-conversation-state-providers","title":"Creating Custom Conversation State Providers","text":"<p>Implement the <code>ConversationStateProviderPlugin</code> protocol, defining <code>load_state</code>, <code>save_state</code>, and <code>delete_state</code> methods to interact with your chosen backend (database, file, etc.). Register your plugin via entry points or <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_guardrails/","title":"Using Guardrails","text":"<p>Guardrails in Genie Tooling provide a mechanism to enforce policies and safety checks on inputs, outputs, and tool usage attempts. They are implemented as plugins and integrated into the core <code>Genie</code> facade operations.</p>"},{"location":"guides/using_guardrails/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>GuardrailManager</code>: Orchestrates the execution of different types of guardrail plugins.</li> <li>Guardrail Plugin Types:<ul> <li><code>InputGuardrailPlugin</code>: Checks data provided to the system or an LLM (e.g., user prompts, chat messages).</li> <li><code>OutputGuardrailPlugin</code>: Checks data produced by the system or an LLM (e.g., LLM responses, tool execution results before final output).</li> <li><code>ToolUsageGuardrailPlugin</code>: Checks if a specific tool usage attempt (tool + parameters) is permissible before execution.</li> </ul> </li> <li><code>GuardrailViolation</code> (TypedDict): The result of a guardrail check:     <pre><code>from typing import Literal, Optional, Dict, Any, TypedDict\n\nclass GuardrailViolation(TypedDict):\n    action: Literal[\"allow\", \"block\", \"warn\"]\n    reason: Optional[str]\n    guardrail_id: Optional[str]\n    details: Optional[Dict[str, Any]]\n</code></pre><ul> <li><code>allow</code>: The check passed.</li> <li><code>block</code>: The operation should be prevented.</li> <li><code>warn</code>: The operation can proceed, but a warning should be logged or noted.</li> </ul> </li> </ul>"},{"location":"guides/using_guardrails/#configuration","title":"Configuration","text":"<p>Guardrails are configured in <code>MiddlewareConfig</code>, primarily through <code>FeatureSettings</code> for enabling lists of guardrails, and <code>guardrail_configurations</code> for specific plugin settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        input_guardrails=[\"keyword_blocklist_guardrail\"], # Enable by alias\n        output_guardrails=[\"keyword_blocklist_guardrail\"],\n        # tool_usage_guardrails=[\"my_custom_tool_usage_policy_v1\"] # Example\n    ),\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": { # Canonical ID\n            \"blocklist\": [\"unsafe_topic\", \"banned_phrase\"],\n            \"case_sensitive\": False,\n            \"action_on_match\": \"block\" # or \"warn\"\n        },\n        # \"my_custom_tool_usage_policy_v1\": { ... }\n    }\n)\n</code></pre> <ul> <li><code>features.input_guardrails</code>, <code>features.output_guardrails</code>, <code>features.tool_usage_guardrails</code>: Lists of plugin IDs or aliases for guardrails to activate for each category.</li> <li><code>guardrail_configurations</code>: A dictionary where keys are canonical guardrail plugin IDs (or aliases) and values are their specific configuration dictionaries.</li> </ul>"},{"location":"guides/using_guardrails/#implicit-integration","title":"Implicit Integration","text":"<p>Guardrails are automatically invoked by relevant <code>Genie</code> facade methods:</p> <ul> <li>Input Guardrails:<ul> <li>Checked by <code>genie.llm.chat()</code> and <code>genie.llm.generate()</code> before sending data to the LLM.</li> <li>Checked by <code>genie.run_command()</code> on the user's command string before processing.</li> </ul> </li> <li>Output Guardrails:<ul> <li>Checked by <code>genie.llm.chat()</code> and <code>genie.llm.generate()</code> on the LLM's response before returning it.</li> <li>Checked by <code>genie.execute_tool()</code> (via the invocation strategy) on the raw tool result before transformation.</li> </ul> </li> <li>Tool Usage Guardrails:<ul> <li>Checked by <code>genie.execute_tool()</code> (via the invocation strategy) before the tool's <code>execute()</code> method is called.</li> <li>Checked by <code>genie.run_command()</code> after a tool and its parameters have been determined by the command processor, but before execution and before HITL (if HITL is also active).</li> </ul> </li> </ul> <p>Behavior on \"block\": *   If an input guardrail blocks, the operation (e.g., LLM call) is prevented, and a <code>PermissionError</code> is typically raised by the <code>Genie</code> facade method. *   If an output guardrail blocks, the original output is replaced with a message indicating it was blocked (e.g., <code>\"[RESPONSE BLOCKED: Reason]\"</code>). *   If a tool usage guardrail blocks, the tool execution is prevented, and an error is typically returned by <code>genie.run_command()</code> or <code>genie.execute_tool()</code>.</p>"},{"location":"guides/using_guardrails/#built-in-guardrails","title":"Built-in Guardrails","text":"<ul> <li><code>KeywordBlocklistGuardrailPlugin</code> (alias: <code>keyword_blocklist_guardrail</code>):<ul> <li>Implements <code>InputGuardrailPlugin</code> and <code>OutputGuardrailPlugin</code>.</li> <li>Checks text data against a configurable list of keywords.</li> <li>Configuration:<ul> <li><code>blocklist</code> (List[str]): Keywords to block/warn on.</li> <li><code>case_sensitive</code> (bool, default: <code>False</code>): Whether matching is case-sensitive.</li> <li><code>action_on_match</code> (Literal[\"block\", \"warn\"], default: <code>\"block\"</code>): Action to take if a keyword is found.</li> </ul> </li> </ul> </li> </ul>"},{"location":"guides/using_guardrails/#creating-custom-guardrail-plugins","title":"Creating Custom Guardrail Plugins","text":"<ol> <li> <p>Choose the Base Protocol:</p> <ul> <li><code>InputGuardrailPlugin</code>: Implement <code>async def check_input(self, data: Any, context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>.</li> <li><code>OutputGuardrailPlugin</code>: Implement <code>async def check_output(self, data: Any, context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>.</li> <li><code>ToolUsageGuardrailPlugin</code>: Implement <code>async def check_tool_usage(self, tool: Tool, params: Dict[str, Any], context: Optional[Dict[str, Any]]) -&gt; GuardrailViolation</code>. A single plugin class can implement multiple of these protocols if it's designed to check different types of data.</li> </ul> </li> <li> <p>Implement the Check Logic: Your method should analyze the <code>data</code> (and <code>context</code> or <code>tool</code>/<code>params</code>) and return a <code>GuardrailViolation</code> dictionary.</p> </li> <li> <p>Register Your Plugin: Use entry points in <code>pyproject.toml</code> or place it in a directory specified by <code>plugin_dev_dirs</code> in <code>MiddlewareConfig</code>.</p> </li> <li> <p>Configure It: Add its ID to the appropriate list in <code>features</code> (e.g., <code>features.input_guardrails</code>) and provide any necessary configuration in <code>guardrail_configurations</code>.</p> </li> </ol>"},{"location":"guides/using_human_in_loop/","title":"Human-in-the-Loop (HITL) Approvals (<code>genie.human_in_loop</code>)","text":"<p>Genie Tooling supports Human-in-the-Loop (HITL) workflows, allowing critical actions or ambiguous decisions to be paused for human review and approval before proceeding. This is primarily managed via the <code>genie.human_in_loop</code> interface.</p>"},{"location":"guides/using_human_in_loop/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>HITLInterface</code> (<code>genie.human_in_loop</code>): The facade interface for requesting human approval.</li> <li><code>HumanApprovalRequestPlugin</code>: A plugin responsible for presenting an approval request to a human and collecting their response.<ul> <li>Built-in: <code>CliApprovalPlugin</code> (alias: <code>cli_hitl_approver</code>) - Prompts on the command line.</li> </ul> </li> <li><code>ApprovalRequest</code> (TypedDict): Data structure for an approval request:     <pre><code>from typing import Literal, Optional, Dict, Any, TypedDict\n\nclass ApprovalRequest(TypedDict):\n    request_id: str\n    prompt: str # Message shown to the human\n    data_to_approve: Dict[str, Any] # Data needing approval\n    context: Optional[Dict[str, Any]]\n    timeout_seconds: Optional[int]\n</code></pre></li> <li><code>ApprovalResponse</code> (TypedDict): Data structure for the human's response:     <pre><code>class ApprovalResponse(TypedDict):\n    request_id: str\n    status: Literal[\"pending\", \"approved\", \"denied\", \"timeout\", \"error\"]\n    approver_id: Optional[str]\n    reason: Optional[str] # Reason for denial or comments\n    timestamp: Optional[float]\n</code></pre></li> </ul>"},{"location":"guides/using_human_in_loop/#configuration","title":"Configuration","text":"<p>Configure the default HITL approval plugin via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code>.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        hitl_approver=\"cli_hitl_approver\" # Use CLI for approvals\n    ),\n    # Example: Configure the CliApprovalPlugin (though it has no specific config by default)\n    # hitl_approver_configurations={\n    #     \"cli_approval_plugin_v1\": {} \n    # }\n)\n</code></pre>"},{"location":"guides/using_human_in_loop/#integration-with-genierun_command","title":"Integration with <code>genie.run_command()</code>","text":"<p>The most common use case for HITL is to require approval before a tool selected by <code>genie.run_command()</code> is executed. If an <code>HITLManager</code> is active (i.e., <code>features.hitl_approver</code> is not <code>\"none\"</code>), <code>genie.run_command()</code> will automatically: 1.  Determine the tool and parameters. 2.  Create an <code>ApprovalRequest</code> with the tool ID and parameters. 3.  Call <code>genie.human_in_loop.request_approval()</code>. 4.  If approved, execute the tool. If denied or timed out, return an error/message.</p> <p>Example: If <code>hitl_approver</code> is set to <code>\"cli_hitl_approver\"</code> in <code>FeatureSettings</code> and <code>calculator_tool</code> is enabled in <code>tool_configurations</code>: <pre><code># genie = await Genie.create(config=app_config_with_hitl)\n# command_result = await genie.run_command(\"What is 15 times 7?\") \n\n# The CLI will prompt:\n# --- HUMAN APPROVAL REQUIRED ---\n# Request ID: &lt;uuid&gt;\n# Prompt: Approve execution of tool 'calculator_tool' with params: {'num1': 15, 'num2': 7, 'operation': 'multiply'} for goal 'What is 15 times 7?'?\n# Data/Action: {'tool_id': 'calculator_tool', 'params': {'num1': 15, 'num2': 7, 'operation': 'multiply'}, 'step_reasoning': None}\n# Approve? (yes/no/y/n): \n</code></pre> If the user types \"no\" and provides a reason, <code>command_result</code> might look like: <pre><code>{\n  \"error\": \"Tool execution denied by HITL: User intervention required.\",\n  \"thought_process\": \"The LLM selected calculator_tool for 15 times 7.\",\n  \"hitl_decision\": {\n    \"request_id\": \"...\", \"status\": \"denied\", \"approver_id\": \"cli_user\", \n    \"reason\": \"User intervention required.\", \"timestamp\": ...\n  }\n}\n</code></pre></p>"},{"location":"guides/using_human_in_loop/#manual-approval-requests-with-geniehuman_in_looprequest_approval","title":"Manual Approval Requests with <code>genie.human_in_loop.request_approval()</code>","text":"<p>You can also manually trigger an approval request from your custom logic:</p> <pre><code>from genie_tooling.hitl.types import ApprovalRequest\nimport uuid\n\nrequest_details = ApprovalRequest(\n    request_id=str(uuid.uuid4()),\n    prompt=\"Is it okay to proceed with this sensitive operation on customer data?\",\n    data_to_approve={\"customer_id\": \"cust_789\", \"operation\": \"data_wipe\"},\n    timeout_seconds=300 # 5 minutes\n)\n\napproval_response = await genie.human_in_loop.request_approval(request_details)\n# approval_response = await genie.human_in_loop.request_approval(request_details, approver_id=\"my_specific_hitl_plugin\")\n\n\nif approval_response[\"status\"] == \"approved\":\n    print(f\"Operation approved by {approval_response.get('approver_id', 'N/A')}. Reason: {approval_response.get('reason')}\")\n    # ... proceed with sensitive operation ...\nelse:\n    print(f\"Operation {approval_response['status']}. Reason: {approval_response.get('reason')}\")\n</code></pre>"},{"location":"guides/using_human_in_loop/#creating-custom-hitl-approval-plugins","title":"Creating Custom HITL Approval Plugins","text":"<p>Implement the <code>HumanApprovalRequestPlugin</code> protocol, defining the <code>request_approval(request: ApprovalRequest)</code> method. This method should handle the presentation of the request to a human (e.g., via a web UI, messaging platform, ticketing system) and return an <code>ApprovalResponse</code>. Register your plugin via entry points or <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_llm_providers/","title":"Using LLM Providers","text":"<p>Genie Tooling allows you to easily interact with various Large Language Models (LLMs) through a unified interface: <code>genie.llm</code>. This interface abstracts the specifics of different LLM provider APIs.</p>"},{"location":"guides/using_llm_providers/#the-geniellm-interface","title":"The <code>genie.llm</code> Interface","text":"<p>Once you have a <code>Genie</code> instance, you can access LLM functionalities:</p> <ul> <li><code>async genie.llm.generate(prompt: str, provider_id: Optional[str] = None, stream: bool = False, **kwargs) -&gt; Union[LLMCompletionResponse, AsyncIterable[LLMCompletionChunk]]</code>:     For text completion or generation tasks.<ul> <li><code>prompt</code>: The input prompt string.</li> <li><code>provider_id</code>: Optional. The ID of the LLM provider plugin to use. If <code>None</code>, the default LLM provider configured in <code>MiddlewareConfig</code> (via <code>features.llm</code> or <code>default_llm_provider_id</code>) is used.</li> <li><code>stream</code>: Optional (default <code>False</code>). If <code>True</code>, returns an async iterable of <code>LLMCompletionChunk</code> objects.</li> <li><code>**kwargs</code>: Additional provider-specific parameters (e.g., <code>temperature</code>, <code>max_tokens</code>, <code>model</code> to override the default for that provider, <code>output_schema</code> for structured output).</li> </ul> </li> <li><code>async genie.llm.chat(messages: List[ChatMessage], provider_id: Optional[str] = None, stream: bool = False, **kwargs) -&gt; Union[LLMChatResponse, AsyncIterable[LLMChatChunk]]</code>:     For conversational interactions.<ul> <li><code>messages</code>: A list of <code>ChatMessage</code> dictionaries (see Types).</li> <li><code>provider_id</code>: Optional. The ID of the LLM provider plugin to use.</li> <li><code>stream</code>: Optional (default <code>False</code>). If <code>True</code>, returns an async iterable of <code>LLMChatChunk</code> objects.</li> <li><code>**kwargs</code>: Additional provider-specific parameters (e.g., <code>temperature</code>, <code>tools</code>, <code>tool_choice</code>, <code>output_schema</code> for structured output).</li> </ul> </li> <li><code>async genie.llm.parse_output(response: Union[LLMChatResponse, LLMCompletionResponse], parser_id: Optional[str] = None, schema: Optional[Any] = None) -&gt; ParsedOutput</code>:     Parses the text content from an LLM response (either <code>LLMChatResponse</code> or <code>LLMCompletionResponse</code>) using a configured <code>LLMOutputParserPlugin</code>.<ul> <li><code>response</code>: The LLM response object.</li> <li><code>parser_id</code>: Optional. The ID of the <code>LLMOutputParserPlugin</code> to use. If <code>None</code>, the default configured parser is used.</li> <li><code>schema</code>: Optional. A schema (e.g., Pydantic model class, JSON schema dict) to guide parsing, if supported by the parser.</li> <li>Returns the parsed data (e.g., a dictionary, a Pydantic model instance).</li> <li>Raises <code>ValueError</code> if parsing fails or content cannot be extracted.</li> </ul> </li> </ul>"},{"location":"guides/using_llm_providers/#configuring-llm-providers","title":"Configuring LLM Providers","text":"<p>LLM providers are primarily configured using <code>FeatureSettings</code> in your <code>MiddlewareConfig</code>.</p>"},{"location":"guides/using_llm_providers/#example-using-ollama","title":"Example: Using Ollama","text":"<p><pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\nfrom genie_tooling.llm_providers.types import ChatMessage\nimport asyncio\n\nasync def main():\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            llm=\"ollama\",  # Select Ollama as the default\n            llm_ollama_model_name=\"mistral:latest\" # Specify the model for Ollama\n        )\n    )\n    genie = await Genie.create(config=app_config)\n\n    response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hi from Genie!\"}])\n    print(response['message']['content'])\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> Ensure Ollama is running (<code>ollama serve</code>) and the specified model (<code>mistral:latest</code>) is pulled (<code>ollama pull mistral</code>).</p>"},{"location":"guides/using_llm_providers/#example-using-openai","title":"Example: Using OpenAI","text":"<pre><code># Requires OPENAI_API_KEY environment variable to be set.\n# Genie's default EnvironmentKeyProvider will pick it up.\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\"\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-gemini","title":"Example: Using Gemini","text":"<pre><code># Requires GOOGLE_API_KEY environment variable to be set.\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"gemini\",\n        llm_gemini_model_name=\"gemini-1.5-flash-latest\"\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-llamacpp-server-mode","title":"Example: Using Llama.cpp (Server Mode)","text":"<pre><code># Assumes a Llama.cpp server is running at the specified URL.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp\",\n        llm_llama_cpp_model_name=\"your-model-alias-on-server\", # Alias/model server uses\n        llm_llama_cpp_base_url=\"http://localhost:8080\" # Default, adjust if needed\n        # llm_llama_cpp_api_key_name=\"MY_LLAMA_SERVER_KEY\" # If server requires API key\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#example-using-llamacpp-internal-mode","title":"Example: Using Llama.cpp (Internal Mode)","text":"<pre><code># Requires llama-cpp-python library and a GGUF model file.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp_internal\",\n        llm_llama_cpp_internal_model_path=\"/path/to/your/model.gguf\", # IMPORTANT: Set this path\n        llm_llama_cpp_internal_n_gpu_layers=-1, # Offload all possible layers to GPU\n        llm_llama_cpp_internal_n_ctx=4096,      # Example context size\n        llm_llama_cpp_internal_chat_format=\"mistral\" # Or \"llama-2\", \"chatml\", etc.\n    )\n)\n# genie = await Genie.create(config=app_config)\n# ... use genie.llm.chat or genie.llm.generate ...\n</code></pre>"},{"location":"guides/using_llm_providers/#overriding-provider-settings","title":"Overriding Provider Settings","text":"<p>You can override settings for specific LLM providers using the <code>llm_provider_configurations</code> dictionary in <code>MiddlewareConfig</code>. Keys can be the canonical plugin ID or a recognized alias.</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\", # Default is OpenAI\n        llm_openai_model_name=\"gpt-3.5-turbo\" # Default model for OpenAI\n    ),\n    llm_provider_configurations={\n        \"openai_llm_provider_v1\": { # Canonical ID for OpenAI provider\n            \"model_name\": \"gpt-4-turbo-preview\", # Override the model for OpenAI\n            \"request_timeout_seconds\": 180 \n        },\n        \"ollama\": { # Alias for Ollama provider\n            \"model_name\": \"llama3:latest\",\n            \"request_timeout_seconds\": 240\n        },\n        \"llama_cpp_internal_llm_provider_v1\": { # Canonical ID for internal Llama.cpp\n            \"model_path\": \"/another/model.gguf\",\n            \"n_gpu_layers\": 0, # CPU only for this specific override\n            \"chat_format\": \"chatml\",\n            \"model_name_for_logging\": \"custom_internal_llama\"\n        }\n    }\n)\n# genie = await Genie.create(config=app_config)\n\n# This will use OpenAI with gpt-4-turbo-preview\n# await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello OpenAI!\"}])\n\n# This will use Ollama with llama3:latest\n# await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Ollama!\"}], provider_id=\"ollama\")\n\n# This will use the internal Llama.cpp provider with /another/model.gguf\n# await genie.llm.generate(\"Test internal Llama.cpp\", provider_id=\"llama_cpp_internal\")\n</code></pre>"},{"location":"guides/using_llm_providers/#api-keys-and-keyprovider","title":"API Keys and <code>KeyProvider</code>","text":"<p>LLM providers that require API keys (like OpenAI, Gemini, or a secured Llama.cpp server) will attempt to fetch them using the configured <code>KeyProvider</code>. By default, Genie uses <code>EnvironmentKeyProvider</code>, which reads keys from environment variables (e.g., <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>). You can provide a custom <code>KeyProvider</code> instance to <code>Genie.create()</code> for more sophisticated key management. See the Configuration Guide for details. The internal Llama.cpp provider does not use API keys managed via <code>KeyProvider</code>.</p>"},{"location":"guides/using_llm_providers/#structured-output-gbnf-with-llamacpp-providers","title":"Structured Output (GBNF with Llama.cpp Providers)","text":"<p>Both the Llama.cpp server provider (<code>llama_cpp_llm_provider_v1</code>) and the internal Llama.cpp provider (<code>llama_cpp_internal_llm_provider_v1</code>) support GBNF grammar for constrained, structured output. You can pass a Pydantic model class or a JSON schema dictionary to the <code>output_schema</code> parameter of <code>genie.llm.generate()</code> or <code>genie.llm.chat()</code>.</p> <p><pre><code>from pydantic import BaseModel\n\nclass MyData(BaseModel):\n    name: str\n    value: int\n\n# Assuming 'genie' is configured with a Llama.cpp provider (server or internal)\n# For Llama.cpp server, ensure it's started with GBNF support enabled.\n# For Llama.cpp internal, ensure the model is GBNF-compatible.\n\n# Using with generate:\n# response_gen = await genie.llm.generate(\n#     prompt=\"Extract name and value: Name is Alpha, Value is 10. Output JSON.\",\n#     output_schema=MyData,\n#     # Llama.cpp server might need n_predict for GBNF with /v1/completions\n#     # n_predict=256 \n# )\n# if response_gen['text']:\n#     parsed_gen = await genie.llm.parse_output(response_gen, schema=MyData)\n\n# Using with chat:\n# response_chat = await genie.llm.chat(\n#     messages=[{\"role\": \"user\", \"content\": \"User: Name is Beta, Value is 20. Output JSON.\"}],\n#     output_schema=MyData\n# )\n# if response_chat['message']['content']:\n#     parsed_chat = await genie.llm.parse_output(response_chat, schema=MyData)\n</code></pre> The provider will attempt to convert the Pydantic model/JSON schema into a GBNF grammar string and pass it to the Llama.cpp backend. Ensure your prompt instructs the LLM to output JSON matching the schema.</p>"},{"location":"guides/using_llm_providers/#parsing-llm-output","title":"Parsing LLM Output","text":"<p>Often, you'll want an LLM to produce structured output (e.g., JSON). The <code>genie.llm.parse_output()</code> method helps with this.</p> <p>Example: Parsing JSON output <pre><code># Assuming 'genie' is initialized and an LLM provider is configured.\n# And a JSONOutputParserPlugin (json_output_parser_v1) is available.\n\n# Configure default output parser (optional, can also specify per call)\n# app_config = MiddlewareConfig(\n#     features=FeatureSettings(llm=\"ollama\", default_llm_output_parser=\"json_output_parser\")\n# )\n# genie = await Genie.create(config=app_config)\n\n\nprompt_for_json = \"Generate a JSON object with keys 'name' and 'city'.\"\nllm_response = await genie.llm.generate(prompt_for_json)\n# llm_response['text'] might be: 'Sure, here is the JSON: {\"name\": \"Test User\", \"city\": \"Genieville\"}'\n\ntry:\n    # Uses default parser if configured, or specify with parser_id=\"json_output_parser_v1\"\n    parsed_data = await genie.llm.parse_output(llm_response) \n    print(f\"Parsed data: {parsed_data}\")\n    # Output: Parsed data: {'name': 'Test User', 'city': 'Genieville'}\nexcept ValueError as e:\n    print(f\"Failed to parse LLM output: {e}\")\n</code></pre></p> <p>Example: Parsing into a Pydantic model <pre><code>from pydantic import BaseModel\n\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n# Configure Pydantic parser (pydantic_output_parser_v1)\n# app_config = MiddlewareConfig(\n#     features=FeatureSettings(llm=\"ollama\", default_llm_output_parser=\"pydantic_output_parser\")\n# )\n# genie = await Genie.create(config=app_config)\n\nprompt_for_pydantic = \"Create a JSON for a user named Bob, age 42.\"\nllm_response = await genie.llm.generate(prompt_for_pydantic)\n# llm_response['text'] might be: '```json\\n{\"name\": \"Bob\", \"age\": 42}\\n```'\n\ntry:\n    user_instance = await genie.llm.parse_output(llm_response, schema=UserInfo)\n    if isinstance(user_instance, UserInfo):\n        print(f\"User: {user_instance.name}, Age: {user_instance.age}\")\nexcept ValueError as e:\n    print(f\"Failed to parse into Pydantic model: {e}\")\n</code></pre> See the specific <code>LLMOutputParserPlugin</code> documentation for details on their capabilities and configuration (e.g., <code>JSONOutputParserPlugin</code>, <code>PydanticOutputParserPlugin</code>).</p>"},{"location":"guides/using_llm_providers/#chatmessage-type","title":"<code>ChatMessage</code> Type","text":"<p>The <code>messages</code> parameter for <code>genie.llm.chat()</code> expects a list of <code>ChatMessage</code> dictionaries:</p> <pre><code>from genie_tooling.llm_providers.types import ChatMessage, ToolCall\n\n# User message\nuser_message: ChatMessage = {\"role\": \"user\", \"content\": \"What's the weather in London?\"}\n\n# Assistant message (simple text response)\nassistant_text_response: ChatMessage = {\"role\": \"assistant\", \"content\": \"The weather in London is pleasant.\"}\n\n# Assistant message requesting a tool call\nassistant_tool_call_request: ChatMessage = {\n    \"role\": \"assistant\",\n    \"content\": None, # Content can be None if only tool_calls are present\n    \"tool_calls\": [\n        {\n            \"id\": \"call_weather_london_123\",\n            \"type\": \"function\",\n            \"function\": {\"name\": \"get_weather\", \"arguments\": '{\"city\": \"London\", \"units\": \"celsius\"}'}\n        }\n    ]\n}\n\n# Tool message (response from executing a tool)\ntool_response_message: ChatMessage = {\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_weather_london_123\", # Matches the ID from assistant's request\n    \"name\": \"get_weather\", # Name of the function that was called\n    \"content\": '{\"temperature\": 15, \"condition\": \"Cloudy\"}' # JSON string of the tool's output\n}\n</code></pre> <p>The <code>LLMChatResponse</code> from <code>genie.llm.chat()</code> will contain an assistant's message in this format.</p>"},{"location":"guides/using_prompts/","title":"Using the Prompt Management System (<code>genie.prompts</code>)","text":"<p>Genie Tooling provides a flexible prompt management system accessible via <code>genie.prompts</code>. This allows you to register, retrieve, and render prompt templates, separating prompt engineering concerns from your application logic.</p>"},{"location":"guides/using_prompts/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>PromptInterface</code> (<code>genie.prompts</code>): The facade interface for all prompt-related operations.</li> <li><code>PromptRegistryPlugin</code>: Responsible for storing and retrieving raw prompt template content (e.g., from a file system, database).<ul> <li>Built-in: <code>FileSystemPromptRegistryPlugin</code> (alias: <code>file_system_prompt_registry</code>).</li> </ul> </li> <li><code>PromptTemplatePlugin</code>: Responsible for rendering a raw template string with provided data.<ul> <li>Built-in: <code>BasicStringFormatTemplatePlugin</code> (alias: <code>basic_string_formatter</code>), <code>Jinja2ChatTemplatePlugin</code> (alias: <code>jinja2_chat_formatter</code>).</li> </ul> </li> </ul>"},{"location":"guides/using_prompts/#configuration","title":"Configuration","text":"<p>You configure the default prompt registry and template engine via <code>FeatureSettings</code> or explicit <code>MiddlewareConfig</code> settings.</p> <p>Via <code>FeatureSettings</code>:</p> <pre><code>from genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        # ... other features ...\n        prompt_registry=\"file_system_prompt_registry\", # Default\n        prompt_template_engine=\"jinja2_chat_formatter\"   # Default\n    ),\n    # Configure the FileSystemPromptRegistryPlugin\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": { # Canonical ID\n            \"base_path\": \"./my_application_prompts\",\n            \"template_suffix\": \".j2\" # If using Jinja2 templates\n        }\n    }\n    # No specific config needed for Jinja2ChatTemplatePlugin by default\n)\n</code></pre> <p>Explicit Configuration:</p> <pre><code>app_config = MiddlewareConfig(\n    default_prompt_registry_id=\"file_system_prompt_registry_v1\",\n    default_prompt_template_plugin_id=\"jinja2_chat_template_v1\",\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\"base_path\": \"prompts\"}\n    }\n)\n</code></pre>"},{"location":"guides/using_prompts/#using-genieprompts","title":"Using <code>genie.prompts</code>","text":""},{"location":"guides/using_prompts/#1-getting-raw-template-content","title":"1. Getting Raw Template Content","text":"<pre><code># Assuming 'my_application_prompts/my_task.j2' exists\ntemplate_content = await genie.prompts.get_prompt_template_content(\n    name=\"my_task\", \n    # version=\"v1\", # Optional version\n    # registry_id=\"custom_registry\" # Optional, if not using default\n)\nif template_content:\n    print(f\"Raw template: {template_content}\")\n</code></pre>"},{"location":"guides/using_prompts/#2-rendering-a-string-prompt","title":"2. Rendering a String Prompt","text":"<p>This is useful for prompts that result in a single string, often for completion LLMs.</p> <pre><code>from genie_tooling.prompts.types import PromptData\n\nprompt_data: PromptData = {\"topic\": \"AI ethics\", \"length\": \"short\"}\n\n# Uses default registry and default template engine\nrendered_string = await genie.prompts.render_prompt(\n    name=\"summarization_prompt\", # e.g., 'my_application_prompts/summarization_prompt.txt'\n    data=prompt_data\n)\nif rendered_string:\n    print(f\"Rendered string prompt: {rendered_string}\")\n    # Example: \"Summarize AI ethics in a short paragraph.\"\n</code></pre>"},{"location":"guides/using_prompts/#3-rendering-a-chat-prompt","title":"3. Rendering a Chat Prompt","text":"<p>This is used for prompts that result in a list of <code>ChatMessage</code> dictionaries, suitable for chat-based LLMs. The <code>Jinja2ChatTemplatePlugin</code> is particularly useful here as Jinja2 can easily render structured JSON/YAML.</p> <p>Example Jinja2 template (<code>my_application_prompts/chat_style_prompt.j2</code>): <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful {{ persona }}.\"},\n    {\"role\": \"user\", \"content\": \"Tell me about {{ subject }}.\"}\n]\n</code></pre></p> <p>Python code: <pre><code>from genie_tooling.prompts.types import PromptData\n\nchat_data: PromptData = {\"persona\": \"historian\", \"subject\": \"the Roman Empire\"}\n\n# Uses default registry and default template engine (configured to jinja2_chat_formatter)\nchat_messages = await genie.prompts.render_chat_prompt(\n    name=\"chat_style_prompt\", \n    data=chat_data\n)\nif chat_messages:\n    print(f\"Rendered chat messages: {chat_messages}\")\n    # Output:\n    # [\n    #   {'role': 'system', 'content': 'You are a helpful historian.'},\n    #   {'role': 'user', 'content': 'Tell me about the Roman Empire.'}\n    # ]\n    # response = await genie.llm.chat(chat_messages)\n</code></pre></p>"},{"location":"guides/using_prompts/#4-listing-available-templates","title":"4. Listing Available Templates","text":"<pre><code>available_templates = await genie.prompts.list_templates()\n# Or for a specific registry:\n# available_templates = await genie.prompts.list_templates(registry_id=\"my_other_registry\")\n\nfor template_id in available_templates:\n    print(f\"- Name: {template_id['name']}, Version: {template_id.get('version', 'N/A')}, Desc: {template_id.get('description')}\")\n</code></pre>"},{"location":"guides/using_prompts/#creating-custom-prompt-plugins","title":"Creating Custom Prompt Plugins","text":"<ul> <li><code>PromptRegistryPlugin</code>: Implement <code>get_template_content</code> and <code>list_available_templates</code>.</li> <li><code>PromptTemplatePlugin</code>: Implement <code>render</code> (for string output) and <code>render_chat_messages</code> (for <code>List[ChatMessage]</code> output).</li> </ul> <p>Register your custom plugins via entry points in <code>pyproject.toml</code> or ensure they are discoverable via <code>plugin_dev_dirs</code>.</p>"},{"location":"guides/using_rag/","title":"Using Retrieval Augmented Generation (RAG)","text":"<p>Genie Tooling provides a flexible RAG system through the <code>genie.rag</code> interface, allowing you to index data from various sources and perform semantic searches.</p>"},{"location":"guides/using_rag/#core-rag-operations-via-genierag","title":"Core RAG Operations via <code>genie.rag</code>","text":"<p>The <code>genie.rag</code> interface simplifies common RAG tasks:</p> <ul> <li><code>genie.rag.index_directory(path, collection_name, ...)</code>: Indexes all supported files within a local directory.</li> <li><code>genie.rag.index_web_page(url, collection_name, ...)</code>: Fetches content from a web URL, extracts text, and indexes it.</li> <li><code>genie.rag.search(query, collection_name, top_k, ...)</code>: Performs a semantic search against an indexed collection.</li> </ul>"},{"location":"guides/using_rag/#configuring-rag-components","title":"Configuring RAG Components","text":"<p>RAG components (Document Loaders, Text Splitters, Embedding Generators, Vector Stores, Retrievers) are configured primarily using <code>FeatureSettings</code> within your <code>MiddlewareConfig</code>.</p> <pre><code>import asyncio\nfrom pathlib import Path\nfrom genie_tooling.config.models import MiddlewareConfig\nfrom genie_tooling.config.features import FeatureSettings\nfrom genie_tooling.genie import Genie\n\nasync def main():\n    app_config = MiddlewareConfig(\n        features=FeatureSettings(\n            # RAG Embedder (e.g., for creating embeddings of your documents)\n            rag_embedder=\"sentence_transformer\", # Alias for \"sentence_transformer_embedder_v1\"\n            # rag_embedder_st_model_name=\"all-MiniLM-L6-v2\", # Default ST model\n\n            # RAG Vector Store (e.g., for storing and searching embeddings)\n            rag_vector_store=\"faiss\", # Alias for \"faiss_vector_store_v1\" (in-memory)\n            # Or for persistent ChromaDB:\n            # rag_vector_store=\"chroma\",\n            # rag_vector_store_chroma_path=\"./my_rag_db\",\n            # rag_vector_store_chroma_collection_name=\"my_documents\",\n            # Or for Qdrant:\n            # rag_vector_store=\"qdrant\", # Example using Qdrant\n            # rag_vector_store_qdrant_url=\"http://localhost:6333\", # Or path for local Qdrant\n            # rag_vector_store_qdrant_collection_name=\"my_qdrant_docs\",\n            # rag_vector_store_qdrant_embedding_dim=384, # Set to your embedder's dimension (e.g., 384 for all-MiniLM-L6-v2)\n\n            # Default RAG Loader (used by index_directory if not specified)\n            rag_loader=\"file_system\", # Alias for \"file_system_loader_v1\" (default for index_directory)\n            # For index_web_page, it defaults to \"web_page_loader_v1\" internally if rag_loader is \"web_page\" or not set.\n        )\n    )\n    genie = await Genie.create(config=app_config)\n\n    # Create a dummy directory and file for indexing\n    data_path = Path(\"./rag_data_example\")\n    data_path.mkdir(exist_ok=True)\n    (data_path / \"sample.txt\").write_text(\"Genie makes RAG easy and configurable.\")\n\n    # Index the directory\n    collection = \"my_sample_collection\"\n    await genie.rag.index_directory(str(data_path), collection_name=collection)\n    print(f\"Indexed documents from '{data_path}' into '{collection}'.\")\n\n    # Search the collection\n    query = \"How is RAG with Genie?\"\n    results = await genie.rag.search(query, collection_name=collection, top_k=1)\n    if results:\n        print(f\"Search for '{query}':\")\n        for res in results:\n            print(f\"  - Score: {res.score:.4f}, Content: {res.content[:100]}...\")\n    else:\n        print(f\"No results found for '{query}'.\")\n\n    # Clean up dummy data\n    (data_path / \"sample.txt\").unlink(missing_ok=True) # Added missing_ok=True\n    if data_path.exists(): data_path.rmdir() # Only rmdir if it's empty and exists\n\n    await genie.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/using_rag/#overriding-rag-component-configurations","title":"Overriding RAG Component Configurations","text":"<p>You can override configurations for specific RAG component plugins (Document Loaders, Text Splitters, Embedding Generators, Vector Stores, Retrievers) using their respective <code>*_configurations</code> dictionaries in <code>MiddlewareConfig</code>.</p> <p>For example, to configure the <code>WebPageLoader</code> to use Trafilatura for better content extraction when <code>genie.rag.index_web_page()</code> is called (and <code>rag_loader</code> feature is set to <code>\"web_page\"</code> or not set, allowing the default):</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        rag_loader=\"web_page\", # Explicitly or implicitly uses web_page_loader_v1\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\"\n    ),\n    document_loader_configurations={\n        \"web_page_loader_v1\": { # Canonical ID of the WebPageLoader\n            \"use_trafilatura\": True,\n            # \"trafilatura_include_comments\": False # Other loader-specific settings\n        }\n    },\n    embedding_generator_configurations={\n        \"sentence_transformer_embedder_v1\": {\n            \"model_name\": \"paraphrase-multilingual-MiniLM-L12-v2\" # Use a different ST model\n        }\n    }\n)\n</code></pre> <p>When calling <code>genie.rag.index_directory()</code> or <code>genie.rag.search()</code>, you can also pass <code>loader_id</code>, <code>splitter_id</code>, <code>embedder_id</code>, <code>vector_store_id</code>, <code>retriever_id</code>, and their corresponding <code>*_config</code> dictionaries to override the defaults for that specific call.</p>"},{"location":"guides/using_rag/#advanced-usage","title":"Advanced Usage","text":"<p>The <code>genie.rag</code> interface internally uses the <code>RAGManager</code>. For advanced scenarios requiring direct interaction with the <code>RAGManager</code> or individual RAG component plugins, you can access it via <code>genie._rag_manager</code> (though this is generally not needed for typical use cases).</p> <p>Refer to Creating RAG Plugins for information on developing your own RAG components.</p>"},{"location":"guides/using_tools/","title":"Using Tools","text":"<p>Genie Tooling provides robust mechanisms for defining, discovering, and executing tools. Tools represent discrete actions an AI agent can perform.</p> <p>The primary way to interact with tools is through the <code>Genie</code> facade.</p>"},{"location":"guides/using_tools/#enabling-tools-automatic-vs-explicit","title":"Enabling Tools: Automatic vs. Explicit","text":"<p>Genie supports two modes for enabling tools, controlled by the <code>auto_enable_registered_tools</code> flag in <code>MiddlewareConfig</code>. This is a critical security and configuration concept.</p>"},{"location":"guides/using_tools/#automatic-mode-default-for-development","title":"Automatic Mode (Default for Development)","text":"<p>By default, <code>auto_enable_registered_tools</code> is <code>True</code>. This is designed for a frictionless developer experience, especially during prototyping.</p> <ul> <li>How it works: Any function decorated with <code>@tool</code> and registered via <code>await genie.register_tool_functions([...])</code> is automatically enabled and available for use by <code>genie.execute_tool</code> and <code>genie.run_command</code>.</li> <li>Configuration: You only need to add an entry to <code>tool_configurations</code> if a tool requires specific settings for its <code>setup()</code> method. Class-based tools (like the built-in <code>calculator_tool</code>) generally still need to be listed to be enabled.</li> </ul> <pre><code># In your main application file:\nfrom genie_tooling import tool, Genie\nfrom genie_tooling.config.models import MiddlewareConfig\n\n@tool\ndef my_simple_tool():\n    return \"It works!\"\n\n@tool\ndef my_configurable_tool(api_endpoint: str):\n    # ... uses api_endpoint ...\n    return \"Configured tool works!\"\n\n# Development configuration\napp_config = MiddlewareConfig(\n    auto_enable_registered_tools=True, # This is the default\n    tool_configurations={\n        # Only tools needing specific config are listed here\n        \"my_configurable_tool\": {\"api_endpoint\": \"https://api.example.com\"},\n        # Built-in class-based tools still need to be listed to be enabled\n        \"calculator_tool\": {},\n    }\n)\n# genie = await Genie.create(config=app_config)\n# await genie.register_tool_functions([my_simple_tool, my_configurable_tool])\n\n# All three tools are now active:\n# await genie.execute_tool(\"my_simple_tool\")\n# await genie.execute_tool(\"my_configurable_tool\")\n# await genie.execute_tool(\"calculator_tool\", ...)\n</code></pre>"},{"location":"guides/using_tools/#explicit-mode-recommended-for-production","title":"Explicit Mode (Recommended for Production)","text":"<p>For production environments, it is strongly recommended to set <code>auto_enable_registered_tools=False</code>. This provides a clear, secure, and auditable manifest of the agent's capabilities.</p> <ul> <li>How it works: A tool is only active if its identifier is present as a key in the <code>tool_configurations</code> dictionary. This applies to both class-based plugins and <code>@tool</code> decorated functions.</li> <li>Security: This prevents accidental exposure of development or debugging tools in a production setting. It enforces the principle of least privilege.</li> </ul> <pre><code># In your production configuration:\napp_config = MiddlewareConfig(\n    auto_enable_registered_tools=False, # Explicitly disable auto-enablement\n    tool_configurations={\n        # Only tools listed here will be active, regardless of what's registered.\n        \"my_simple_tool\": {}, # Enable with no config\n        \"my_configurable_tool\": {\"api_endpoint\": \"https://api.example.com\"},\n        \"calculator_tool\": {}\n        # A hypothetical 'dev_debug_tool' would NOT be active even if registered.\n    }\n)\n</code></pre>"},{"location":"guides/using_tools/#executing-tools-directly-with-genieexecute_tool","title":"Executing Tools Directly with <code>genie.execute_tool()</code>","text":"<p>If you know which tool you want to use and have its parameters, you can execute it directly:</p> <pre><code># Assuming 'genie' is initialized and 'calculator_tool' is enabled.\ncalc_result = await genie.execute_tool(\n    \"calculator_tool\",\n    num1=10,\n    num2=5,\n    operation=\"multiply\"\n)\nprint(f\"Calculator Result: {calc_result}\")\n# Output: Calculator Result: {'result': 50.0, 'error_message': None}\n</code></pre>"},{"location":"guides/using_tools/#processing-natural-language-commands-with-genierun_command","title":"Processing Natural Language Commands with <code>genie.run_command()</code>","text":"<p>For more agentic behavior, where the system needs to interpret a natural language command to select and parameterize an enabled tool, use <code>genie.run_command()</code>. This method leverages a configured Command Processor plugin.</p> <p><pre><code># Assuming 'genie' is initialized with an LLM-assisted command processor\n# and 'calculator_tool' is enabled.\ncommand_text = \"What is the result of 75 divided by 3?\"\ncommand_output = await genie.run_command(command_text)\nprint(f\"Command Output: {command_output}\")\n</code></pre> Refer to the Using Command Processors guide for more details.</p>"},{"location":"guides/using_tools/#defining-tools","title":"Defining Tools","text":"<p>Genie supports two main ways to define tools:</p> <ol> <li>Plugin-based Tools: Create a class that inherits from <code>genie_tooling.tools.abc.Tool</code>. See Creating Tool Plugins.</li> <li>Decorator-based Tools: Use the <code>@genie_tooling.tool</code> decorator on your Python functions.</li> </ol> <p>Remember that regardless of how a tool is defined, its enablement is controlled by the <code>auto_enable_registered_tools</code> flag and the <code>tool_configurations</code> dictionary.</p>"},{"location":"tutorials/E01_simple_agent_cli/","title":"Tutorial: Simple Agent CLI (E01)","text":"<p>This tutorial corresponds to the example file <code>examples/E01_simple_agent_cli.py</code>.</p> <p>It demonstrates a basic command-line agent that does not use an LLM for tool selection. Instead, it shows how to: - Configure the <code>simple_keyword</code> command processor, which maps keywords like \"calculate\" or \"weather\" to specific tool IDs. - Explicitly enable the <code>calculator_tool</code> and <code>open_weather_map_tool</code> in <code>tool_configurations</code>. - Use <code>genie.run_command()</code> to process user input. The keyword processor will find the tool and then prompt the user for the necessary parameters.</p>"},{"location":"tutorials/E01_simple_agent_cli/#example-code","title":"Example Code","text":""},{"location":"tutorials/E01_simple_agent_cli/#examplese01_simple_agent_clipy","title":"examples/E01_simple_agent_cli.py","text":"<p>\"\"\" Example: Simple Agent CLI (Refactored for Genie Facade)</p> <p>This example demonstrates a basic command-line agent that uses the Genie facade to interact with tools, configured via FeatureSettings.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Set an environment variable for OpenWeatherMap API key if you want to test that tool:    <code>export OPENWEATHERMAP_API_KEY=\"your_actual_key\"</code> 3. Run from the root of the project:    <code>poetry run python examples/E01_simple_agent_cli.py</code></p> <p>The agent will: - Initialize Genie with a simple keyword command processor. - Ask for your input (e.g., \"calculate 10 + 5\", \"weather in London\"). - Use genie.run_command() to process the input and execute the tool. - Display the result. \"\"\" import asyncio import json import logging import os from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p>"},{"location":"tutorials/E01_simple_agent_cli/#genie-uses-environmentkeyprovider-by-default-if-no-key_provider_instance-is-given","title":"Genie uses EnvironmentKeyProvider by default if no key_provider_instance is given","text":""},{"location":"tutorials/E01_simple_agent_cli/#and-key_provider_id-is-not-set-or-set-to-env_keys-in-middlewareconfig","title":"and key_provider_id is not set or set to \"env_keys\" in MiddlewareConfig.","text":"<p>async def run_simple_agent_cli():     print(\"--- Simple Agent CLI (Genie Facade Version) ---\")</p> <pre><code># 1. Configure Middleware using FeatureSettings\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\", # Using simple_keyword, no LLM needed for command processing itself\n        command_processor=\"simple_keyword\",\n        # No RAG or complex tool lookup needed for this basic CLI\n        rag_embedder=\"none\",\n        tool_lookup=\"none\",\n    ),\n    # Configure the simple_keyword_processor\n    command_processor_configurations={\n        \"simple_keyword_processor_v1\": { # Canonical ID\n            \"keyword_map\": {\n                \"calculate\": \"calculator_tool\",\n                \"math\": \"calculator_tool\",\n                \"add\": \"calculator_tool\",\n                \"plus\": \"calculator_tool\",\n                \"weather\": \"open_weather_map_tool\",\n                \"forecast\": \"open_weather_map_tool\",\n            },\n            \"keyword_priority\": [\"calculate\", \"math\", \"add\", \"plus\", \"weather\", \"forecast\"]\n        }\n    },\n    tool_configurations={\n        # Explicitly enable the tools to be used\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {}\n        # No specific config needed for these tools if API keys are handled\n        # by the default EnvironmentKeyProvider.\n    }\n)\n\n# 2. Instantiate Genie\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie facade...\")\n    # No key_provider_instance needed if using EnvironmentKeyProvider (default)\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized successfully!\")\n\n    # 3. CLI Loop\n    print(\"\\nType 'quit' to exit.\")\n    if not os.getenv(\"OPENWEATHERMAP_API_KEY\"):\n        print(\"Note: OPENWEATHERMAP_API_KEY not set. 'weather' commands will select the tool but execution will fail.\")\n\n    while True:\n        try:\n            user_query = await asyncio.to_thread(input, \"\\n&gt; Your query: \")\n        except KeyboardInterrupt:\n            print(\"\\nExiting...\")\n            break\n        if user_query.lower() == \"quit\":\n            print(\"Exiting...\")\n            break\n        if not user_query.strip():\n            continue\n\n        print(f\"Processing command: '{user_query}'\")\n        try:\n            # genie.run_command will use the configured 'simple_keyword_processor_v1'\n            # which will prompt for parameters if a tool is matched.\n            command_result = await genie.run_command(user_query)\n\n            print(\"\\nCommand Result:\")\n            if command_result:\n                print(json.dumps(command_result, indent=2, default=str))\n                if command_result.get(\"tool_result\") and command_result[\"tool_result\"].get(\"error_message\"):\n                    print(f\"Tool Execution Error: {command_result['tool_result']['error_message']}\")\n                elif command_result.get(\"error\"):\n                     print(f\"Command Processing Error: {command_result['error']}\")\n            else:\n                print(\"Command did not produce a result.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing the command: {e}\")\n            logging.exception(\"Error details:\")\n\nexcept Exception as e:\n    print(f\"Failed to initialize or run agent: {e}\")\n    logging.exception(\"Initialization/runtime error details:\")\nfinally:\n    if genie:\n        print(\"\\nTearing down Genie facade...\")\n        await genie.close()\n        print(\"Genie facade torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # For more detailed library logs:     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_simple_agent_cli())</p>"},{"location":"tutorials/E02_ollama_chat_example/","title":"Tutorial: Ollama Chat Example (E02)","text":"<p>This tutorial corresponds to the example file <code>examples/E02_ollama_chat_example.py</code>.</p> <p>It demonstrates the simplest way to interact with a Large Language Model using Genie. It shows how to: - Use <code>FeatureSettings</code> to quickly configure Genie to use an Ollama provider (<code>llm=\"ollama\"</code>). - Specify the model to use (e.g., <code>mistral:latest</code>). - Use <code>genie.llm.chat()</code> to send a message to the LLM and receive a response.</p> <p>Prerequisite: Ensure you have an Ollama instance running and the specified model pulled (e.g., <code>ollama pull mistral</code>).</p>"},{"location":"tutorials/E02_ollama_chat_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E02_ollama_chat_example/#examplese02_ollama_chat_examplepy","title":"examples/E02_ollama_chat_example.py","text":"<p>\"\"\" Example: Ollama Chat with Genie Facade</p> <p>This example demonstrates how to use the Genie facade to chat with an Ollama-hosted LLM (e.g., Mistral).</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Ensure Ollama is running and the model 'mistral:latest' (or your chosen model) is pulled:    <code>ollama serve</code> <code>ollama pull mistral</code> 3. Run from the root of the project:    <code>poetry run python examples/E02_ollama_chat_example.py</code> \"\"\" import asyncio import logging from typing import List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage</p> <p>async def run_ollama_chat_demo():     print(\"--- Ollama Chat Example ---\")</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\" # Or any other model you have pulled\n    )\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with Ollama...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    messages: List[ChatMessage] = [\n        {\"role\": \"user\", \"content\": \"Hello, Ollama! Tell me a fun fact about llamas.\"}\n    ]\n    print(f\"\\nSending to Ollama: {messages[0]['content']}\")\n\n    response = await genie.llm.chat(messages)\n    # Default provider is Ollama as per FeatureSettings\n\n    assistant_response = response.get(\"message\", {}).get(\"content\", \"No content received.\")\n    print(f\"\\nOllama says: {assistant_response}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Ollama chat error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_ollama_chat_demo())</p>"},{"location":"tutorials/E03_openai_chat_example/","title":"Tutorial: OpenAI Chat Example (E03)","text":"<p>This tutorial corresponds to the example file <code>examples/E03_openai_chat_example.py</code>.</p> <p>It demonstrates how to configure Genie to use the OpenAI API. It shows how to: - Use <code>FeatureSettings</code> to select the OpenAI provider (<code>llm=\"openai\"</code>). - Specify the model to use (e.g., <code>gpt-3.5-turbo</code>). - Rely on the default <code>EnvironmentKeyProvider</code> to automatically pick up the <code>OPENAI_API_KEY</code> from your environment variables.</p> <p>Prerequisite: You must have your <code>OPENAI_API_KEY</code> set as an environment variable.</p>"},{"location":"tutorials/E03_openai_chat_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E03_openai_chat_example/#examplese03_openai_chat_examplepy","title":"examples/E03_openai_chat_example.py","text":"<p>\"\"\" Example: OpenAI Chat with Genie Facade</p> <p>This example demonstrates how to use the Genie facade to chat with an OpenAI LLM (e.g., gpt-3.5-turbo).</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Set your OpenAI API key as an environment variable:    <code>export OPENAI_API_KEY=\"your_openai_api_key\"</code> 3. Run from the root of the project:    <code>poetry run python examples/E03_openai_chat_example.py</code> \"\"\" import asyncio import logging import os from typing import List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage</p> <p>async def run_openai_chat_demo():     print(\"--- OpenAI Chat Example ---\")</p> <pre><code>if not os.getenv(\"OPENAI_API_KEY\"):\n    print(\"ERROR: OPENAI_API_KEY environment variable not set. Please set it to run this demo.\")\n    return\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\"\n    )\n    # KeyProvider defaults to EnvironmentKeyProvider, which will pick up OPENAI_API_KEY\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with OpenAI...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    messages: List[ChatMessage] = [\n        {\"role\": \"user\", \"content\": \"Hello, OpenAI! What is the capital of France?\"}\n    ]\n    print(f\"\\nSending to OpenAI: {messages[0]['content']}\")\n\n    response = await genie.llm.chat(messages) # Uses default provider (OpenAI)\n\n    assistant_response = response.get(\"message\", {}).get(\"content\", \"No content received.\")\n    print(f\"\\nOpenAI says: {assistant_response}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"OpenAI chat error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_openai_chat_demo())</p>"},{"location":"tutorials/E04_gemini_chat_example/","title":"Tutorial: Gemini Chat Example (E04)","text":"<p>This tutorial corresponds to the example file <code>examples/E04_gemini_chat_example.py</code>.</p> <p>It demonstrates how to configure Genie to use the Google Gemini API. It shows how to: - Use <code>FeatureSettings</code> to select the Gemini provider (<code>llm=\"gemini\"</code>). - Specify the model to use (e.g., <code>gemini-1.5-flash-latest</code>). - Rely on the default <code>EnvironmentKeyProvider</code> to automatically pick up the <code>GOOGLE_API_KEY</code> from your environment variables.</p> <p>Prerequisite: You must have your <code>GOOGLE_API_KEY</code> set as an environment variable.</p>"},{"location":"tutorials/E04_gemini_chat_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E04_gemini_chat_example/#examplese04_gemini_chat_examplepy","title":"examples/E04_gemini_chat_example.py","text":"<p>\"\"\" Example: Gemini Chat with Genie Facade</p> <p>This example demonstrates how to use the Genie facade to chat with a Google Gemini LLM (e.g., gemini-1.5-flash-latest).</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras --extras gemini</code>). 2. Set your Google API key as an environment variable:    <code>export GOOGLE_API_KEY=\"your_google_api_key\"</code> 3. Run from the root of the project:    <code>poetry run python examples/E04_gemini_chat_example.py</code> \"\"\" import asyncio import logging import os from typing import List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage</p> <p>async def run_gemini_chat_demo():     print(\"--- Gemini Chat Example ---\")</p> <pre><code>if not os.getenv(\"GOOGLE_API_KEY\"):\n    print(\"ERROR: GOOGLE_API_KEY environment variable not set. Please set it to run this demo.\")\n    return\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"gemini\",\n        llm_gemini_model_name=\"gemini-1.5-flash-latest\"\n    )\n    # KeyProvider defaults to EnvironmentKeyProvider\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with Gemini...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    messages: List[ChatMessage] = [\n        {\"role\": \"user\", \"content\": \"Hello, Gemini! Can you write a short poem about AI?\"}\n    ]\n    print(f\"\\nSending to Gemini: {messages[0]['content']}\")\n\n    response = await genie.llm.chat(messages) # Uses default provider (Gemini)\n\n    assistant_response = response.get(\"message\", {}).get(\"content\", \"No content received.\")\n    print(f\"\\nGemini says: {assistant_response}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Gemini chat error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_gemini_chat_demo())</p>"},{"location":"tutorials/E05_rag_pipeline_demo/","title":"Tutorial: RAG Pipeline Demo (E05)","text":"<p>This tutorial corresponds to the example file <code>examples/E05_rag_pipeline_demo.py</code>.</p> <p>It demonstrates how to set up and use a complete, local Retrieval Augmented Generation (RAG) pipeline. It shows how to: - Configure a local RAG pipeline using <code>FeatureSettings</code> (Sentence Transformers for embeddings and FAISS for an in-memory vector store). - Index documents from a local directory using <code>genie.rag.index_directory()</code>. - Perform a similarity search on the indexed documents using <code>genie.rag.search()</code>.</p>"},{"location":"tutorials/E05_rag_pipeline_demo/#example-code","title":"Example Code","text":""},{"location":"tutorials/E05_rag_pipeline_demo/#examplese05_rag_pipeline_demopy","title":"examples/E05_rag_pipeline_demo.py","text":"<p>\"\"\" Example: RAG Pipeline Demo using Genie Facade (Updated)</p> <p>This example demonstrates setting up and using a RAG pipeline to index local text files and perform similarity searches using the Genie facade and FeatureSettings for simplified configuration.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>).    You'll need dependencies for local RAG, e.g., sentence-transformers, faiss-cpu. 2. The script will create dummy data files in examples/data/ if they don't exist. 3. Run from the root of the project:    <code>poetry run python examples/E05_rag_pipeline_demo.py</code></p> <p>The demo will: - Initialize the Genie facade using FeatureSettings for RAG components. - Index documents from the 'examples/data/' directory. - Perform a search query against the indexed documents. - Print the search results. \"\"\" import asyncio import logging from pathlib import Path from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def main():     print(\"--- RAG Pipeline Demo using Genie Facade (FeatureSettings) ---\")</p> <pre><code>current_file_dir = Path(__file__).parent\ndata_dir = current_file_dir / \"data\"\ndata_dir.mkdir(exist_ok=True) # Ensure data directory exists\n\n# Create dummy files if they don't exist for the demo\ndoc1_path = data_dir / \"doc1.txt\"\ndoc2_path = data_dir / \"doc2.txt\"\ndoc3_path = data_dir / \"doc3.txt\"\n\nif not doc1_path.exists(): doc1_path.write_text(\"The quick brown fox jumps over the lazy dog.\\nLarge language models are transforming AI.\")\nif not doc2_path.exists(): doc2_path.write_text(\"Genie Tooling provides a hyper-pluggable middleware.\\nRetrieval Augmented Generation enhances LLM responses.\")\nif not doc3_path.exists(): doc3_path.write_text(\"Python is a versatile programming language.\\nAsync programming is key for I/O bound tasks.\")\n\n\n# 1. Configure Middleware using FeatureSettings for RAG\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\",\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\",\n    )\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie facade...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized.\")\n\n    collection_name_for_demo = \"my_local_rag_collection_e05\"\n    print(f\"\\nIndexing documents from '{data_dir}' into collection '{collection_name_for_demo}'...\")\n\n    index_result = await genie.rag.index_directory(\n        str(data_dir),\n        collection_name=collection_name_for_demo\n    )\n    print(f\"Indexing result: {index_result}\")\n    if index_result.get(\"status\") != \"success\":\n        print(f\"ERROR: Indexing failed: {index_result.get('message')}\")\n        return\n\n    query = \"What is Genie Tooling?\"\n    print(f\"\\nPerforming search for query: '{query}' in collection '{collection_name_for_demo}'\")\n\n    search_results = await genie.rag.search(\n        query,\n        collection_name=collection_name_for_demo,\n        top_k=2\n    )\n\n    if not search_results:\n        print(\"No search results found.\")\n    else:\n        print(\"\\nSearch Results:\")\n        for i, result_chunk in enumerate(search_results):\n            print(f\"  --- Result {i+1} (Score: {result_chunk.score:.4f}, ID: {result_chunk.id}) ---\")\n            print(f\"  Content: {result_chunk.content[:200]}...\")\n            print(f\"  Metadata: {result_chunk.metadata}\")\n            print(\"  ------------------------------------\")\n\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred: {e}\")\n    logging.exception(\"Error details:\")\nfinally:\n    if genie:\n        print(\"\\n--- Tearing down Genie facade ---\")\n        await genie.close()\n        print(\"Genie facade teardown complete.\")\n\n    # Clean up dummy files (optional, but good for repeated test runs)\n    # for p in [doc1_path, doc2_path, doc3_path]:\n    #     p.unlink(missing_ok=True)\n    # if data_dir.exists() and not any(data_dir.iterdir()): # Only remove if empty\n    #     data_dir.rmdir()\n    # For simplicity, we'll leave the data dir and files for now.\n    # To fully clean up FAISS index files if they were persisted to default location:\n    # default_faiss_path = Path(\"./.genie_data/faiss\")\n    # if default_faiss_path.exists():\n    #     shutil.rmtree(default_faiss_path, ignore_errors=True)\n    #     print(f\"Cleaned up FAISS data at {default_faiss_path}\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(main())</p>"},{"location":"tutorials/E06_run_command_simple_keyword_example/","title":"Tutorial: Simple Keyword Command (E06)","text":"<p>This tutorial corresponds to the example file <code>examples/E06_run_command_simple_keyword_example.py</code>.</p> <p>It demonstrates how to configure and use the <code>simple_keyword</code> command processor. This processor does not use an LLM. It shows how to: - Define a <code>keyword_map</code> to associate keywords (like \"calculate\", \"sum\") with a specific tool (<code>calculator_tool</code>). - Use <code>genie.run_command()</code>, which will trigger the processor to find the tool and then interactively prompt the user for the tool's parameters.</p>"},{"location":"tutorials/E06_run_command_simple_keyword_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E06_run_command_simple_keyword_example/#examplese06_run_command_simple_keyword_examplepy","title":"examples/E06_run_command_simple_keyword_example.py","text":"<p>\"\"\" Example: genie.run_command() with Simple Keyword Processor</p> <p>This example demonstrates using genie.run_command() with the 'simple_keyword' command processor.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Run from the root of the project:    <code>poetry run python examples/E06_run_command_simple_keyword_example.py</code> \"\"\" import asyncio import json import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_simple_keyword_command_demo():     print(\"--- Simple Keyword Command Processor Demo ---\")</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\", # No LLM needed for this processor\n        command_processor=\"simple_keyword\"\n    ),\n    command_processor_configurations={\n        \"simple_keyword_processor_v1\": { # Canonical ID\n            \"keyword_map\": {\n                \"calculate\": \"calculator_tool\",\n                \"sum\": \"calculator_tool\",\n                \"add\": \"calculator_tool\",\n                \"plus\": \"calculator_tool\",\n            },\n            \"keyword_priority\": [\"calculate\", \"sum\", \"add\", \"plus\"]\n        }\n    },\n    tool_configurations={\n        \"calculator_tool\": {} # Enable the calculator tool\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    command_text = \"calculate 25 plus 75\"\n    print(f\"\\nRunning command: '{command_text}'\")\n    print(\"The SimpleKeywordToolSelectorProcessorPlugin will prompt for parameters if a tool is matched.\")\n\n    # For non-interactive, you'd typically use execute_tool or an LLM-assisted processor.\n    # This demo shows the prompting behavior.\n    command_result = await genie.run_command(command_text)\n\n    print(\"\\nCommand Result:\")\n    if command_result:\n        print(json.dumps(command_result, indent=2))\n    else:\n        print(\"Command did not produce a result.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     asyncio.run(run_simple_keyword_command_demo())</p>"},{"location":"tutorials/E07_run_command_llm_assisted_example/","title":"Tutorial: LLM-Assisted Command (E07)","text":"<p>This tutorial corresponds to the example file <code>examples/E07_run_command_llm_assisted_example.py</code>.</p> <p>It demonstrates the core agentic capability of using an LLM to interpret a natural language command. It shows how to: - Configure the <code>llm_assisted</code> command processor. - Configure an embedding-based <code>tool_lookup</code> service to help the LLM find relevant tools. - Use <code>genie.run_command()</code>, which will leverage the LLM to select the correct tool and extract its parameters from the user's sentence, all in one step.</p>"},{"location":"tutorials/E07_run_command_llm_assisted_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E07_run_command_llm_assisted_example/#examplese07_run_command_llm_assisted_examplepy","title":"examples/E07_run_command_llm_assisted_example.py","text":"<p>\"\"\" Example: genie.run_command() with LLM-Assisted Processor</p> <p>This example demonstrates using genie.run_command() with the 'llm_assisted' command processor, using Ollama as the backing LLM.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Ensure Ollama is running and 'mistral:latest' (or your chosen model) is pulled. 3. Run from the root of the project:    <code>poetry run python examples/E07_run_command_llm_assisted_example.py</code> \"\"\" import asyncio import json import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_llm_assisted_command_demo():     print(\"--- LLM-Assisted Command Processor Demo (Ollama) ---\")</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        command_processor=\"llm_assisted\",\n        # Use default compact_text_formatter for command_processor_formatter_id_alias\n        # Use default embedding tool lookup with default ST embedder and compact_text_formatter\n        tool_lookup=\"embedding\",\n    ),\n    # Optional: Configure tool_lookup_top_k for the LLM-assisted processor\n    command_processor_configurations={\n        \"llm_assisted_tool_selection_processor_v1\": { # Canonical ID\n            \"tool_lookup_top_k\": 3\n        }\n    },\n    tool_configurations={\n        # Ensure tools the LLM might select are enabled\n        \"calculator_tool\": {}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    command_text = \"What is 123 multiplied by 4?\"\n    print(f\"\\nRunning command: '{command_text}'\")\n\n    command_result = await genie.run_command(command_text)\n\n    print(\"\\nCommand Result:\")\n    if command_result:\n        print(json.dumps(command_result, indent=2, default=str))\n        if command_result.get(\"tool_result\", {}).get(\"error_message\"):\n             print(f\"Tool Execution Error: {command_result['tool_result']['error_message']}\")\n        elif command_result.get(\"error\"):\n             print(f\"Command Processing Error: {command_result['error']}\")\n    else:\n        print(\"Command did not produce a result.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_llm_assisted_command_demo())</p>"},{"location":"tutorials/E08_decorator_tool_example/","title":"Tutorial: @tool Decorator (E08)","text":"<p>This tutorial corresponds to the example file <code>examples/E08_decorator_tool_example.py</code>.</p> <p>It demonstrates the easiest way to create custom tools in Genie. It shows how to: - Define a standard Python function (sync or async). - Apply the <code>@genie_tooling.tool</code> decorator to it, which automatically generates the necessary metadata and schema from the function's signature and docstring. - Register the decorated function with <code>genie.register_tool_functions()</code>. - Execute the tool directly using <code>genie.execute_tool()</code>.</p>"},{"location":"tutorials/E08_decorator_tool_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E08_decorator_tool_example/#examplese08_decorator_tool_examplepy","title":"examples/E08_decorator_tool_example.py","text":"<p>\"\"\" Example: Using the @tool Decorator</p> <p>This example demonstrates defining simple functions, decorating them with @genie_tooling.tool, and registering them with Genie. By default, these tools are automatically enabled for use.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Run from the root of the project:    <code>poetry run python examples/E08_decorator_tool_example.py</code> \"\"\" import asyncio import logging from typing import Optional</p> <p>from genie_tooling import tool  # Import the decorator from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p>"},{"location":"tutorials/E08_decorator_tool_example/#1-define-your-functions-and-decorate-them","title":"1. Define your functions and decorate them","text":"<p>@tool async def greet_user(name: str, enthusiasm_level: int = 1) -&gt; str:     \"\"\"     Greets a user with a specified level of enthusiasm.</p> <pre><code>Args:\n    name (str): The name of the user to greet.\n    enthusiasm_level (int): How enthusiastic the greeting should be (1-3).\n\nReturns:\n    str: The generated greeting message.\n\"\"\"\nif not 1 &lt;= enthusiasm_level &lt;= 3:\n    return f\"Sorry {name}, I can only be enthusiastic from level 1 to 3.\"\n\ngreeting = f\"Hello, {name}\"\ngreeting += \"!\" * enthusiasm_level\nreturn greeting\n</code></pre> <p>@tool def simple_math_operation(a: float, b: float, operation: str = \"add\") -&gt; float:     \"\"\"     Performs a simple math operation.     Args:         a (float): First number.         b (float): Second number.         operation (str): 'add' or 'subtract'.     Returns:         float: The result of the operation.     \"\"\"     if operation == \"add\":         return a + b     elif operation == \"subtract\":         return a - b     raise ValueError(\"Unknown operation for simple_math_operation\")</p> <p>async def run_decorator_tool_demo():     print(\"--- @tool Decorator Example ---\")</p> <pre><code># By default, `auto_enable_registered_tools` is True, so we don't need\n# to list the decorated tools in `tool_configurations` to enable them.\n# For production, set `auto_enable_registered_tools=False` and explicitly\n# list all tools you want to activate.\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\"\n    ),\n    # tool_configurations is empty because our tools are auto-enabled.\n    tool_configurations={}\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    # 2. Register the decorated functions with Genie\n    await genie.register_tool_functions([greet_user, simple_math_operation])\n    print(\"Decorated functions registered and auto-enabled.\")\n\n    # 3. Execute the tools using their function names as identifiers\n    print(\"\\nExecuting 'greet_user' tool...\")\n    greeting_result = await genie.execute_tool(\n        \"greet_user\",\n        name=\"Alice\",\n        enthusiasm_level=3\n    )\n    print(f\"Result from greet_user: {greeting_result}\")\n\n    print(\"\\nExecuting 'simple_math_operation' tool...\")\n    math_result = await genie.execute_tool(\n        \"simple_math_operation\",\n        a=10.5,\n        b=5.5,\n        operation=\"subtract\"\n    )\n    print(f\"Result from simple_math_operation: {math_result}\")\n\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     asyncio.run(run_decorator_tool_demo())</p>"},{"location":"tutorials/E09_web_rag_example/","title":"Tutorial: Web RAG (E09)","text":"<p>This tutorial corresponds to the example file <code>examples/E09_web_rag_example.py</code>.</p> <p>It demonstrates how to use the RAG pipeline to ingest data directly from the web. It shows how to: - Configure the <code>WebPageLoader</code> plugin (aliased as <code>web_page</code>). - Use <code>genie.rag.index_web_page()</code> to fetch content from a URL, extract its main text, and index it into a vector store. - Perform a semantic search over the ingested web content.</p>"},{"location":"tutorials/E09_web_rag_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E09_web_rag_example/#examplese09_web_rag_examplepy","title":"examples/E09_web_rag_example.py","text":"<p>\"\"\" Example: RAG with WebPageLoader using Genie Facade</p> <p>This example demonstrates indexing content from a web page and performing a similarity search using the Genie facade and FeatureSettings.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>).    You'll need dependencies for web loading (beautifulsoup4, trafilatura),    local RAG (sentence-transformers, faiss-cpu). 2. Run from the root of the project:    <code>poetry run python examples/E09_web_rag_example.py</code> \"\"\" import asyncio import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_web_rag_demo():     print(\"--- Web Page RAG Demo ---\")</p> <pre><code># URL to index (Python's Getting Started page as an example)\nweb_page_url = \"https://www.python.org/about/gettingstarted/\"\ncollection_name = \"python_docs_web_collection_e09\"\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\", # Not needed for RAG indexing/search focus\n        command_processor=\"none\",\n\n        rag_loader=\"web_page\", # Use WebPageLoader by default\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\", # In-memory FAISS for this demo\n    ),\n    # Optionally configure WebPageLoader (e.g., to use trafilatura)\n    document_loader_configurations={\n        \"web_page_loader_v1\": { # Canonical ID\n            \"use_trafilatura\": True # Attempt to use trafilatura for better content extraction\n        }\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie for Web RAG...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    # Index the web page\n    print(f\"\\nIndexing web page: {web_page_url} into collection '{collection_name}'...\")\n    index_result = await genie.rag.index_web_page(\n        web_page_url,\n        collection_name=collection_name\n    )\n    print(f\"Indexing result: {index_result}\")\n    if index_result.get(\"status\") != \"success\":\n        print(f\"ERROR: Indexing failed: {index_result.get('message')}\")\n        return\n\n    # Perform a search\n    query = \"What are Python libraries?\"\n    print(f\"\\nPerforming search for: '{query}' in '{collection_name}'\")\n    search_results = await genie.rag.search(\n        query,\n        collection_name=collection_name,\n        top_k=2\n    )\n\n    if not search_results:\n        print(\"No search results found.\")\n    else:\n        print(\"\\nSearch Results:\")\n        for i, chunk in enumerate(search_results):\n            print(f\"  --- Result {i+1} (Score: {chunk.score:.4f}) ---\")\n            print(f\"  Content: {chunk.content[:300]}...\") # Print snippet\n            print(f\"  Source: {chunk.metadata.get('url')}\")\n            print(\"  ------------------------------------\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Web RAG demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG) # For detailed logs     asyncio.run(run_web_rag_demo())</p>"},{"location":"tutorials/E10_chroma_tool_lookup_showcase/","title":"Tutorial: ChromaDB Tool Lookup (E10)","text":"<p>This tutorial corresponds to the example file <code>examples/E10_chroma_tool_lookup_showcase.py</code>.</p> <p>It demonstrates how to make the embedding-based tool lookup service persistent. It shows how to: - Configure the <code>embedding</code> tool lookup provider to use ChromaDB as its backend via <code>FeatureSettings</code>. - Specify a local path for the ChromaDB database, allowing tool embeddings to persist between application runs. - Use the <code>llm_assisted</code> command processor, which will now benefit from this persistent tool index.</p>"},{"location":"tutorials/E10_chroma_tool_lookup_showcase/#example-code","title":"Example Code","text":""},{"location":"tutorials/E10_chroma_tool_lookup_showcase/#examplese10_chroma_tool_lookup_showcasepy","title":"examples/E10_chroma_tool_lookup_showcase.py","text":"<p>\"\"\" Example: ChromaDB-backed Tool Lookup Showcase (Updated)</p> <p>This example demonstrates using ChromaDB for tool lookup via the LLMAssistedCommandProcessor, configured using FeatureSettings.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>).    You'll need dependencies for chromadb-client, sentence-transformers, and ollama. 2. Ensure Ollama is running and the model specified (e.g., 'mistral:latest') is pulled:    <code>ollama serve</code> <code>ollama pull mistral</code> 3. Set OPENWEATHERMAP_API_KEY if you want the weather tool to execute successfully:    <code>export OPENWEATHERMAP_API_KEY=\"your_key\"</code> 4. Run from the root of the project:    <code>poetry run python examples/E10_chroma_tool_lookup_showcase.py</code></p> <p>The demo will: - Initialize Genie with an LLM-assisted command processor. - Configure tool lookup to use embeddings stored in ChromaDB. - Process commands, demonstrating tool selection and execution. \"\"\" import asyncio import json import logging import os import shutil from pathlib import Path from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_chroma_tool_lookup_showcase():     print(\"--- ChromaDB-backed Tool Lookup Showcase (FeatureSettings) ---\")</p> <pre><code>current_file_dir = Path(__file__).parent\nchroma_tool_embeddings_path_str = str(current_file_dir / \"chroma_db_tool_embeddings_e10\")\n\n# Clean up previous run's data\nif Path(chroma_tool_embeddings_path_str).exists():\n    print(f\"Cleaning up existing ChromaDB tool embeddings data at: {chroma_tool_embeddings_path_str}\")\n    shutil.rmtree(chroma_tool_embeddings_path_str)\n\ntool_embeddings_collection_name = \"genie_tool_definitions_chroma_e10\"\n\n# 1. Configure Middleware using FeatureSettings\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n\n        command_processor=\"llm_assisted\",\n        command_processor_formatter_id_alias=\"compact_text_formatter\",\n\n        tool_lookup=\"embedding\",\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\",\n        tool_lookup_embedder_id_alias=\"st_embedder\",\n        tool_lookup_chroma_path=chroma_tool_embeddings_path_str,\n        tool_lookup_chroma_collection_name=tool_embeddings_collection_name,\n    ),\n    command_processor_configurations={\n        \"llm_assisted_tool_selection_processor_v1\": {\n            \"tool_lookup_top_k\": 3\n        }\n    },\n    tool_configurations={\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie facade...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized for ChromaDB-backed tool lookup.\")\n    print(f\"Tool embeddings will be indexed in ChromaDB at: {chroma_tool_embeddings_path_str}\")\n    print(f\"Using collection: {tool_embeddings_collection_name}\")\n\n    print(\"\\n--- Using Tool Lookup via LLM-Assisted Command Processor ---\")\n\n    commands_to_test = [\n        \"What is the sum of 15 and 30?\",\n        \"Tell me the current weather in Paris.\"\n    ]\n\n    if not os.getenv(\"OPENWEATHERMAP_API_KEY\") and any(\"weather\" in cmd.lower() for cmd in commands_to_test):\n        print(\"\\nWARNING: OPENWEATHERMAP_API_KEY not set. Weather tool lookup might select the tool, but execution will likely fail.\")\n        print('You can set it with: export OPENWEATHERMAP_API_KEY=\"your_key\"\\n')\n\n    for cmd_text in commands_to_test:\n        print(f\"\\nProcessing command: '{cmd_text}'\")\n        try:\n            command_result = await genie.run_command(command=cmd_text)\n            if command_result:\n                print(f\"  Thought Process: {command_result.get('thought_process')}\")\n                if command_result.get(\"tool_result\"):\n                    print(f\"  Tool Result: {json.dumps(command_result['tool_result'], indent=2)}\")\n                elif command_result.get(\"error\"):\n                    print(f\"  Command Error: {command_result['error']}\")\n                elif command_result.get(\"message\"):\n                    print(f\"  Message: {command_result['message']}\")\n                else:\n                    print(f\"  Raw Command Result: {json.dumps(command_result, indent=2)}\")\n            else:\n                print(\"  Command processor returned no result.\")\n        except Exception as e:\n            print(f\"  Error processing command '{cmd_text}': {e}\")\n            logging.exception(\"Command processing error details:\")\n\n    print(f\"\\n[Persistence Demo] Tool definition embeddings are stored in ChromaDB at '{chroma_tool_embeddings_path_str}'.\")\n    print(\"You can inspect this directory after the script finishes.\")\n\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred in the showcase: {e}\")\n    logging.exception(\"Showcase error details:\")\nfinally:\n    if genie:\n        print(\"\\nGenie facade tearing down...\")\n        await genie.close()\n        print(\"Genie facade torn down.\")\n    # Optional: Clean up ChromaDB directory after test\n    # if Path(chroma_tool_embeddings_path_str).exists():\n    #     shutil.rmtree(chroma_tool_embeddings_path_str)\n    #     print(f\"Cleaned up ChromaDB data at: {chroma_tool_embeddings_path_str}\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_chroma_tool_lookup_showcase())</p>"},{"location":"tutorials/E11_advanced_showcase_agent/","title":"Tutorial: Advanced Showcase (E11)","text":"<p>This tutorial corresponds to the example file <code>examples/E11_advanced_showcase_agent.py</code>.</p> <p>It demonstrates a more complex configuration, combining multiple features and plugins. It shows how to: - Configure multiple LLM providers (<code>ollama</code>, <code>openai</code>, <code>gemini</code>) and select them at runtime. - Configure multiple command processors (<code>llm_assisted</code>, <code>simple_keyword</code>) and select them at runtime. - Use a custom <code>KeyProvider</code> for fetching API keys. - Combine RAG, command processing, and direct tool execution in a single application.</p>"},{"location":"tutorials/E11_advanced_showcase_agent/#example-code","title":"Example Code","text":""},{"location":"tutorials/E11_advanced_showcase_agent/#examplese11_advanced_showcase_agentpy","title":"examples/E11_advanced_showcase_agent.py","text":"<p>\"\"\" Example: Advanced Agent Showcase using Genie Facade</p> <p>This example demonstrates configuring and using various components of the Genie Tooling middleware through the Genie facade. \"\"\" import asyncio import json import logging import os from typing import List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.core.types import Plugin as CorePluginType from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage from genie_tooling.security.key_provider import KeyProvider</p> <p>logger = logging.getLogger(name)</p>"},{"location":"tutorials/E11_advanced_showcase_agent/#-1-custom-keyprovider-","title":"--- 1. Custom KeyProvider ---","text":"<p>class DemoKeyProvider(KeyProvider, CorePluginType):     plugin_id: str = \"demo_showcase_key_provider_v1\"     async def get_key(self, key_name: str) -&gt; Optional[str]:         val = os.environ.get(key_name)         if not val:             print(f\"[KeyProvider - Showcase] WARNING: Key '{key_name}' not found.\")         return val     async def setup(self, config=None): print(f\"[{self.plugin_id}] Setup.\")     async def teardown(self): print(f\"[{self.plugin_id}] Teardown.\")</p> <p>async def main():     print(\"--- Advanced Agent Showcase ---\")     key_provider_instance = DemoKeyProvider()</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\", # Default LLM\n        llm_ollama_model_name=\"mistral:latest\",\n        llm_openai_model_name=\"gpt-3.5-turbo\", # For OpenAI provider if used\n        llm_gemini_model_name=\"gemini-1.5-flash-latest\", # For Gemini provider if used\n\n        command_processor=\"llm_assisted\", # Default command processor\n        command_processor_formatter_id_alias=\"compact_text_formatter\", # For LLM-assisted\n\n        tool_lookup=\"embedding\", # Default tool lookup\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\", # For indexing\n        tool_lookup_embedder_id_alias=\"st_embedder\", # For tool lookup embeddings\n\n        rag_loader=\"web_page\", # Default RAG loader\n        rag_embedder=\"sentence_transformer\", # Default RAG embedder\n        rag_vector_store=\"faiss\", # Default RAG vector store\n    ),\n    default_log_level=\"INFO\",\n\n    llm_provider_configurations={\n        \"ollama\": { \"request_timeout_seconds\": 180.0 },\n        \"openai\": { \"model_name\": \"gpt-4-turbo-preview\" },\n        \"gemini\": {}\n    },\n    command_processor_configurations={\n        \"simple_keyword_cmd_proc\": { # Alias for simple_keyword_processor_v1\n            \"keyword_map\": {\n                \"calculate\": \"calculator_tool\", \"math\": \"calculator_tool\",\n                \"weather\": \"open_weather_map_tool\", \"forecast\": \"open_weather_map_tool\"\n            }\n        },\n        \"llm_assisted_cmd_proc\": { # Alias for llm_assisted_tool_selection_processor_v1\n            \"tool_lookup_top_k\": 3\n        }\n    },\n    tool_configurations={ # Tools must be enabled\n        \"calculator_tool\": {},\n        \"open_weather_map_tool\": {},\n        \"generic_code_execution_tool\": {}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie facade with showcase configuration...\")\n    genie = await Genie.create(config=app_config, key_provider_instance=key_provider_instance)\n    print(\"Genie facade initialized.\")\n\n    # --- 1. LLM Interaction ---\n    print(\"\\n--- LLM Showcase ---\")\n    if os.getenv(\"OPENAI_API_KEY\"):\n        try:\n            print(\"Trying OpenAI LLM (gpt-4-turbo-preview) for chat...\")\n            chat_messages: List[ChatMessage] = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n            openai_response = await genie.llm.chat(messages=chat_messages, provider_id=\"openai\")\n            print(f\"OpenAI Response: {openai_response['message']['content']}\")\n        except Exception as e:\n            print(f\"Error with OpenAI LLM: {e}\")\n    else:\n        print(\"Skipping OpenAI LLM showcase (OPENAI_API_KEY not set).\")\n\n    try:\n        print(\"\\nTrying default LLM (Ollama/Mistral) for generation...\")\n        ollama_response = await genie.llm.generate(\"Explain RAG in one sentence.\", options={\"temperature\": 0.7})\n        print(f\"Ollama/Mistral Response: {ollama_response['text']}\")\n    except Exception as e:\n        print(f\"Error with Ollama LLM: {e}\")\n\n    # --- 2. RAG Showcase ---\n    print(\"\\n--- RAG Showcase ---\")\n    rag_collection = \"showcase_rag_collection_e11\"\n    dummy_url = \"https://www.python.org/about/gettingstarted/\"\n    try:\n        print(f\"Indexing web page: {dummy_url} into collection '{rag_collection}'...\")\n        index_result = await genie.rag.index_web_page(dummy_url, collection_name=rag_collection)\n        print(f\"Web page indexing result: {json.dumps(index_result, indent=2)}\")\n        if index_result.get(\"status\") == \"success\":\n            rag_results = await genie.rag.search(\"What are Python libraries?\", collection_name=rag_collection, top_k=1)\n            if rag_results:\n                print(f\"Top RAG: {rag_results[0].content[:300]}... (Source: {rag_results[0].metadata.get('url')})\")\n    except Exception as e:\n        print(f\"Error during RAG showcase: {e}\")\n\n    # --- 3. Command Processing Showcase ---\n    print(\"\\n--- Command Processing Showcase ---\")\n    commands = [\"calculate 100 times 3.14\", \"what's the weather like in London?\"]\n    for cmd_text in commands:\n        print(f\"\\nProcessing command: '{cmd_text}' (using default LLM-assisted processor)\")\n        try:\n            cmd_result = await genie.run_command(command=cmd_text)\n            print(f\"Result: {json.dumps(cmd_result, indent=2, default=str)}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n    print(\"\\nProcessing command: 'sum 5 and 7' (using simple_keyword processor)\")\n    try:\n        keyword_cmd_result = await genie.run_command(\"sum 5 and 7\", processor_id=\"simple_keyword_cmd_proc\")\n        print(f\"Keyword Result: {json.dumps(keyword_cmd_result, indent=2, default=str)}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    # --- 4. Direct Tool Invocation ---\n    print(\"\\n--- Direct Tool Invocation Showcase ---\")\n    if os.getenv(\"OPENWEATHERMAP_API_KEY\"):\n        try:\n            weather_result = await genie.execute_tool(\"open_weather_map_tool\", city=\"New York, US\", units=\"metric\")\n            print(f\"Direct Weather: {json.dumps(weather_result, indent=2)}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    else:\n        print(\"Skipping direct OpenWeatherMapTool (OPENWEATHERMAP_API_KEY not set).\")\n\nexcept Exception as e:\n    print(f\"\\nUNEXPECTED CRITICAL error: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie facade torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(main())</p>"},{"location":"tutorials/E12_custom_key_provider_example/","title":"Tutorial: Custom KeyProvider (E12)","text":"<p>This tutorial corresponds to the example file <code>examples/E12_custom_key_provider_example.py</code>.</p> <p>It demonstrates how to implement and use a custom <code>KeyProvider</code> class. This is useful for integrating with custom secret management systems (like HashiCorp Vault, AWS Secrets Manager, etc.) instead of relying on environment variables. It shows how to: - Create a class that implements the <code>KeyProvider</code> protocol. - Implement the <code>async def get_key(...)</code> method. - Pass an instance of your custom provider to <code>Genie.create()</code>.</p>"},{"location":"tutorials/E12_custom_key_provider_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E12_custom_key_provider_example/#examplese12_custom_key_provider_examplepy","title":"examples/E12_custom_key_provider_example.py","text":"<p>\"\"\" Example: Using a Custom KeyProvider with Genie</p> <p>This example demonstrates how to implement a custom KeyProvider and use it with the Genie facade, for instance, to fetch an OpenAI API key.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Set an environment variable for your custom key provider to read:    <code>export MY_APP_OPENAI_KEY=\"your_actual_openai_api_key\"</code> 3. Run from the root of the project:    <code>poetry run python examples/E12_custom_key_provider_example.py</code> \"\"\" import asyncio import logging import os from typing import Any, Dict, List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.core.types import (     Plugin as CorePluginType, ) from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage from genie_tooling.security.key_provider import KeyProvider</p> <p>logger = logging.getLogger(name)</p>"},{"location":"tutorials/E12_custom_key_provider_example/#1-implement-your-applications-keyprovider","title":"1. Implement your application's KeyProvider","text":"<p>class MyAppCustomKeyProvider(KeyProvider, CorePluginType):     plugin_id = \"my_app_custom_key_provider_v1\"</p> <pre><code>async def get_key(self, key_name: str) -&gt; str | None:\n    if key_name == \"OPENAI_API_KEY\":\n        return os.environ.get(\"MY_APP_OPENAI_KEY\")\n    logger.debug(f\"[{self.plugin_id}] Requested key '{key_name}', not specifically handled by this provider.\")\n    return None\n\nasync def setup(self, config: Optional[Dict[str, Any]] = None):\n    logger.info(f\"[{self.plugin_id}] Custom KeyProvider setup.\")\n\nasync def teardown(self):\n    logger.info(f\"[{self.plugin_id}] Custom KeyProvider teardown.\")\n</code></pre> <p>async def run_custom_key_provider_demo():     print(\"--- Custom KeyProvider Demo ---\")</p> <pre><code>if not os.getenv(\"MY_APP_OPENAI_KEY\"):\n    print(\"ERROR: MY_APP_OPENAI_KEY environment variable not set. Please set it to run this demo.\")\n    return\n\nmy_key_provider = MyAppCustomKeyProvider()\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"openai\",\n        llm_openai_model_name=\"gpt-3.5-turbo\"\n    )\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with custom KeyProvider...\")\n    genie = await Genie.create(\n        config=app_config,\n        key_provider_instance=my_key_provider\n    )\n    print(\"Genie initialized!\")\n\n    messages: List[ChatMessage] = [\n        {\"role\": \"user\", \"content\": \"Hello via custom KeyProvider! What's a fun fact?\"}\n    ]\n    print(f\"\\nSending to OpenAI: {messages[0]['content']}\")\n\n    response = await genie.llm.chat(messages)\n\n    assistant_response = response.get(\"message\", {}).get(\"content\", \"No content received.\")\n    print(f\"\\nOpenAI (via custom KeyProvider) says: {assistant_response}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Custom KeyProvider demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_custom_key_provider_demo())</p>"},{"location":"tutorials/E13_google_search_demo/","title":"Tutorial: Google Search Tool (E13)","text":"<p>This tutorial corresponds to the example file <code>examples/E13_google_search_demo.py</code>.</p> <p>It demonstrates how to use the built-in <code>GoogleSearchTool</code>. It shows how to: - Enable the tool in <code>tool_configurations</code>. - Provide the necessary API keys (<code>GOOGLE_API_KEY</code>, <code>GOOGLE_CSE_ID</code>) via environment variables. - Execute the tool directly using <code>genie.execute_tool()</code>.</p>"},{"location":"tutorials/E13_google_search_demo/#example-code","title":"Example Code","text":""},{"location":"tutorials/E13_google_search_demo/#examplese13_google_search_demopy","title":"examples/E13_google_search_demo.py","text":"<p>\"\"\" Example: GoogleSearchTool Demo</p> <p>This example demonstrates configuring and using the GoogleSearchTool via the Genie facade.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Set Environment Variables:    - <code>export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"</code>    - <code>export GOOGLE_CSE_ID=\"YOUR_CUSTOM_SEARCH_ENGINE_ID\"</code>    (Replace with your actual key and CSE ID from Google Cloud Console     and Programmable Search Engine setup) 3. Run from the root of the project:    <code>poetry run python examples/E13_google_search_demo.py</code></p> <p>The demo will: - Initialize Genie with the GoogleSearchTool. - Perform a search query using the tool. - Print the search results or an error message. \"\"\" import asyncio import json import logging import os</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_google_search_demo():     print(\"--- GoogleSearchTool Demo ---\")</p> <pre><code>if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n    print(\"\\nERROR: Please set GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables to run this demo.\")\n    print(\"Example: \")\n    print('  export GOOGLE_API_KEY=\"your_actual_api_key\"')\n    print('  export GOOGLE_CSE_ID=\"your_actual_cse_id\"')\n    return\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\"\n    ),\n    tool_configurations={\n        \"google_search_tool_v1\": {} # Enable the Google Search tool\n    }\n)\n\ngenie: Genie | None = None\ntry:\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized.\")\n\n    search_query = \"What is Retrieval Augmented Generation?\"\n    num_results_to_fetch = 3\n\n    print(f\"\\nAttempting to search for: '{search_query}' (max {num_results_to_fetch} results)...\")\n    search_result = await genie.execute_tool(\n        \"google_search_tool_v1\",\n        query=search_query,\n        num_results=num_results_to_fetch\n    )\n\n    print(\"\\nSearch Result:\")\n    print(json.dumps(search_result, indent=2))\n\n    if search_result.get(\"error\"):\n        print(f\"\\nSearch failed: {search_result['error']}\")\n    elif search_result.get(\"results\"):\n        print(f\"\\nSuccessfully retrieved {len(search_result['results'])} results.\")\n        for i, item in enumerate(search_result[\"results\"]):\n            print(f\"  Result {i+1}:\")\n            print(f\"    Title: {item.get('title')}\")\n            print(f\"    Link: {item.get('link')}\")\n            print(f\"    Snippet: {item.get('snippet')[:100]}...\")\n    else:\n        print(\"\\nSearch returned no results and no error.\")\n\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie facade torn down.\")\n</code></pre> <p>if name == \"main\":     logging.basicConfig(level=logging.INFO)     asyncio.run(run_google_search_demo())</p>"},{"location":"tutorials/E14_filesystem_tool_demo/","title":"Tutorial: FileSystem Tool (E14)","text":"<p>This tutorial corresponds to the example file <code>examples/E14_filesystem_tool_demo.py</code>.</p> <p>It demonstrates how to configure and use the <code>SandboxedFileSystemTool</code> for safe file operations. It shows how to: - Enable the tool and configure its <code>sandbox_base_path</code> in <code>tool_configurations</code>. - Perform <code>write_file</code>, <code>read_file</code>, and <code>list_directory</code> operations. - Verify that path traversal attempts outside the sandbox are prevented.</p>"},{"location":"tutorials/E14_filesystem_tool_demo/#example-code","title":"Example Code","text":""},{"location":"tutorials/E14_filesystem_tool_demo/#examplese14_filesystem_tool_demopy","title":"examples/E14_filesystem_tool_demo.py","text":"<p>\"\"\" Example: SandboxedFileSystemTool Demo</p> <p>This example demonstrates configuring and using the SandboxedFileSystemTool via the Genie facade.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Run from the root of the project:    <code>poetry run python examples/E14_filesystem_tool_demo.py</code></p> <p>The demo will: - Create a sandbox directory ('./my_agent_sandbox_demo_e14'). - Initialize Genie with the SandboxedFileSystemTool configured to use this sandbox. - Perform write, read, and list operations using the tool. - Clean up the sandbox directory. \"\"\" import asyncio import os import shutil from pathlib import Path</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.core.types import Plugin as CorePluginType from genie_tooling.genie import Genie from genie_tooling.security.key_provider import KeyProvider</p> <p>class DemoFSKeyProvider(KeyProvider, CorePluginType):     plugin_id = \"demo_fs_key_provider_e14_v1\"     async def get_key(self, key_name: str) -&gt; str | None: return os.environ.get(key_name)     async def setup(self, config=None): pass     async def teardown(self): pass</p> <p>async def run_fs_tool_demo():     print(\"--- SandboxedFileSystemTool Demo ---\")</p> <pre><code>sandbox_dir_name = \"my_agent_sandbox_demo_e14\"\nsandbox_path = Path(__file__).parent / sandbox_dir_name\n\nif sandbox_path.exists():\n    shutil.rmtree(sandbox_path)\nsandbox_path.mkdir(parents=True, exist_ok=True)\nprint(f\"Sandbox directory prepared at: {sandbox_path.resolve()}\")\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\"\n    ),\n    tool_configurations={\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": str(sandbox_path.resolve())}\n    }\n)\n\nkey_provider = DemoFSKeyProvider()\ngenie: Genie | None = None\n\ntry:\n    genie = await Genie.create(config=app_config, key_provider_instance=key_provider)\n    print(\"Genie facade initialized with SandboxedFileSystemTool.\")\n\n    file_to_write = \"test_doc.txt\"\n    content_to_write = \"Hello from the sandboxed world of Genie!\"\n    subdir_file = \"notes/important.md\"\n    subdir_content = \"# Project Ideas\\n- Build more cool agents!\"\n\n    print(f\"\\nAttempting to write '{file_to_write}'...\")\n    write_result = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"write_file\",\n        path=file_to_write,\n        content=content_to_write\n    )\n    print(f\"Write result: {write_result}\")\n    assert write_result.get(\"success\") is True\n\n    print(f\"\\nAttempting to write '{subdir_file}'...\")\n    write_subdir_result = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"write_file\",\n        path=subdir_file,\n        content=subdir_content,\n        overwrite=True\n    )\n    print(f\"Write to subdir result: {write_subdir_result}\")\n    assert write_subdir_result.get(\"success\") is True\n\n    print(f\"\\nAttempting to read '{file_to_write}'...\")\n    read_result = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"read_file\",\n        path=file_to_write\n    )\n    print(f\"Read result: {read_result}\")\n    assert read_result.get(\"success\") is True\n    assert read_result.get(\"content\") == content_to_write\n\n    print(\"\\nAttempting to list sandbox root directory '.' ...\")\n    list_result_root = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"list_directory\",\n        path=\".\"\n    )\n    print(f\"List directory (root) result: {list_result_root}\")\n    assert list_result_root.get(\"success\") is True\n    assert file_to_write in list_result_root.get(\"file_list\", [])\n    assert \"notes\" in list_result_root.get(\"file_list\", [])\n\n    print(\"\\nAttempting to list 'notes' subdirectory...\")\n    list_result_subdir = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"list_directory\",\n        path=\"notes\"\n    )\n    print(f\"List directory ('notes') result: {list_result_subdir}\")\n    assert list_result_subdir.get(\"success\") is True\n    assert Path(subdir_file).name in list_result_subdir.get(\"file_list\", [])\n\n    print(\"\\nAttempting path traversal (should fail)...\")\n    traversal_result = await genie.execute_tool(\n        \"sandboxed_fs_tool_v1\",\n        operation=\"read_file\",\n        path=\"../outside_file.txt\"\n    )\n    print(f\"Path traversal result: {traversal_result}\")\n    assert traversal_result.get(\"success\") is False\n    assert \"Path traversal attempt detected\" in traversal_result.get(\"message\", \"\")\n\nexcept Exception as e:\n    print(f\"\\nAn unexpected error occurred: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie facade torn down.\")\n\n    if sandbox_path.exists():\n        shutil.rmtree(sandbox_path)\n        print(f\"Sandbox directory '{sandbox_path.resolve()}' cleaned up.\")\n</code></pre> <p>if name == \"main\":     import logging     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     asyncio.run(run_fs_tool_demo())</p>"},{"location":"tutorials/E15_prompt_management_example/","title":"Tutorial: Prompt Management (E15)","text":"<p>This tutorial corresponds to the example file <code>examples/E15_prompt_management_example.py</code>.</p> <p>It demonstrates how to use the prompt management system (<code>genie.prompts</code>) to separate prompt templates from application code. It shows how to: - Configure <code>FileSystemPromptRegistryPlugin</code> to load templates from a directory. - Use different template engines (<code>BasicStringFormatTemplatePlugin</code>, <code>Jinja2ChatTemplatePlugin</code>). - Render simple string prompts and complex, structured chat prompts from templates.</p>"},{"location":"tutorials/E15_prompt_management_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E15_prompt_management_example/#examplese15_prompt_management_examplepy","title":"examples/E15_prompt_management_example.py","text":"<p>\"\"\" Example: Using the Prompt Management System (<code>genie.prompts</code>)</p> <p>This example demonstrates how to use <code>genie.prompts</code> to load and render prompt templates. It assumes you have a <code>FileSystemPromptRegistryPlugin</code> configured and some template files.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. The script will create a temporary prompt directory and files. 3. Run from the root of the project:    <code>poetry run python examples/E15_prompt_management_example.py</code> \"\"\" import asyncio import logging import shutil  # For cleanup from pathlib import Path from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.prompts.types import PromptData</p> <p>async def run_prompt_management_demo():     print(\"--- Prompt Management Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>prompt_dir = Path(__file__).parent / \"my_app_prompts_e15\" # Use example-local dir\nprompt_dir.mkdir(exist_ok=True)\n(prompt_dir / \"greeting.txt\").write_text(\"Hello, {name}! Welcome to {place}.\")\n(prompt_dir / \"chat_intro.j2\").write_text(\n    '[\\n'\n    '  {\"role\": \"system\", \"content\": \"You are a {{ bot_role }} assistant.\"},\\n'\n    '  {\"role\": \"user\", \"content\": \"Please tell me about {{ topic }}.\"}\\n'\n    ']'\n)\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        prompt_registry=\"file_system_prompt_registry\",\n    ),\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\n            \"base_path\": str(prompt_dir.resolve()),\n        }\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie for prompt management...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(\"\\n--- Available Templates ---\")\n    templates = await genie.prompts.list_templates()\n    if templates:\n        for t_id in templates:\n            print(f\"- Name: {t_id['name']}, Version: {t_id.get('version', 'N/A')}\")\n    else:\n        print(\"No templates found (check base_path and suffix).\")\n\n    print(\"\\n--- Raw Template Content (greeting.txt) ---\")\n    raw_greeting = await genie.prompts.get_prompt_template_content(name=\"greeting\")\n    print(raw_greeting)\n\n    print(\"\\n--- Rendered String Prompt (greeting.txt) ---\")\n    greeting_data: PromptData = {\"name\": \"Explorer\", \"place\": \"Genieville\"}\n    rendered_greeting = await genie.prompts.render_prompt(\n        name=\"greeting\",\n        data=greeting_data,\n        template_engine_id=\"basic_string_formatter\" # Explicitly use basic formatter\n    )\n    print(rendered_greeting)\n\n    print(\"\\n--- Rendered Chat Prompt (chat_intro.j2) ---\")\n    chat_data: PromptData = {\"bot_role\": \"helpful\", \"topic\": \"AI agents\"}\n    rendered_chat_messages = await genie.prompts.render_chat_prompt(\n        name=\"chat_intro\",\n        data=chat_data,\n        template_engine_id=\"jinja2_chat_formatter\" # Explicitly use Jinja2 formatter\n    )\n    print(rendered_chat_messages)\n\n    if rendered_chat_messages:\n        print(\"\\n--- Sending rendered chat to LLM ---\")\n        try:\n            chat_response = await genie.llm.chat(rendered_chat_messages)\n            print(f\"LLM Response: {chat_response['message']['content'][:150]}...\")\n        except Exception as e_llm:\n            print(f\"LLM chat error: {e_llm}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Prompt management demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n    if prompt_dir.exists():\n        shutil.rmtree(prompt_dir)\n        print(f\"Cleaned up demo prompt directory: {prompt_dir}\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_prompt_management_demo())</p>"},{"location":"tutorials/E16_conversation_state_example/","title":"Tutorial: Conversation State (E16)","text":"<p>This tutorial corresponds to the example file <code>examples/E16_conversation_state_example.py</code>.</p> <p>It demonstrates how to manage conversation history using <code>genie.conversation</code>. It shows how to: - Configure a <code>ConversationStateProviderPlugin</code> (e.g., <code>in_memory_convo_provider</code>). - Load, add messages to, and delete conversation state for a given session ID. - Observe how metadata like <code>created_at</code> and <code>last_updated</code> is automatically managed.</p>"},{"location":"tutorials/E16_conversation_state_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E16_conversation_state_example/#examplese16_conversation_state_examplepy","title":"examples/E16_conversation_state_example.py","text":"<p>\"\"\" Example: Using Conversation State Management (<code>genie.conversation</code>)</p> <p>This example demonstrates how to load, add messages to, and save conversation state using <code>genie.conversation</code>. It uses the in-memory state provider by default.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Run from the root of the project:    <code>poetry run python examples/E16_conversation_state_example.py</code> \"\"\" import asyncio import logging import uuid from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage</p> <p>async def run_conversation_state_demo():     print(\"--- Conversation State Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        conversation_state_provider=\"in_memory_convo_provider\"\n    ),\n)\n\ngenie: Optional[Genie] = None\nsession_id = f\"demo_session_e16_{uuid.uuid4().hex[:8]}\"\nprint(f\"Using Session ID: {session_id}\")\n\ntry:\n    print(\"\\nInitializing Genie for conversation state...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(f\"\\n1. Loading state for session '{session_id}'...\")\n    initial_state = await genie.conversation.load_state(session_id)\n    if initial_state:\n        print(f\"  Found existing state: {initial_state}\")\n    else:\n        print(\"  No existing state found (as expected for new session).\")\n\n    print(\"\\n2. Adding user message...\")\n    user_msg: ChatMessage = {\"role\": \"user\", \"content\": \"Hello Genie, how are you?\"}\n    await genie.conversation.add_message(session_id, user_msg)\n    print(f\"  Added: {user_msg}\")\n\n    state_after_user = await genie.conversation.load_state(session_id)\n    if state_after_user:\n        print(f\"  Current history: {state_after_user['history']}\")\n        print(f\"  Metadata: {state_after_user.get('metadata')}\")\n    else:\n        print(\"  Error: State not found after adding user message.\")\n        return\n\n    print(\"\\n3. Adding assistant message...\")\n    assistant_msg: ChatMessage = {\"role\": \"assistant\", \"content\": \"I am doing well, thank you for asking!\"}\n    await genie.conversation.add_message(session_id, assistant_msg)\n    print(f\"  Added: {assistant_msg}\")\n\n    final_state = await genie.conversation.load_state(session_id)\n    if final_state:\n        print(\"\\n--- Final Conversation State ---\")\n        print(f\"Session ID: {final_state['session_id']}\")\n        print(\"History:\")\n        for msg in final_state[\"history\"]:\n            print(f\"  - {msg['role']}: {msg['content']}\")\n        print(f\"Metadata: {final_state.get('metadata')}\")\n    else:\n        print(\"  Error: Final state not found.\")\n\n    print(f\"\\n4. Deleting state for session '{session_id}'...\")\n    deleted = await genie.conversation.delete_state(session_id)\n    print(f\"  Deletion successful: {deleted}\")\n\n    state_after_delete = await genie.conversation.load_state(session_id)\n    assert state_after_delete is None, \"State should be None after deletion.\"\n    print(\"  State confirmed deleted.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Conversation state demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_conversation_state_demo())</p>"},{"location":"tutorials/E17_observability_tracing_example/","title":"Tutorial: Observability &amp; Tracing (E17)","text":"<p>This tutorial corresponds to the example file <code>examples/E17_observability_tracing_example.py</code>.</p> <p>It demonstrates how to use Genie's built-in tracing capabilities for debugging and monitoring. It shows how to: - Configure a tracer plugin (e.g., <code>ConsoleTracerPlugin</code> or <code>OpenTelemetryTracerPlugin</code>). - Observe the automatic trace events generated by core <code>Genie</code> operations (like LLM calls). - Manually emit custom application-level trace events using <code>genie.observability.trace_event()</code>.</p>"},{"location":"tutorials/E17_observability_tracing_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E17_observability_tracing_example/#examplese17_observability_tracing_examplepy","title":"examples/E17_observability_tracing_example.py","text":"<p>\"\"\" Example: Using Observability and Interaction Tracing (<code>genie.observability</code>)</p> <p>This example demonstrates how to enable and use the interaction tracing feature of Genie Tooling. It shows configuration for both ConsoleTracerPlugin and the new OpenTelemetryTracerPlugin.</p> <p>To Run with Console Tracer: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Run from the root of the project:    <code>poetry run python examples/E17_observability_tracing_example.py</code>    You should see trace logs in your console output.</p> <p>To Run with OpenTelemetry Tracer (e.g., to Jaeger): 1. Ensure OTel dependencies are installed: <code>poetry install --extras observability</code> 2. Start an OTel collector (e.g., Jaeger all-in-one):    <code>docker run -d --name jaeger -e COLLECTOR_OTLP_ENABLED=true -p 16686:16686 -p 4317:4317 -p 4318:4318 jaegertracing/all-in-one:latest</code> 3. Modify the <code>app_config</code> below to use <code>otel_tracer</code> and configure the OTLP endpoint. 4. Run the script. Traces should appear in Jaeger UI (http://localhost:16686). \"\"\" import asyncio import logging import traceback import uuid from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_observability_demo():     print(\"--- Observability and Tracing Example ---\")</p> <pre><code>logging.basicConfig(level=logging.INFO)\n\napp_config_console = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        observability_tracer=\"console_tracer\"\n    ),\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": {\n            \"log_level\": \"INFO\"\n        }\n    }\n)\n\napp_config_otel = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        observability_tracer=\"otel_tracer\",\n        token_usage_recorder=\"otel_metrics_recorder\"\n    ),\n    observability_tracer_configurations={\n        \"otel_tracer_plugin_v1\": {\n            \"otel_service_name\": \"genie-e17-demo-app\",\n            \"otel_service_version\": \"0.1.0\",\n            \"exporter_type\": \"otlp_http\",\n            \"otlp_http_endpoint\": \"http://localhost:4318/v1/traces\",\n            \"resource_attributes\": {\"deployment.environment\": \"development_e17\"}\n        }\n    }\n)\n\n# --- CHOOSE CONFIGURATION TO RUN ---\napp_config = app_config_console\n# app_config = app_config_otel # Uncomment to use OpenTelemetry\n# ------------------------------------\n\ngenie: Optional[Genie] = None\ntry:\n    print(f\"\\nInitializing Genie with tracer: {app_config.features.observability_tracer}...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized! Automatic traces will now be generated.\")\n\n    print(\"\\n--- Performing operations that trigger automatic traces ---\")\n    try:\n        chat_response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Tracer! Tell me a short story.\"}])\n        print(f\"LLM Response: {chat_response['message']['content'][:60]}...\")\n    except Exception as e_llm:\n        print(f\"LLM call failed (expected if Ollama not running): {e_llm}\")\n\n    print(\"\\n--- Emitting a custom trace event ---\")\n    custom_correlation_id = str(uuid.uuid4())\n    await genie.observability.trace_event(\n        event_name=\"my_app.custom_operation.start\",\n        data={\"user_id\": \"test_user\", \"input_param\": \"example_value\"},\n        component=\"MyApplicationLogic\",\n        correlation_id=custom_correlation_id\n    )\n    await asyncio.sleep(0.1)\n    try:\n        raise ValueError(\"Something went wrong in custom op!\")\n    except ValueError as e_custom:\n        await genie.observability.trace_event(\n            event_name=\"my_app.custom_operation.error\",\n            data={\n                \"status\": \"failed\",\n                \"error_message\": str(e_custom),\n                \"error_type\": type(e_custom).__name__,\n                \"error_stacktrace\": traceback.format_exc()\n                },\n            component=\"MyApplicationLogic\",\n            correlation_id=custom_correlation_id\n        )\n    print(\"Custom trace events (including an error) emitted.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Observability demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_observability_demo())</p>"},{"location":"tutorials/E18_human_in_loop_example/","title":"Tutorial: Human-in-the-Loop (E18)","text":"<p>This tutorial corresponds to the example file <code>examples/E18_human_in_loop_example.py</code>.</p> <p>It demonstrates how to add a human approval step before executing critical actions. It shows how to: - Configure a <code>HumanApprovalRequestPlugin</code> (e.g., <code>cli_hitl_approver</code>). - See how <code>genie.run_command()</code> automatically triggers the HITL flow before executing a tool. - Understand the approval/denial workflow from the user's perspective.</p>"},{"location":"tutorials/E18_human_in_loop_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E18_human_in_loop_example/#examplese18_human_in_loop_examplepy","title":"examples/E18_human_in_loop_example.py","text":"<p>\"\"\" Example: Using Human-in-the-Loop (HITL) (<code>genie.human_in_loop</code>)</p> <p>This example demonstrates how to configure and use the Human-in-the-Loop feature for approvals. It uses the <code>CliApprovalPlugin</code> by default, which will prompt on the command line.</p> <p>This example will primarily show HITL integrated with <code>genie.run_command</code>.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Ensure Ollama is running and 'mistral:latest' is pulled for the LLM-assisted processor. 3. Run from the root of the project:    <code>poetry run python examples/E18_human_in_loop_example.py</code>    You will be prompted in the console to approve/deny tool execution. \"\"\" import asyncio import json import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_hitl_demo():     print(\"--- Human-in-the-Loop (HITL) Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        command_processor=\"llm_assisted\",\n        tool_lookup=\"embedding\",\n        hitl_approver=\"cli_hitl_approver\"\n    ),\n    tool_configurations={\n        \"calculator_tool\": {}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with HITL enabled...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(\"\\n--- `run_command` with HITL ---\")\n    command_text = \"What is 15 multiplied by 7?\"\n    print(f\"Sending command: '{command_text}'\")\n    print(\"The system will now select the 'calculator_tool' and then prompt for human approval before execution.\")\n\n    command_result = await genie.run_command(command_text)\n\n    print(\"\\nCommand Result after HITL:\")\n    print(json.dumps(command_result, indent=2, default=str))\n\n    if command_result and command_result.get(\"tool_result\"):\n        print(f\"\\nTool Result: {command_result['tool_result']}\")\n    elif command_result and \"hitl_decision\" in command_result and command_result[\"hitl_decision\"].get(\"status\") != \"approved\":\n        print(f\"\\nTool execution was {command_result['hitl_decision']['status']}. Reason: {command_result['hitl_decision'].get('reason')}\")\n    elif command_result and command_result.get(\"error\"):\n         print(f\"\\nCommand Error: {command_result['error']}\")\n    else:\n         print(f\"\\nCommand did not result in a tool call or expected HITL flow: {command_result}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"HITL demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_hitl_demo())</p>"},{"location":"tutorials/E19_llm_output_parsing_example/","title":"Tutorial: LLM Output Parsing (E19)","text":"<p>This tutorial corresponds to the example file <code>examples/E19_llm_output_parsing_example.py</code>.</p> <p>It demonstrates how to reliably extract structured data from an LLM's text response. It shows how to: - Use <code>genie.llm.parse_output()</code> to convert LLM text into Python objects. - Configure and use <code>JSONOutputParserPlugin</code> to extract JSON. - Configure and use <code>PydanticOutputParserPlugin</code> to parse JSON directly into a Pydantic model instance for validation and type safety.</p>"},{"location":"tutorials/E19_llm_output_parsing_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E19_llm_output_parsing_example/#examplese19_llm_output_parsing_examplepy","title":"examples/E19_llm_output_parsing_example.py","text":"<p>\"\"\" Example: Using LLM Output Parsing (<code>genie.llm.parse_output</code>)</p> <p>This example demonstrates how to use <code>genie.llm.parse_output</code> to parse structured data (like JSON) from an LLM's text response. It also shows parsing into a Pydantic model.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>).    (Pydantic is a core dependency). 2. Ensure Ollama is running and 'mistral:latest' is pulled (or configure a different LLM). 3. Run from the root of the project:    <code>poetry run python examples/E19_llm_output_parsing_example.py</code> \"\"\" import asyncio import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import LLMChatResponse from pydantic import BaseModel, Field</p> <p>class ExtractedInfo(BaseModel):     name: str = Field(description=\"The full name of the person.\")     age: Optional[int] = Field(None, description=\"The person's age, if mentioned.\")     city: Optional[str] = Field(None, description=\"The city the person lives in, if mentioned.\")</p> <p>async def run_llm_output_parsing_demo():     print(\"--- LLM Output Parsing Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        default_llm_output_parser=\"json_output_parser\" # Set default\n    )\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie for LLM output parsing...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(\"\\n--- Parsing to Dictionary (JSON) ---\")\n    prompt_for_json = (\n        \"Extract the name, age, and city from the following text \"\n        \"and return it as a JSON object: \"\n        \"'John Doe is 42 years old and lives in New York. He enjoys programming.' \"\n        \"Ensure your output is ONLY the JSON object.\"\n    )\n    print(f\"Sending prompt for JSON: {prompt_for_json}\")\n\n    try:\n        llm_response_json: LLMChatResponse = await genie.llm.chat(\n            [{\"role\": \"user\", \"content\": prompt_for_json}]\n        )\n        print(f\"LLM Raw Text Output: {llm_response_json['message']['content']}\")\n\n        parsed_dict = await genie.llm.parse_output(llm_response_json) # Uses default JSON parser\n        print(f\"Parsed Dictionary: {parsed_dict}\")\n        if isinstance(parsed_dict, dict):\n            print(f\"  Name from dict: {parsed_dict.get('name')}\")\n    except Exception as e_json:\n        print(f\"Error during JSON parsing: {e_json}\")\n\n\n    print(\"\\n--- Parsing to Pydantic Model ---\")\n    prompt_for_pydantic = (\n        \"From the text 'Alice is thirty and resides in London.', \"\n        \"extract the name, age, and city. Format as a JSON object \"\n        \"suitable for a model with fields: name (str), age (int, optional), city (str, optional).\"\n        \"Output ONLY the JSON object.\"\n    )\n    print(f\"Sending prompt for Pydantic: {prompt_for_pydantic}\")\n\n    try:\n        llm_response_pydantic: LLMChatResponse = await genie.llm.chat(\n             [{\"role\": \"user\", \"content\": prompt_for_pydantic}]\n        )\n        print(f\"LLM Raw Text Output: {llm_response_pydantic['message']['content']}\")\n\n        parsed_model_instance = await genie.llm.parse_output(\n            llm_response_pydantic,\n            parser_id=\"pydantic_output_parser_v1\",\n            schema=ExtractedInfo\n        )\n\n        if isinstance(parsed_model_instance, ExtractedInfo):\n            print(f\"Parsed Pydantic Model: {parsed_model_instance.model_dump()}\")\n            print(f\"  Name from model: {parsed_model_instance.name}\")\n            print(f\"  Age from model: {parsed_model_instance.age}\")\n        else:\n            print(f\"Parsing did not return an ExtractedInfo instance. Got: {type(parsed_model_instance)}\")\n\n    except Exception as e_pydantic:\n        print(f\"Error during Pydantic parsing: {e_pydantic}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"LLM output parsing demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_llm_output_parsing_demo())</p>"},{"location":"tutorials/E20_token_usage_example/","title":"Tutorial: Token Usage Tracking (E20)","text":"<p>This tutorial corresponds to the example file <code>examples/E20_token_usage_example.py</code>.</p> <p>It demonstrates how to monitor LLM token consumption for cost and performance analysis. It shows how to: - Configure a <code>TokenUsageRecorderPlugin</code> (e.g., <code>in_memory_token_recorder</code>). - Observe how token usage is automatically recorded after <code>genie.llm</code> calls. - Retrieve and interpret a usage summary using <code>genie.usage.get_summary()</code>.</p>"},{"location":"tutorials/E20_token_usage_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E20_token_usage_example/#examplese20_token_usage_examplepy","title":"examples/E20_token_usage_example.py","text":"<p>\"\"\" Example: Using Token Usage Tracking (<code>genie.usage</code>)</p> <p>This example demonstrates how to enable and use the token usage tracking feature of Genie Tooling. It shows configuration for both the <code>InMemoryTokenUsageRecorderPlugin</code> and the <code>OpenTelemetryMetricsTokenRecorderPlugin</code>.</p> <p>To Run with InMemory Recorder: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Ensure Ollama is running and 'mistral:latest' is pulled (or configure a different LLM). 3. Run from the root of the project:    <code>poetry run python examples/E20_token_usage_example.py</code></p> <p>To Run with OpenTelemetry Metrics Recorder (e.g., to Prometheus via OTel Collector): 1. Ensure OTel dependencies: <code>poetry install --extras observability</code> 2. Set up an OTel Collector configured to scrape Prometheus metrics and export them    (e.g., to a Prometheus instance). 3. Modify <code>app_config</code> below to use <code>token_usage_recorder=\"otel_metrics_recorder\"</code>    and ensure <code>observability_tracer=\"otel_tracer\"</code> is also configured for the OTel SDK to be initialized. 4. Run the script. Metrics should be available in your Prometheus/Grafana setup. \"\"\" import asyncio import json import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_token_usage_demo():     print(\"--- Token Usage Tracking Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>app_config_in_memory = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        token_usage_recorder=\"in_memory_token_recorder\"\n    )\n)\n\napp_config_otel_metrics = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        token_usage_recorder=\"otel_metrics_recorder\",\n        observability_tracer=\"otel_tracer\" # OTel SDK needs to be initialized\n    ),\n    observability_tracer_configurations={\n        \"otel_tracer_plugin_v1\": {\n            \"otel_service_name\": \"genie-e20-token-metrics-app\",\n            \"exporter_type\": \"console\" # Or \"otlp_http\" to send to a collector\n        }\n    }\n)\n\n# --- CHOOSE CONFIGURATION TO RUN ---\napp_config = app_config_in_memory\n# app_config = app_config_otel_metrics # Uncomment to use OTel Metrics\n# ------------------------------------\n\ngenie: Optional[Genie] = None\ntry:\n    print(f\"\\nInitializing Genie with token usage recorder: {app_config.features.token_usage_recorder}...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(\"\\n--- Making LLM calls ---\")\n    try:\n        await genie.llm.chat([{\"role\": \"user\", \"content\": \"Tell me a short story about a robot.\"}])\n        print(\"First LLM chat call complete.\")\n        await genie.llm.generate(\"What is the capital of France?\")\n        print(\"Second LLM generate call complete.\")\n        await genie.llm.chat([{\"role\": \"user\", \"content\": \"Another question for the same model.\"}])\n        print(\"Third LLM chat call complete.\")\n\n    except Exception as e_llm:\n        print(f\"LLM call failed (expected if Ollama not running): {e_llm}\")\n        print(\"Token usage summary might be empty or incomplete.\")\n\n    print(\"\\n--- Token Usage Summary ---\")\n    # For in-memory, get_summary returns a dict. For OTel, it returns a status message.\n    summary = await genie.usage.get_summary()\n    print(json.dumps(summary, indent=2))\n\n    if app_config.features.token_usage_recorder == \"in_memory_token_recorder\" and summary:\n        recorder_id = \"in_memory_token_usage_recorder_v1\"\n        if recorder_id in summary and isinstance(summary[recorder_id], dict) and isinstance(summary[recorder_id].get(\"by_model\"), dict):\n            print(\"\\nBreakdown by model (for in_memory_token_recorder):\")\n            for model_name, data in summary[recorder_id][\"by_model\"].items():\n                print(f\"  Model: {model_name}\")\n                print(f\"    Calls: {data.get('count')}\")\n                print(f\"    Prompt Tokens: {data.get('prompt')}\")\n                print(f\"    Completion Tokens: {data.get('completion')}\")\n                print(f\"    Total Tokens: {data.get('total')}\")\n    elif app_config.features.token_usage_recorder == \"otel_metrics_recorder\":\n        print(\"\\nFor 'otel_metrics_recorder', view metrics in your OpenTelemetry backend (e.g., Prometheus/Grafana).\")\n        print(\"Example Prometheus queries:\")\n        print(\"  - sum(rate(llm_request_tokens_prompt_total[5m])) by (llm_model_name)\")\n        print(\"  - sum(rate(llm_request_tokens_completion_total[5m])) by (llm_provider_id)\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Token usage demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_token_usage_demo())</p>"},{"location":"tutorials/E21_guardrails_example/","title":"Tutorial: Guardrails (E21)","text":"<p>This tutorial corresponds to the example file <code>examples/E21_guardrails_example.py</code>.</p> <p>It demonstrates how to enforce safety policies on LLM inputs and outputs. It shows how to: - Configure the built-in <code>KeywordBlocklistGuardrailPlugin</code>. - See how input guardrails can block potentially harmful prompts before they reach the LLM. - See how output guardrails can sanitize or block LLM responses that violate a policy.</p>"},{"location":"tutorials/E21_guardrails_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E21_guardrails_example/#examplese21_guardrails_examplepy","title":"examples/E21_guardrails_example.py","text":"<p>\"\"\" Example: Using Guardrails</p> <p>This example demonstrates how to configure and use Guardrails in Genie Tooling. It uses the built-in <code>KeywordBlocklistGuardrailPlugin</code>.</p> <p>To Run: 1. Ensure Genie Tooling is installed (<code>poetry install --all-extras</code>). 2. Ensure Ollama is running and 'mistral:latest' is pulled (or configure a different LLM). 3. Run from the root of the project:    <code>poetry run python examples/E21_guardrails_example.py</code>    Observe how inputs/outputs containing 'secret' or 'forbidden' are handled. \"\"\" import asyncio import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_guardrails_demo():     print(\"--- Guardrails Example ---\")     logging.basicConfig(level=logging.INFO)</p> <pre><code>app_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\",\n        llm_ollama_model_name=\"mistral:latest\",\n        command_processor=\"llm_assisted\",\n        tool_lookup=\"embedding\",\n        input_guardrails=[\"keyword_blocklist_guardrail\"],\n        output_guardrails=[\"keyword_blocklist_guardrail\"],\n    ),\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": {\n            \"blocklist\": [\"secret\", \"forbidden_word\", \"sensitive_operation\"],\n            \"case_sensitive\": False,\n            \"action_on_match\": \"block\"\n        }\n    },\n    tool_configurations={\n        \"calculator_tool\": {}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie with Guardrails enabled...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized!\")\n\n    print(\"\\n--- Testing Input Guardrail (LLM Chat) ---\")\n    blocked_input_prompt = \"Tell me a secret about the project.\"\n    print(f\"Sending potentially blocked input: '{blocked_input_prompt}'\")\n    try:\n        chat_response_blocked = await genie.llm.chat([{\"role\": \"user\", \"content\": blocked_input_prompt}])\n        print(f\"LLM Response (should not be reached if blocked): {chat_response_blocked['message']['content']}\")\n    except PermissionError as e_perm:\n        print(f\"Input Guardrail Blocked: {e_perm}\")\n    except Exception as e_llm_in:\n        print(f\"LLM chat error (possibly unrelated to guardrail): {e_llm_in}\")\n\n    allowed_input_prompt = \"Tell me a fun fact.\"\n    print(f\"\\nSending allowed input: '{allowed_input_prompt}'\")\n    try:\n        chat_response_allowed = await genie.llm.chat([{\"role\": \"user\", \"content\": allowed_input_prompt}])\n        print(f\"LLM Response: {chat_response_allowed['message']['content'][:100]}...\")\n    except Exception as e_llm_allowed:\n        print(f\"LLM chat error for allowed input: {e_llm_allowed}\")\n\n\n    print(\"\\n--- Testing Output Guardrail (LLM Generate) ---\")\n    prompt_for_blocked_output = \"Write a sentence that includes the word 'secret'.\"\n    print(f\"Sending prompt for potentially blocked output: '{prompt_for_blocked_output}'\")\n    try:\n        gen_response = await genie.llm.generate(prompt_for_blocked_output)\n        print(f\"LLM Generated Text: {gen_response['text']}\")\n        if \"[RESPONSE BLOCKED\" in gen_response[\"text\"]:\n            print(\"Output Guardrail successfully blocked the response.\")\n        elif \"secret\" in gen_response[\"text\"].lower():\n             print(\"Warning: Output guardrail did not block 'secret' as expected. LLM output was: \", gen_response[\"text\"])\n    except Exception as e_gen:\n        print(f\"LLM generate error: {e_gen}\")\n\n    print(\"\\n--- (Illustrative) Tool Usage Guardrail ---\")\n    print(\"To test tool usage guardrails, enable 'tool_usage_guardrails' in FeatureSettings\")\n    print(\"and try a command that would pass a blocked keyword as a tool parameter.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Guardrails demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_guardrails_demo())</p>"},{"location":"tutorials/E22_distributed_task_example/","title":"Tutorial: Distributed Tasks (E22)","text":"<p>This tutorial corresponds to the example file <code>examples/E22_distributed_task_example.py</code>.</p> <p>It demonstrates how to offload long-running tasks to a distributed queue like Celery or RQ. It shows how to: - Configure a <code>DistributedTaskQueuePlugin</code> for Celery or RQ. - Use <code>genie.task_queue.submit_task()</code> to send a task to a worker. - Use <code>genie.task_queue.get_task_status()</code> and <code>genie.task_queue.get_task_result()</code> to monitor and retrieve results.</p> <p>Note: This example focuses on the client-side API. A running Celery/RQ worker environment is required to execute the tasks.</p>"},{"location":"tutorials/E22_distributed_task_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E22_distributed_task_example/#examplese22_distributed_task_examplepy","title":"examples/E22_distributed_task_example.py","text":"<p>\"\"\" Example: Distributed Task Offloading (Conceptual)</p> <p>This example outlines how one might use the Distributed Task Queue feature with Celery or RQ. It assumes: 1. Celery/RQ and a broker (e.g., Redis) are installed. 2. A worker is running and configured to find tasks. 3. A task (e.g., <code>execute_genie_tool_task</code>) is defined for the worker    that can instantiate a minimal Genie environment or directly execute a tool.</p> <p>This example focuses on the Genie client-side configuration and submission. The worker-side task implementation is beyond this basic example.</p> <p>To Run (Conceptual - requires worker setup): 1. Start Redis: <code>docker run -d -p 6379:6379 redis</code> 2. Start a Celery or RQ worker (details depend on your task definitions). 3. Run this script: <code>poetry run python examples/E22_distributed_task_example.py</code> \"\"\" import asyncio import logging from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>REMOTE_TOOL_EXEC_TASK_NAME = \"genie_tooling.worker_tasks.execute_genie_tool_task\" # Placeholder</p> <p>async def run_distributed_task_demo():     print(\"--- Distributed Task Offloading Demo (Conceptual) ---\")     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)</p> <pre><code># --- Configuration for Celery ---\napp_config_celery = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\",\n        task_queue=\"celery\",\n        task_queue_celery_broker_url=\"redis://localhost:6379/1\",\n        task_queue_celery_backend_url=\"redis://localhost:6379/2\",\n    ),\n    distributed_task_queue_configurations={\n        \"celery_task_queue_v1\": {\n            \"celery_app_name\": \"genie_example_tasks_celery\",\n        }\n    }\n)\n\n# --- Configuration for RQ (Redis Queue) ---\napp_config_rq = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"none\",\n        command_processor=\"none\",\n        task_queue=\"rq\",\n    ),\n    distributed_task_queue_configurations={\n        \"redis_queue_task_plugin_v1\": { # Canonical ID for RQ plugin\n            \"redis_url\": \"redis://localhost:6379/3\", # Separate Redis DB for RQ\n            \"default_queue_name\": \"genie-rq-jobs\"\n        }\n    }\n)\n\n# Select the configuration to use\napp_config = app_config_celery\n# app_config = app_config_rq # Uncomment to test with RQ\n\nprint(f\"Using task queue: {app_config.features.task_queue}\")\n\ngenie: Optional[Genie] = None\ntry:\n    genie = await Genie.create(config=app_config)\n    print(f\"Genie initialized with {app_config.features.task_queue} task queue support.\")\n\n    tool_exec_params = {\n        \"tool_id\": \"calculator_tool\",\n        \"tool_params\": {\"num1\": 200, \"num2\": 25, \"operation\": \"multiply\"},\n        \"context_info\": {\"user_id\": \"demo_user\"}\n    }\n\n    task_id_tool = await genie.task_queue.submit_task(\n        task_name=REMOTE_TOOL_EXEC_TASK_NAME,\n        kwargs=tool_exec_params\n    )\n\n    if task_id_tool:\n        print(f\"Tool execution task for 'calculator_tool' submitted with ID: {task_id_tool}\")\n        result_output = \"Polling for result...\"\n        for i in range(15): # Poll for up to 15 seconds\n            status = await genie.task_queue.get_task_status(task_id_tool)\n            print(f\"  (Poll {i+1}) Task {task_id_tool} status: {status}\")\n            if status == \"success\":\n                tool_result = await genie.task_queue.get_task_result(task_id_tool)\n                result_output = f\"Tool task '{task_id_tool}' successful. Result: {tool_result}\"\n                break\n            elif status in [\"failure\", \"revoked\"]:\n                result_output = f\"Tool task '{task_id_tool}' failed or revoked. Status: {status}\"\n                try:\n                    error_details = await genie.task_queue.get_task_result(task_id_tool)\n                    result_output += f\" Details: {error_details}\"\n                except Exception:\n                    pass # Error details might not be available or might raise\n                break\n            await asyncio.sleep(1)\n        print(result_output)\n    else:\n        print(\"Failed to submit tool execution task.\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n    logging.exception(\"Distributed task demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     print(\"This example is conceptual and requires a configured Celery/RQ worker.\")     print(\"It demonstrates the client-side API usage for both.\")     asyncio.run(run_distributed_task_demo())</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/","title":"Tutorial: Llama.cpp Server E2E (E23)","text":"<p>This tutorial corresponds to the example file <code>examples/E23_local_e2e_llama_cpp_server.py</code>.</p> <p>It provides a comprehensive end-to-end test using the Llama.cpp server provider. It covers most of Genie's features in a single, integrated script, including: - LLM chat and generation with Pydantic parsing (GBNF). - RAG indexing and search. - Custom tool registration and execution. - Command processing with HITL. - Prompt management and conversation state. - Guardrails and token usage tracking. - A simple ReAct agent loop.</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#example-code","title":"Example Code","text":""},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#examplese23_local_e2e_llama_cpp_serverpy","title":"examples/E23_local_e2e_llama_cpp_server.py","text":"<p>\"\"\" End-to-End Test for Genie Tooling with Llama.cpp Server Provider</p> <p>This example demonstrates a comprehensive flow using the Genie facade, targeting a Llama.cpp server (Ollama-compatible API) for LLM operations. It covers LLM chat/generate with Pydantic parsing, RAG, custom tool execution, command processing with HITL, prompt management, conversation state, guardrails, and a simple ReActAgent.</p> <p>It also demonstrates the use of the <code>@traceable</code> decorator for adding custom application logic to the observability trace.</p> <p>Prerequisites: 1. <code>genie-tooling</code> installed (<code>poetry install --all-extras</code>). 2. A Llama.cpp server running and accessible, typically at <code>http://localhost:8080</code>.    The server should be configured with a GBNF-compatible model (e.g., Mistral).    You can set the model alias used by the server via the <code>LLAMA_CPP_MODEL_ALIAS</code>    environment variable (defaults to \"mistral:latest\" if not set).    Example server command:    <code>./server -m mistral-7b-instruct-v0.2.Q4_K_M.gguf -c 4096 --host 0.0.0.0 --port 8080 --api-key mysecretkey --model-alias mistral:latest --cont-batching --embedding --gbnf-enabled</code>    (Adjust model path and other server parameters as needed. If using an API key,     set <code>LLAMA_CPP_API_KEY=\"mysecretkey\"</code> in your environment for this script.) \"\"\" import asyncio import json import logging import os import shutil import uuid from pathlib import Path from typing import Any, Dict, Optional</p> <p>from genie_tooling import tool from genie_tooling.agents.react_agent import ReActAgent from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.observability import traceable  # Import the new decorator from pydantic import BaseModel from pydantic import Field as PydanticField</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#-configuration-","title":"--- Configuration ---","text":"<p>LLAMA_CPP_BASE_URL = os.getenv(\"LLAMA_CPP_BASE_URL\", \"http://localhost:8080\") LLAMA_CPP_MODEL_ALIAS = os.getenv(\"LLAMA_CPP_MODEL_ALIAS\", \"mistral:latest\") # Model alias server uses LLAMA_CPP_API_KEY_NAME = \"LLAMA_CPP_API_KEY\" # Env var name for the API key if server needs one</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#-pydantic-model-for-llm-output-parsing-","title":"--- Pydantic Model for LLM Output Parsing ---","text":"<p>class ExtractedDetails(BaseModel):     item_name: str = PydanticField(description=\"The name of the item.\")     quantity: int = PydanticField(gt=0, description=\"The quantity of the item.\")     notes: Optional[str] = PydanticField(None, description=\"Optional notes about the item.\")</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#-helper-function-decorated-with-traceable-","title":"--- Helper function decorated with @traceable ---","text":"<p>@traceable async def _get_size_from_path(file_path: Path, context: Dict[str, Any]) -&gt; int:     \"\"\"A helper function to demonstrate custom tracing.\"\"\"     # This function's execution will appear as a nested span in the trace.     # The 'file_path' argument will be automatically added as a span attribute.     logging.info(f\"[_get_size_from_path] Getting size for: {file_path}\")     return file_path.stat().st_size</p>"},{"location":"tutorials/E23_local_e2e_llama_cpp_server/#-custom-tool-definition-","title":"--- Custom Tool Definition ---","text":"<p>@tool async def get_file_metadata(file_path: str, context: Dict[str, Any]) -&gt; Dict[str, Any]:     \"\"\"     Retrieves metadata for a specified file within the agent's sandbox.     Args:         file_path (str): The relative path to the file within the agent's sandbox.         context (Dict[str, Any]): The invocation context, used for tracing.     Returns:         Dict[str, Any]: A dictionary containing file metadata (name, size, exists) or an error.     \"\"\"     sandbox_base = Path(\"./e23_agent_sandbox\")     try:         prospective_path = (sandbox_base / file_path).resolve()         if not str(prospective_path).startswith(str(sandbox_base.resolve())):             return {\"error\": \"Path traversal attempt detected.\", \"path_resolved\": str(prospective_path)}         full_path = prospective_path     except Exception as e:         return {\"error\": f\"Path resolution error: {e!s}\"}</p> <pre><code>if full_path.exists() and full_path.is_file():\n    # Call the traceable helper function, passing the context through\n    file_size = await _get_size_from_path(file_path=full_path, context=context)\n    return {\"file_name\": full_path.name, \"size_bytes\": file_size, \"exists\": True}\nelse:\n    return {\"error\": \"File not found or is not a file.\", \"path_checked\": str(full_path), \"exists\": False}\n</code></pre> <p>async def run_local_e2e_llama_cpp_server():     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     # logging.getLogger(\"genie_tooling.llm_providers.impl.llama_cpp_provider\").setLevel(logging.DEBUG)</p> <pre><code>print(f\"--- Genie Tooling Local E2E Test (Llama.cpp Server @ {LLAMA_CPP_BASE_URL} with model {LLAMA_CPP_MODEL_ALIAS}) ---\")\nif os.getenv(LLAMA_CPP_API_KEY_NAME):\n    print(f\"Using API Key from environment variable: {LLAMA_CPP_API_KEY_NAME}\")\nelse:\n    print(f\"No API Key ({LLAMA_CPP_API_KEY_NAME}) found in environment. Assuming server does not require one.\")\n\n\nsandbox_dir = Path(\"./e23_agent_sandbox\")\nrag_data_dir = sandbox_dir / \"rag_docs\"\nprompt_dir = Path(\"./e23_prompts\")\n\nfor p in [sandbox_dir, rag_data_dir, prompt_dir]:\n    if p.exists(): shutil.rmtree(p)\n    p.mkdir(parents=True, exist_ok=True)\n\n(rag_data_dir / \"doc1.txt\").write_text(\"Llama.cpp is a C/C++ port of Llama for fast inference.\")\n(rag_data_dir / \"doc2.txt\").write_text(\"Genie Tooling uses llama.cpp via its Ollama-compatible API for structured output.\")\n(prompt_dir / \"greeting_template.j2\").write_text(\n    '[{\"role\": \"system\", \"content\": \"You are {{ bot_name }}.\"},\\n'\n    ' {\"role\": \"user\", \"content\": \"Hello! My name is {{ user_name }}.\"}]'\n)\n(sandbox_dir / \"testfile.txt\").write_text(\"This is a test file for metadata.\")\n(prompt_dir / \"react_agent_system_prompt_v1.j2\").write_text(\n    \"You are ReActBot. Your goal is: {{ goal }}.\\n\"\n    \"Available tools:\\n{{ tool_definitions }}\\n\"\n    \"Scratchpad (Thought/Action/Observation cycles):\\n{{ scratchpad }}\\nThought:\"\n)\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp\", # Target Llama.cpp server\n        llm_llama_cpp_base_url=LLAMA_CPP_BASE_URL,\n        llm_llama_cpp_model_name=LLAMA_CPP_MODEL_ALIAS,\n        llm_llama_cpp_api_key_name=LLAMA_CPP_API_KEY_NAME if os.getenv(LLAMA_CPP_API_KEY_NAME) else None,\n\n        command_processor=\"llm_assisted\",\n        command_processor_formatter_id_alias=\"compact_text_formatter\",\n        tool_lookup=\"embedding\",\n        tool_lookup_embedder_id_alias=\"st_embedder\",\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\",\n        rag_loader=\"file_system\",\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\",\n        cache=\"in-memory\",\n        observability_tracer=\"console_tracer\",\n        hitl_approver=\"cli_hitl_approver\",\n        token_usage_recorder=\"in_memory_token_recorder\",\n        input_guardrails=[\"keyword_blocklist_guardrail\"],\n        prompt_registry=\"file_system_prompt_registry\",\n        prompt_template_engine=\"jinja2_chat_formatter\",\n        conversation_state_provider=\"in_memory_convo_provider\",\n        default_llm_output_parser=\"pydantic_output_parser\"\n    ),\n    # The @tool decorated `get_file_metadata` is auto-enabled by default.\n    # Class-based tools like calculator and sandboxed_fs still need to be enabled.\n    tool_configurations={\n        \"calculator_tool\": {},\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": str(sandbox_dir.resolve())},\n    },\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": {\n            \"blocklist\": [\"super_secret_project_X\", \"highly_classified_info\"],\n            \"action_on_match\": \"block\"\n        }\n    },\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\n            \"base_path\": str(prompt_dir.resolve()),\n            \"template_suffix\": \".j2\"\n        }\n    },\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": {\"log_level\": \"INFO\"}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\n[1] Initializing Genie facade...\")\n    genie = await Genie.create(config=app_config)\n    await genie.register_tool_functions([get_file_metadata])\n    print(\"Genie facade initialized and custom tool registered!\")\n\n    print(\"\\n[2] Testing LLM Chat and Generate (with Pydantic parsing)...\")\n    try:\n        chat_resp = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Llama.cpp server! Write a short, friendly greeting.\"}])\n        print(f\"  LLM Chat Response: {chat_resp['message']['content'][:100]}...\")\n\n        gen_prompt = \"Extract details: Item is 'SuperWidget', quantity is 55, notes: 'Handle with care'.\"\n        gen_resp = await genie.llm.generate(\n            prompt=f\"You must output ONLY a valid JSON object. {gen_prompt}\",\n            output_schema=ExtractedDetails, # For GBNF\n            temperature=0.1,\n            n_predict=256 # Llama.cpp server needs n_predict for GBNF with /v1/completions\n        )\n        print(f\"  LLM Generate (raw text for Pydantic): {gen_resp['text']}\")\n        parsed_details = await genie.llm.parse_output(gen_resp, schema=ExtractedDetails)\n        if isinstance(parsed_details, ExtractedDetails):\n            print(f\"  Parsed Pydantic: Name='{parsed_details.item_name}', Qty='{parsed_details.quantity}', Notes='{parsed_details.notes}'\")\n            assert parsed_details.item_name == \"SuperWidget\"\n            assert parsed_details.quantity == 55\n        else:\n            assert False, f\"Pydantic parsing failed, got type: {type(parsed_details)}\"\n    except Exception as e_llm:\n        print(f\"  LLM Error: {e_llm} (Is Llama.cpp server running with model '{LLAMA_CPP_MODEL_ALIAS}' at {LLAMA_CPP_BASE_URL} and GBNF enabled?)\")\n        raise\n\n    print(\"\\n[3] Testing RAG (indexing and search)...\")\n    try:\n        rag_collection_name = \"e23_llama_docs\"\n        index_result = await genie.rag.index_directory(str(rag_data_dir.resolve()), collection_name=rag_collection_name)\n        print(f\"  Indexed documents from '{rag_data_dir.resolve()}'. Result: {index_result}\")\n        assert index_result.get(\"status\") == \"success\"\n        rag_results = await genie.rag.search(\"What is Llama.cpp?\", collection_name=rag_collection_name, top_k=1)\n        if rag_results:\n            print(f\"  RAG Search Result: '{rag_results[0].content[:100]}...' (Score: {rag_results[0].score:.2f})\")\n            assert \"Llama.cpp\" in rag_results[0].content\n        else:\n            assert False, \"RAG search returned no results\"\n    except Exception as e_rag:\n        print(f\"  RAG Error: {e_rag}\")\n        raise\n\n    print(\"\\n[4] Testing direct custom tool execution (get_file_metadata)...\")\n    try:\n        metadata_result = await genie.execute_tool(\"get_file_metadata\", file_path=\"testfile.txt\")\n        print(f\"  Metadata for 'testfile.txt': {metadata_result}\")\n        assert metadata_result.get(\"file_name\") == \"testfile.txt\"\n        assert metadata_result.get(\"exists\") is True\n    except Exception as e_tool_direct:\n        print(f\"  Direct tool execution error: {e_tool_direct}\")\n        raise\n\n    print(\"\\n[5] Testing `run_command` (LLM-assisted, HITL)...\")\n    try:\n        command_text = \"What is the size of the file named testfile.txt in the sandbox?\"\n        print(f\"  Sending command: '{command_text}' (Approval may be requested on CLI)\")\n        command_result = await genie.run_command(command_text)\n        print(f\"  `run_command` result: {json.dumps(command_result, indent=2, default=str)}\")\n        assert command_result.get(\"tool_result\", {}).get(\"size_bytes\") is not None or \\\n               command_result.get(\"hitl_decision\", {}).get(\"status\") != \"approved\", \\\n               f\"Tool did not run or HITL was not approved. Result: {command_result}\"\n    except Exception as e_run_cmd:\n        print(f\"  `run_command` error: {e_run_cmd}\")\n        raise\n\n    print(\"\\n[6] Testing Prompt Management...\")\n    try:\n        prompt_data = {\"bot_name\": \"E23-Bot\", \"user_name\": \"Tester\"}\n        rendered_chat_prompt = await genie.prompts.render_chat_prompt(name=\"greeting_template\", data=prompt_data)\n        assert rendered_chat_prompt is not None and len(rendered_chat_prompt) == 2\n        print(f\"  Rendered chat prompt: {rendered_chat_prompt}\")\n    except Exception as e_prompt:\n        print(f\"  Prompt management error: {e_prompt}\")\n        raise\n\n    print(\"\\n[7] Testing Conversation State...\")\n    try:\n        session_id = f\"e23_session_{uuid.uuid4().hex[:6]}\"\n        await genie.conversation.add_message(session_id, {\"role\": \"user\", \"content\": \"First turn in e23 test.\"})\n        await genie.conversation.add_message(session_id, {\"role\": \"assistant\", \"content\": \"Acknowledged first turn.\"})\n        state = await genie.conversation.load_state(session_id)\n        assert state is not None and len(state[\"history\"]) == 2\n        print(f\"  Conversation history for {session_id} (last 2): {state['history'][-2:]}\")\n    except Exception as e_convo:\n        print(f\"  Conversation state error: {e_convo}\")\n        raise\n\n    print(\"\\n[8] Testing Input Guardrail...\")\n    try:\n        blocked_input = \"Tell me about super_secret_project_X.\"\n        print(f\"  Sending potentially blocked input: '{blocked_input}'\")\n        await genie.llm.chat([{\"role\": \"user\", \"content\": blocked_input}])\n        assert False, \"Input guardrail did not block as expected.\"\n    except PermissionError as e_perm:\n        print(f\"  Guardrail test: Input successfully blocked: {e_perm}\")\n    except Exception as e_guard:\n        print(f\"  Guardrail test error (possibly unrelated to guardrail): {e_guard}\")\n        raise\n\n    print(\"\\n[9] Testing ReActAgent...\")\n    try:\n        react_agent = ReActAgent(genie=genie, agent_config={\"max_iterations\": 3})\n        agent_goal = \"What is 15 plus 7 using the calculator?\"\n        print(f\"  Agent Goal: '{agent_goal}' (Tool use by agent does not trigger interactive HITL here)\")\n        agent_result = await react_agent.run(goal=agent_goal)\n        print(f\"  ReActAgent Result Status: {agent_result['status']}\")\n        print(f\"  ReActAgent Output: {str(agent_result['output'])[:200]}...\")\n        assert agent_result[\"status\"] == \"success\"\n        assert \"22\" in str(agent_result[\"output\"])\n    except Exception as e_agent:\n        print(f\"  ReActAgent Error: {e_agent}\")\n        raise\n\n    print(\"\\n[10] Testing Token Usage Summary...\")\n    try:\n        usage_summary = await genie.usage.get_summary()\n        print(f\"  Token Usage: {json.dumps(usage_summary, indent=2)}\")\n        recorder_id = \"in_memory_token_usage_recorder_v1\"\n        assert recorder_id in usage_summary, f\"Recorder '{recorder_id}' not found in usage summary.\"\n        assert usage_summary[recorder_id][\"total_records\"] &gt; 0\n    except Exception as e_usage:\n        print(f\"  Token usage error: {e_usage}\")\n        raise\n\n    print(\"\\n--- E2E Test PASSED ---\")\n\nexcept Exception as e_main:\n    print(f\"\\nE2E Test FAILED with critical error: {e_main}\")\n    logging.error(\"E2E Main Error\", exc_info=True)\n    raise\nfinally:\n    if genie:\n        print(\"\\n[11] Tearing down Genie facade...\")\n        await genie.close()\n        print(\"Genie facade torn down.\")\n\n    print(\"\\n[12] Cleaning up test files/directories...\")\n    for p_cleanup in [sandbox_dir, prompt_dir]:\n        if p_cleanup.exists(): shutil.rmtree(p_cleanup, ignore_errors=True)\n    print(\"Cleanup complete.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_local_e2e_llama_cpp_server())</p>"},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/","title":"Tutorial: Llama.cpp Internal E2E (E24)","text":"<p>This tutorial corresponds to the example file <code>examples/E24_local_e2e_llama_cpp_internal.py</code>.</p> <p>It provides a comprehensive end-to-end test using the Llama.cpp internal provider, which runs GGUF models directly without a separate server. It covers the same range of features as the Llama.cpp server E2E test, demonstrating a fully local and self-contained agent setup.</p> <p>Prerequisite: You must download a GGUF model file and update the path in the example script.</p>"},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#example-code","title":"Example Code","text":""},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#examplese24_local_e2e_llama_cpp_internalpy","title":"examples/E24_local_e2e_llama_cpp_internal.py","text":"<p>\"\"\" End-to-End Test for Genie Tooling with Llama.cpp Internal Provider</p> <p>This example demonstrates a comprehensive flow using the Genie facade, specifically targeting the Llama.cpp internal provider for LLM operations. It covers LLM chat/generate with Pydantic parsing, RAG, custom tool execution, command processing with HITL, prompt management, conversation state, guardrails, and a simple ReActAgent.</p> <p>Prerequisites: 1. <code>genie-tooling</code> installed (<code>poetry install --all-extras --extras llama_cpp_internal</code>). 2. A GGUF-format LLM model file downloaded locally. 3. Update <code>LLAMA_CPP_INTERNAL_MODEL_PATH</code> below to point to your model file. \"\"\" import asyncio import json import logging import os import shutil from pathlib import Path from typing import Any, Dict, Optional</p> <p>from genie_tooling import tool from genie_tooling.agents.react_agent import ReActAgent from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from pydantic import BaseModel from pydantic import Field as PydanticField</p>"},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#-configuration-","title":"--- Configuration ---","text":""},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#user-action-required-update-this-path-to-your-gguf-model-file","title":"!!! USER ACTION REQUIRED: Update this path to your GGUF model file !!!","text":"<p>LLAMA_CPP_INTERNAL_MODEL_PATH = os.getenv(\"LLAMA_CPP_INTERNAL_MODEL_PATH\", \"/path/to/your/model.gguf\")</p>"},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#example-llama_cpp_internal_model_path-homeusermodelsmistral-7b-instruct-v02q4_k_mgguf","title":"Example: LLAMA_CPP_INTERNAL_MODEL_PATH = \"/home/user/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"","text":""},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#-pydantic-model-for-llm-output-parsing-","title":"--- Pydantic Model for LLM Output Parsing ---","text":"<p>class ExtractedDetailsInternal(BaseModel):     item_name: str = PydanticField(description=\"The name of the item.\")     quantity: int = PydanticField(gt=0, description=\"The quantity of the item.\")     notes: Optional[str] = PydanticField(None, description=\"Optional notes about the item.\")</p>"},{"location":"tutorials/E24_local_e2e_llama_cpp_internal/#-custom-tool-definition-","title":"--- Custom Tool Definition ---","text":"<p>@tool async def get_file_metadata_internal(file_path: str) -&gt; Dict[str, Any]:     \"\"\"     Retrieves metadata for a specified file within the agent's sandbox.     Args:         file_path (str): The relative path to the file within the agent's sandbox.     Returns:         Dict[str, Any]: A dictionary containing file metadata (name, size, exists) or an error.     \"\"\"     sandbox_base = Path(\"./e24_agent_sandbox\")     try:         prospective_path = (sandbox_base / file_path).resolve()         if not str(prospective_path).startswith(str(sandbox_base.resolve())):             return {\"error\": \"Path traversal attempt detected.\", \"path_resolved\": str(prospective_path)}         full_path = prospective_path     except Exception as e:         return {\"error\": f\"Path resolution error: {e!s}\"}</p> <pre><code>if full_path.exists() and full_path.is_file():\n    return {\"file_name\": full_path.name, \"size_bytes\": full_path.stat().st_size, \"exists\": True}\nelse:\n    return {\"error\": \"File not found or is not a file.\", \"path_checked\": str(full_path), \"exists\": False}\n</code></pre> <p>async def run_local_e2e_llama_cpp_internal():     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     # logging.getLogger(\"genie_tooling.llm_providers.impl.llama_cpp_internal_provider\").setLevel(logging.DEBUG)</p> <pre><code>model_path_obj = Path(LLAMA_CPP_INTERNAL_MODEL_PATH)\nif LLAMA_CPP_INTERNAL_MODEL_PATH == \"/path/to/your/model.gguf\" or not model_path_obj.exists():\n    print(\"\\nERROR: LLAMA_CPP_INTERNAL_MODEL_PATH is not correctly set or file does not exist.\")\n    print(f\"Please edit this script ('{__file__}') and update LLAMA_CPP_INTERNAL_MODEL_PATH,\")\n    print(\"or set the LLAMA_CPP_INTERNAL_MODEL_PATH environment variable.\")\n    print(f\"Current path: '{LLAMA_CPP_INTERNAL_MODEL_PATH}' (Exists: {model_path_obj.exists()})\\n\")\n    return\n\nprint(f\"--- Genie Tooling Local E2E Test (Llama.cpp Internal with model: {model_path_obj.name}) ---\")\n\nsandbox_dir = Path(\"./e24_agent_sandbox\")\nrag_data_dir = sandbox_dir / \"rag_docs\"\nprompt_dir = Path(\"./e24_prompts\")\n\nfor p in [sandbox_dir, rag_data_dir, prompt_dir]:\n    if p.exists(): shutil.rmtree(p)\n    p.mkdir(parents=True, exist_ok=True)\n\n(rag_data_dir / \"doc1.txt\").write_text(\"Internal Llama.cpp provides local GGUF model execution.\")\n(rag_data_dir / \"doc2.txt\").write_text(\"Genie Tooling integrates llama-cpp-python for this.\")\n(prompt_dir / \"greeting_template_internal.j2\").write_text(\n    '[{\"role\": \"system\", \"content\": \"You are {{ bot_name }} running on Llama.cpp internal.\"},\\n'\n    ' {\"role\": \"user\", \"content\": \"Hello! My name is {{ user_name }}.\"}]'\n)\n(sandbox_dir / \"testfile_internal.txt\").write_text(\"This is a test file for internal metadata.\")\n(prompt_dir / \"react_agent_system_prompt_internal_v1.j2\").write_text(\n    \"You are ReActBot (Internal Llama). Your goal is: {{ goal }}.\\n\"\n    \"Available tools:\\n{{ tool_definitions }}\\n\"\n    \"Scratchpad (Thought/Action/Observation cycles):\\n{{ scratchpad }}\\nThought:\"\n)\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp_internal\",\n        llm_llama_cpp_internal_model_path=str(model_path_obj.resolve()),\n        llm_llama_cpp_internal_n_gpu_layers=-1, # Offload all to GPU if possible\n        llm_llama_cpp_internal_n_ctx=2048,      # Context size\n        llm_llama_cpp_internal_chat_format=\"mistral\", # Example, adjust if your model needs a different one\n        llm_llama_cpp_internal_model_name_for_logging=model_path_obj.stem, # Use model file stem for logging\n\n        command_processor=\"llm_assisted\",\n        command_processor_formatter_id_alias=\"compact_text_formatter\",\n        tool_lookup=\"embedding\",\n        tool_lookup_embedder_id_alias=\"st_embedder\",\n        tool_lookup_formatter_id_alias=\"compact_text_formatter\",\n        rag_loader=\"file_system\",\n        rag_embedder=\"sentence_transformer\",\n        rag_vector_store=\"faiss\",\n        cache=\"in-memory\",\n        observability_tracer=\"console_tracer\",\n        hitl_approver=\"cli_hitl_approver\",\n        token_usage_recorder=\"in_memory_token_recorder\",\n        input_guardrails=[\"keyword_blocklist_guardrail\"],\n        prompt_registry=\"file_system_prompt_registry\",\n        prompt_template_engine=\"jinja2_chat_formatter\",\n        conversation_state_provider=\"in_memory_convo_provider\",\n        default_llm_output_parser=\"pydantic_output_parser\"\n    ),\n    # The @tool decorated `get_file_metadata_internal` is auto-enabled by default.\n    # Class-based tools like calculator and sandboxed_fs still need to be enabled.\n    tool_configurations={\n        \"calculator_tool\": {},\n        \"sandboxed_fs_tool_v1\": {\"sandbox_base_path\": str(sandbox_dir.resolve())},\n    },\n    guardrail_configurations={\n        \"keyword_blocklist_guardrail_v1\": {\n            \"blocklist\": [\"top_secret_internal\", \"classified_gguf\"],\n            \"action_on_match\": \"block\"\n        }\n    },\n    prompt_registry_configurations={\n        \"file_system_prompt_registry_v1\": {\n            \"base_path\": str(prompt_dir.resolve()),\n            \"template_suffix\": \".j2\"\n        }\n    },\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": {\"log_level\": \"INFO\"}\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\n[1] Initializing Genie facade (Llama.cpp Internal)...\")\n    genie = await Genie.create(config=app_config)\n    await genie.register_tool_functions([get_file_metadata_internal])\n    print(\"Genie facade initialized and custom tool registered!\")\n\n    print(\"\\n[2] Testing LLM Chat and Generate (with Pydantic parsing)...\")\n    try:\n        chat_resp = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Llama.cpp internal! Write a short, friendly greeting.\"}])\n        print(f\"  LLM Chat Response: {chat_resp['message']['content'][:100]}...\")\n\n        gen_prompt = \"Extract details: Item is 'LocalWidget', quantity is 77, notes: 'Needs local processing'.\"\n        gen_resp = await genie.llm.generate(\n            prompt=f\"You must output ONLY a valid JSON object. {gen_prompt}\",\n            output_schema=ExtractedDetailsInternal, # For GBNF\n            temperature=0.1,\n            max_tokens=256 # Llama.cpp internal provider maps max_tokens to n_predict\n        )\n        print(f\"  LLM Generate (raw text for Pydantic): {gen_resp['text']}\")\n        parsed_details = await genie.llm.parse_output(gen_resp, schema=ExtractedDetailsInternal)\n        if isinstance(parsed_details, ExtractedDetailsInternal):\n            print(f\"  Parsed Pydantic: Name='{parsed_details.item_name}', Qty='{parsed_details.quantity}', Notes='{parsed_details.notes}'\")\n            assert parsed_details.item_name == \"LocalWidget\"\n            assert parsed_details.quantity == 77\n        else:\n            assert False, f\"Pydantic parsing failed, got type: {type(parsed_details)}\"\n    except Exception as e_llm:\n        print(f\"  LLM Error: {e_llm}\")\n        raise\n\n    print(\"\\n[3] Testing RAG (indexing and search)...\")\n    try:\n        rag_collection_name = \"e24_internal_llama_docs\"\n        index_result = await genie.rag.index_directory(str(rag_data_dir.resolve()), collection_name=rag_collection_name)\n        print(f\"  Indexed documents from '{rag_data_dir.resolve()}'. Result: {index_result}\")\n        assert index_result.get(\"status\") == \"success\"\n        rag_results = await genie.rag.search(\"What is Llama.cpp internal?\", collection_name=rag_collection_name, top_k=1)\n        if rag_results:\n            print(f\"  RAG Search Result: '{rag_results[0].content[:100]}...' (Score: {rag_results[0].score:.2f})\")\n            assert \"Llama.cpp\" in rag_results[0].content\n        else:\n            assert False, \"RAG search returned no results\"\n    except Exception as e_rag:\n        print(f\"  RAG Error: {e_rag}\")\n        raise\n\n    print(\"\\n[4] Testing direct custom tool execution (get_file_metadata_internal)...\")\n    try:\n        metadata_result = await genie.execute_tool(\"get_file_metadata_internal\", file_path=\"testfile_internal.txt\")\n        print(f\"  Metadata for 'testfile_internal.txt': {metadata_result}\")\n        assert metadata_result.get(\"file_name\") == \"testfile_internal.txt\"\n        assert metadata_result.get(\"exists\") is True\n    except Exception as e_tool_direct:\n        print(f\"  Direct tool execution error: {e_tool_direct}\")\n        raise\n\n    print(\"\\n[5] Testing `run_command` (LLM-assisted, HITL)...\")\n    try:\n        command_text = \"What is the size of the file named testfile_internal.txt in the sandbox?\"\n        print(f\"  Sending command: '{command_text}' (Approval may be requested on CLI)\")\n        command_result = await genie.run_command(command_text)\n        print(f\"  `run_command` result: {json.dumps(command_result, indent=2, default=str)}\")\n        assert command_result.get(\"tool_result\", {}).get(\"size_bytes\") is not None or \\\n               command_result.get(\"hitl_decision\", {}).get(\"status\") != \"approved\", \\\n               f\"Tool did not run or HITL was not approved. Result: {command_result}\"\n    except Exception as e_run_cmd:\n        print(f\"  `run_command` error: {e_run_cmd}\")\n        raise\n\n    print(\"\\n[6] Testing ReActAgent with Llama.cpp Internal...\")\n    try:\n        # Use the internal-specific prompt for the agent\n        react_agent = ReActAgent(genie=genie, agent_config={\"max_iterations\": 3, \"system_prompt_id\": \"react_agent_system_prompt_internal_v1\"})\n        agent_goal = \"What is 25 times 4 using the calculator?\"\n        print(f\"  Agent Goal: '{agent_goal}'\")\n        agent_result = await react_agent.run(goal=agent_goal)\n        print(f\"  ReActAgent Result Status: {agent_result['status']}\")\n        print(f\"  ReActAgent Output: {str(agent_result['output'])[:200]}...\")\n        assert agent_result[\"status\"] == \"success\"\n        assert \"100\" in str(agent_result[\"output\"])\n    except Exception as e_agent:\n        print(f\"  ReActAgent Error: {e_agent}\")\n        raise\n\n    print(\"\\n--- E2E Test with Llama.cpp Internal PASSED ---\")\n\nexcept Exception as e_main:\n    print(f\"\\nE2E Test FAILED with critical error: {e_main}\")\n    logging.error(\"E2E Llama.cpp Internal Main Error\", exc_info=True)\n    raise\nfinally:\n    if genie:\n        print(\"\\n[X] Tearing down Genie facade...\")\n        await genie.close()\n        print(\"Genie facade torn down.\")\n\n    print(\"\\n[Y] Cleaning up test files/directories...\")\n    for p_cleanup in [sandbox_dir, prompt_dir]:\n        if p_cleanup.exists(): shutil.rmtree(p_cleanup, ignore_errors=True)\n    print(\"Cleanup complete.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_local_e2e_llama_cpp_internal())</p>"},{"location":"tutorials/E25_llama_cpp_internal_gbnf_parsing/","title":"Tutorial: Llama.cpp Internal GBNF (E25)","text":"<p>This tutorial corresponds to the example file <code>examples/E25_llama_cpp_internal_gbnf_parsing.py</code>.</p> <p>It demonstrates a key feature of the Llama.cpp providers: using GBNF grammar for constrained, structured output. It shows how to: - Define a Pydantic model for the desired output structure. - Pass this model to <code>genie.llm.chat()</code> via the <code>output_schema</code> parameter. - Observe how the Llama.cpp internal provider generates a JSON string that perfectly matches the Pydantic model, which can then be parsed reliably.</p>"},{"location":"tutorials/E25_llama_cpp_internal_gbnf_parsing/#example-code","title":"Example Code","text":""},{"location":"tutorials/E25_llama_cpp_internal_gbnf_parsing/#examplese25_llama_cpp_internal_gbnf_parsingpy","title":"examples/E25_llama_cpp_internal_gbnf_parsing.py","text":""},{"location":"tutorials/E25_llama_cpp_internal_gbnf_parsing/#originally-tstpy","title":"(Originally tst.py)","text":"<p>import asyncio import json import logging import os  # Added os for getenv from pathlib import Path from typing import List, Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie from genie_tooling.llm_providers.types import ChatMessage from pydantic import BaseModel, Field</p> <p>class ExtractedItemInfo(BaseModel):     item_name: str = Field(description=\"The name of the item mentioned.\")     quantity: int = Field(description=\"The quantity of the item.\", gt=0)     color: Optional[str] = Field(None, description=\"The color of the item, if specified.\")</p> <p>async def run_llama_cpp_internal_gbnf_test():     logging.basicConfig(level=logging.INFO)     # logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)     # logging.getLogger(\"genie_tooling.llm_providers.impl.llama_cpp_internal_provider\").setLevel(logging.DEBUG)     # logging.getLogger(\"genie_tooling.utils.gbnf\").setLevel(logging.DEBUG)</p> <pre><code>print(\"--- Llama.cpp Internal Provider GBNF Parsing Test ---\")\n\n# !!! USER ACTION REQUIRED: Update this path to your GGUF model file !!!\n# You can also set the LLAMA_CPP_INTERNAL_MODEL_PATH environment variable.\ndefault_model_path = \"/path/to/your/model.gguf\" # Placeholder\nmodel_path_str = os.getenv(\"LLAMA_CPP_INTERNAL_MODEL_PATH\", default_model_path)\n# Example: model_path_str = \"/home/user/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n\nmodel_path = Path(model_path_str)\nif model_path_str == default_model_path or not model_path.exists():\n    print(\"\\nERROR: Model path not configured or file does not exist.\")\n    print(f\"Please edit this script and set 'model_path_str' (currently '{model_path_str}')\")\n    print(\"or set the LLAMA_CPP_INTERNAL_MODEL_PATH environment variable.\")\n    print(f\"Current path check: '{model_path.resolve()}' (Exists: {model_path.exists()})\\n\")\n    return\n\napp_config = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"llama_cpp_internal\",\n        llm_llama_cpp_internal_model_path=str(model_path.resolve()),\n        llm_llama_cpp_internal_n_gpu_layers=-1, # Offload all layers if possible\n        llm_llama_cpp_internal_chat_format=\"mistral\", # Adjust to your model if needed\n        token_usage_recorder=\"in_memory_token_recorder\",\n        default_llm_output_parser=\"pydantic_output_parser\"\n    )\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(f\"\\nInitializing Genie with Llama.cpp internal provider for GBNF (Model: {model_path.name})...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized successfully!\")\n\n    print(\"\\n--- LLM Chat with GBNF Example (Llama.cpp Internal) ---\")\n    user_prompt_for_gbnf = (\n        \"From the sentence: 'I need 5 red apples for the pie.', \"\n        \"extract the item name, quantity, and color. \"\n        \"Please provide the output as a JSON object with keys: 'item_name', 'quantity', and 'color'.\"\n        \"Output ONLY the JSON object.\"\n    )\n    messages: List[ChatMessage] = [{\"role\": \"user\", \"content\": user_prompt_for_gbnf}]\n    print(f\"Sending message to LLM for GBNF generation: '{messages[0]['content']}'\")\n\n    chat_response = await genie.llm.chat(\n        messages,\n        output_schema=ExtractedItemInfo,\n        temperature=0.1,\n        max_tokens=256\n    )\n    response_content_str = chat_response.get(\"message\", {}).get(\"content\")\n    print(f\"\\nLLM (Llama.cpp Internal) raw JSON string output:\\n{response_content_str}\")\n\n    if response_content_str:\n        try:\n            parsed_info: Optional[ExtractedItemInfo] = await genie.llm.parse_output(\n                chat_response,\n                schema=ExtractedItemInfo\n            )\n            if isinstance(parsed_info, ExtractedItemInfo):\n                print(\"\\nSuccessfully parsed into Pydantic model:\")\n                print(json.dumps(parsed_info.model_dump(), indent=2))\n                assert parsed_info.item_name.lower() == \"apples\"\n                assert parsed_info.quantity == 5\n                assert parsed_info.color and parsed_info.color.lower() == \"red\"\n                print(\"\\nGBNF Test with Pydantic Model PASSED!\")\n            else:\n                print(f\"\\nERROR: Parsing did not return an ExtractedItemInfo instance. Got: {type(parsed_info)}\")\n        except ValueError as ve:\n            print(f\"\\nERROR: Failed to parse or validate LLM output against Pydantic model: {ve}\")\n            print(f\"LLM's raw JSON string was: {response_content_str}\")\n        except Exception as e_parse:\n            print(f\"\\nERROR: An unexpected error occurred during parsing: {e_parse}\")\n    else:\n        print(\"\\nERROR: LLM did not return any content for GBNF parsing.\")\n\n    usage_info = chat_response.get(\"usage\")\n    if usage_info:\n        print(f\"\\nToken usage: {usage_info}\")\n    elif genie and genie.usage:\n        summary = await genie.usage.get_summary()\n        print(f\"\\nToken usage summary from recorder: {summary}\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred: {e}\")\n    logging.exception(\"Llama.cpp internal GBNF test error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie facade torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_llama_cpp_internal_gbnf_test())</p>"},{"location":"tutorials/E26_deep_research_agent_e2e/","title":"Tutorial: Deep Research Agent E2E (E26)","text":"<p>This tutorial corresponds to the example file <code>examples/E26_deep_research_agent_e2e.py</code>.</p> <p>It demonstrates how to configure and run the <code>DeepResearchAgent</code>, a more advanced agent that uses a plan-and-execute strategy. It shows how to: - Configure the agent to use specific tools for web search (DuckDuckGo) and academic paper search (ArXiv). - Configure \"deep retrieval\" tools (<code>WebPageScraperTool</code>, <code>ArxivPDFTextExtractorTool</code>) that the agent can choose to use for in-depth analysis of promising search results. - Observe the agent's planning, execution, and final report generation process.</p>"},{"location":"tutorials/E26_deep_research_agent_e2e/#example-code","title":"Example Code","text":""},{"location":"tutorials/E26_deep_research_agent_e2e/#examplese26_deep_research_agent_e2epy","title":"examples/E26_deep_research_agent_e2e.py","text":"<p>\"\"\" Genie Tooling - DeepResearchAgent E2E Test with Deep Retrieval Tools This script initializes the Genie facade, configures the DeepResearchAgent to use DuckDuckGo for web searches, ArXiv search, and the deep retrieval tools (WebPageScraperTool, ArxivPDFTextExtractorTool), and performs a research query.</p> <p>This example uses the Llama.cpp server provider.</p> <p>Prerequisites: 1. <code>genie-tooling</code> installed with relevant extras:    <code>poetry install --all-extras</code>    (Specifically needs: llama_cpp_server, ddg_search_tool, arxiv_tool, web_tools, pdf_tools) 2. A Llama.cpp server running (see E23 for example command), accessible via LLAMA_CPP_BASE_URL.    Ensure the model alias (LLAMA_CPP_MODEL_ALIAS) matches your server setup. \"\"\" import asyncio import json import logging import os from typing import Optional</p> <p>from dotenv import load_dotenv from genie_tooling.agents.deep_research_agent import DeepResearchAgent from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>logging.basicConfig(level=logging.INFO) logging.getLogger(\"genie_tooling.agents.deep_research_agent\").setLevel(logging.DEBUG)</p>"},{"location":"tutorials/E26_deep_research_agent_e2e/#logginggetloggergenie_toolingsetlevelloggingdebug","title":"logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)","text":"<p>LLAMA_CPP_BASE_URL = os.getenv(\"LLAMA_CPP_BASE_URL\", \"http://localhost:8080\") LLAMA_CPP_MODEL_ALIAS = os.getenv(\"LLAMA_CPP_MODEL_ALIAS\", \"mistral:latest\") LLAMA_CPP_API_KEY_NAME = \"LLAMA_CPP_API_KEY\"</p> <p>async def run_deep_research_test_with_deep_dive():     print(\"--- Genie Tooling: DeepResearchAgent E2E Test (Llama.cpp Server) ---\")     load_dotenv()</p> <pre><code>print(f\"Using Llama.cpp server at: {LLAMA_CPP_BASE_URL} with model alias {LLAMA_CPP_MODEL_ALIAS}\")\nif os.getenv(LLAMA_CPP_API_KEY_NAME):\n    print(f\"Using API Key from environment variable: {LLAMA_CPP_API_KEY_NAME}\")\n\napp_features = FeatureSettings(\n    llm=\"llama_cpp\", # Target Llama.cpp server\n    llm_llama_cpp_base_url=LLAMA_CPP_BASE_URL,\n    llm_llama_cpp_model_name=LLAMA_CPP_MODEL_ALIAS,\n    llm_llama_cpp_api_key_name=LLAMA_CPP_API_KEY_NAME if os.getenv(LLAMA_CPP_API_KEY_NAME) else None,\n\n    default_llm_output_parser=\"pydantic_output_parser\",\n    prompt_registry=\"file_system_prompt_registry\",\n    prompt_template_engine=\"jinja2_chat_formatter\",\n    tool_lookup=\"embedding\",\n    observability_tracer=\"console_tracer\",\n)\n\napp_config = MiddlewareConfig(\n    features=app_features,\n    tool_configurations={\n        \"duckduckgo_search_tool_v1\": {},\n        \"arxiv_search_tool\": {},\n        \"web_page_scraper_tool_v1\": {},\n        \"arxiv_pdf_extractor_tool_v1\": {},\n    }\n)\n\ngenie: Optional[Genie] = None\ntry:\n    print(\"\\nInitializing Genie facade...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie facade initialized successfully!\")\n\n    dra_agent_config = {\n        \"web_search_tool_config\": {\n            \"tool_id\": \"duckduckgo_search_tool_v1\", \"num_results\": 2,\n            \"description_for_llm\": \"Searches the web using DuckDuckGo for general information.\"\n        },\n        \"arxiv_search_tool_config\": {\n            \"tool_id\": \"arxiv_search_tool\", \"max_results\": 1,\n            \"description_for_llm\": \"Searches ArXiv for academic papers.\"\n        },\n        \"web_page_scraper_tool_config\": {\n            \"tool_id\": \"web_page_scraper_tool_v1\",\n            \"description_for_llm\": \"Fetches and extracts text from a web URL.\"\n        },\n        \"arxiv_pdf_extractor_tool_config\": {\n            \"tool_id\": \"arxiv_pdf_extractor_tool_v1\",\n            \"description_for_llm\": \"Downloads and extracts text from an ArXiv PDF.\"\n        },\n        \"rag_search_tool_config\": {\n            \"collection_name\": \"e26_research_docs\", \"top_k\": 1,\n            \"tool_id\": \"internal_rag_search_tool\",\n            \"description_for_llm\": \"Searches an internal knowledge base.\"\n        },\n        \"max_total_gathering_cycles\": 3,\n        \"max_sub_questions_per_plan\": 2,\n        \"max_plan_refinement_cycles\": 1,\n        \"iterative_synthesis_interval\": 2,\n    }\n    research_agent = DeepResearchAgent(genie=genie, agent_config=dra_agent_config)\n    print(\"DeepResearchAgent initialized with deep retrieval capabilities.\")\n\n    query = \"Summarize the key contributions of the 'Transformer' architecture as introduced in 'Attention Is All You Need' (ArXiv:1706.03762) and find one recent (2023-2024) application of Transformers in natural language processing.\"\n\n    print(f\"\\nConducting research for query: '{query}'\")\n    agent_output = await research_agent.run(goal=query)\n\n    print(\"\\n\\n--- Deep Research Agent Output ---\")\n    print(f\"Status: {agent_output['status']}\")\n    print(f\"\\nFinal Report:\\n{agent_output['output']}\")\n\n    print(\"\\n--- Research Plan (Final) ---\")\n    if agent_output.get(\"plan\"):\n        print(json.dumps(agent_output[\"plan\"], indent=2))\n    else:\n        print(\"No plan information available in output.\")\n\n    print(\"\\n--- Research History (Snippets - first 2 if any) ---\")\n    if agent_output.get(\"history\"):\n        snippet_count = 0\n        for item in agent_output[\"history\"]:\n            if item.get(\"type\") == \"snippet_gathered\":\n                print(f\"  Source Type: {item.get('snippet_source_type', item.get('source_type'))}, Identifier: {item.get('snippet_source', item.get('source_identifier'))}\")\n                print(f\"  Sub-query: {item.get('sub_query')}\")\n                print(f\"  Extracted Info (first 150 chars): {str(item.get('extracted_info', ''))[:150]}...\")\n                snippet_count += 1\n                if snippet_count &gt;= 2:\n                    break\n        if snippet_count == 0:\n            print(\"  No snippets were gathered in this run (check agent DEBUG logs for details).\")\n\nexcept Exception as e:\n    print(f\"\\nE2E Test FAILED: An unexpected error occurred: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    if genie:\n        print(\"\\nTearing down Genie facade...\")\n        await genie.close()\n        print(\"Genie facade torn down.\")\n</code></pre> <p>if name == \"main\":     asyncio.run(run_deep_research_test_with_deep_dive())</p>"},{"location":"tutorials/E27_pyvider_log_adapter_tracing_example/","title":"Tutorial: Pyvider Log Adapter (E27)","text":"<p>This tutorial corresponds to the example file <code>examples/E27_pyvider_log_adapter_tracing_example.py</code>.</p> <p>It demonstrates how to integrate Genie's eventing system with the <code>pyvider-telemetry</code> library for advanced structured logging. It shows how to: - Configure Genie to use the <code>PyviderTelemetryLogAdapter</code>. - Use the <code>ConsoleTracerPlugin</code>, which then delegates its output formatting to the configured Pyvider adapter. - Observe how trace events are formatted by Pyvider (e.g., key-value or JSON, potentially with emojis).</p>"},{"location":"tutorials/E27_pyvider_log_adapter_tracing_example/#example-code","title":"Example Code","text":""},{"location":"tutorials/E27_pyvider_log_adapter_tracing_example/#examplese27_pyvider_log_adapter_tracing_examplepy","title":"examples/E27_pyvider_log_adapter_tracing_example.py","text":"<p>\"\"\" Example: Using PyviderTelemetryLogAdapter for Observability Tracing</p> <p>This example demonstrates how to configure Genie Tooling to use the PyviderTelemetryLogAdapter for processing and outputting interaction traces. It builds upon the concepts shown in E17 (Observability &amp; Tracing).</p> <p>When PyviderTelemetryLogAdapter is active and ConsoleTracerPlugin is used, the trace events will be formatted by Pyvider (e.g., key-value or JSON, potentially with emojis) instead of the ConsoleTracerPlugin's default format.</p> <p>To Run: 1. Ensure Genie Tooling is installed with Pyvider support:    <code>poetry install --all-extras</code> or <code>poetry install --extras pyvider_adapter</code> 2. Ensure <code>pyvider-telemetry</code> library is installed (it's an optional dependency). 3. Run from the root of the project:    <code>poetry run python examples/E27_pyvider_log_adapter_tracing_example.py</code>    Observe the console output; it should be formatted by Pyvider. \"\"\" import asyncio import logging import traceback  # For stacktrace example import uuid from typing import Optional</p> <p>from genie_tooling.config.features import FeatureSettings from genie_tooling.config.models import MiddlewareConfig from genie_tooling.genie import Genie</p> <p>async def run_pyvider_observability_demo():     print(\"--- Pyvider Log Adapter with Observability Tracing Example ---\")</p> <pre><code># Configure logging for the application itself (not Pyvider's internal config here)\n# Pyvider will configure its own handlers based on its setup.\nlogging.basicConfig(level=logging.INFO)\n# To see Genie's internal debug logs (not Pyvider formatted unless Pyvider is set to DEBUG for genie_tooling):\n# logging.getLogger(\"genie_tooling\").setLevel(logging.DEBUG)\n\n\n# Configuration using PyviderTelemetryLogAdapter\napp_config_pyvider = MiddlewareConfig(\n    features=FeatureSettings(\n        llm=\"ollama\", # For an operation that generates traces\n        llm_ollama_model_name=\"mistral:latest\",\n\n        logging_adapter=\"pyvider_log_adapter\", # Select Pyvider\n        logging_pyvider_service_name=\"GeniePyviderDemo\", # Service name for Pyvider\n\n        observability_tracer=\"console_tracer\" # ConsoleTracer will use the configured LogAdapter\n    ),\n    # Configuration for PyviderTelemetryLogAdapter\n    log_adapter_configurations={\n        \"pyvider_telemetry_log_adapter_v1\": { # Canonical ID\n            \"service_name\": \"GeniePyviderServiceExplicit\", # Overrides feature setting\n            \"default_level\": \"DEBUG\", # Pyvider's default log level for its logger\n            \"console_formatter\": \"key_value\", # \"key_value\" or \"json\"\n            \"logger_name_emoji_prefix_enabled\": True,\n            \"das_emoji_prefix_enabled\": True,\n            \"omit_timestamp\": False,\n            # Example: Configure redactor for Pyvider adapter\n            # \"redactor_plugin_id\": \"schema_aware_redactor_v1\",\n            # \"redactor_config\": {\"redact_matching_key_names\": True}\n        }\n    },\n    # ConsoleTracerPlugin itself doesn't need much config when delegating\n    observability_tracer_configurations={\n        \"console_tracer_plugin_v1\": {}\n    }\n)\n\napp_config = app_config_pyvider\n\ngenie: Optional[Genie] = None\ntry:\n    print(f\"\\nInitializing Genie with LogAdapter: {app_config.features.logging_adapter} and Tracer: {app_config.features.observability_tracer}...\")\n    genie = await Genie.create(config=app_config)\n    print(\"Genie initialized! Automatic traces will now be generated and processed by PyviderTelemetryLogAdapter.\")\n\n    print(\"\\n--- Performing operations that trigger automatic traces ---\")\n    try:\n        # This LLM call will generate trace events (e.g., llm.chat.start, llm.chat.success)\n        # which ConsoleTracerPlugin will pass to PyviderTelemetryLogAdapter.\n        chat_response = await genie.llm.chat([{\"role\": \"user\", \"content\": \"Hello Pyvider Tracer! Tell me a short story.\"}])\n        print(f\"LLM Response (first 60 chars): {chat_response['message']['content'][:60]}...\")\n    except Exception as e_llm:\n        # This error will also be traced.\n        print(f\"LLM call failed (expected if Ollama not running): {e_llm}\")\n        # Manually trace the error if needed, though Genie's LLMInterface might do it.\n        await genie.observability.trace_event(\n            \"app.llm_call.error\",\n            {\"error\": str(e_llm), \"type\": type(e_llm).__name__},\n            \"PyviderDemoApp\"\n        )\n\n\n    print(\"\\n--- Emitting a custom trace event (will also go through Pyvider) ---\")\n    custom_correlation_id = str(uuid.uuid4())\n    custom_event_data = {\n        \"user_id\": \"pyvider_user\",\n        \"input_param\": \"example_value_for_pyvider\",\n        \"status_override\": \"attempt\" # For DAS mapping in Pyvider adapter\n    }\n    await genie.observability.trace_event(\n        event_name=\"my_app.custom_pyvider_op.start\",\n        data=custom_event_data,\n        component=\"MyApplicationLogic\", # This might become 'domain' in Pyvider\n        correlation_id=custom_correlation_id\n    )\n    await asyncio.sleep(0.1) # Simulate work\n    try:\n        raise ValueError(\"Something went wrong in custom Pyvider op!\")\n    except ValueError as e_custom:\n        await genie.observability.trace_event(\n            event_name=\"my_app.custom_pyvider_op.error\",\n            data={\n                \"status\": \"failed\", # Will be picked up by Pyvider adapter for DAS\n                \"error_message\": str(e_custom),\n                \"error_type\": type(e_custom).__name__,\n                \"error_stacktrace\": traceback.format_exc()\n                },\n            component=\"MyApplicationLogic\",\n            correlation_id=custom_correlation_id\n        )\n    print(\"Custom trace events (including an error) emitted and processed by Pyvider.\")\n\nexcept Exception as e:\n    print(f\"\\nAn error occurred in the demo: {e}\")\n    logging.exception(\"Pyvider observability demo error details:\")\nfinally:\n    if genie:\n        await genie.close()\n        print(\"\\nGenie torn down.\")\n</code></pre> <p>if name == \"main\":     # Note: Pyvider's setup_telemetry might configure the root logger.     # If you want to control formatting for non-Genie logs, configure Python's     # root logger before Genie/Pyvider setup, or adjust Pyvider's config.     asyncio.run(run_pyvider_observability_demo())</p>"}]}