# Tutorial: Llama.cpp Internal E2E (E24)

This tutorial corresponds to the example file `examples/E24_local_e2e_llama_cpp_internal.py`.

It provides a comprehensive end-to-end test using the Llama.cpp internal provider, which runs GGUF models directly without a separate server.

```python
# Full code from examples/E24_local_e2e_llama_cpp_internal.py
# (This will be auto-filled by your documentation generation process if configured,
# or you can paste the example code here manually.)
```
