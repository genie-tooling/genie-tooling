# Tutorial: Llama.cpp Internal E2E (E24)

This tutorial corresponds to the example file `examples/E24_local_e2e_llama_cpp_internal.py`.

It provides a comprehensive end-to-end test using the Llama.cpp internal provider, which runs GGUF models directly without a separate server. It covers the same range of features as the Llama.cpp server E2E test, demonstrating a fully local and self-contained agent setup.

**Prerequisite**: You must download a GGUF model file and update the path in the example script.

## Example Code

--8<-- "examples/E24_local_e2e_llama_cpp_internal.py"
